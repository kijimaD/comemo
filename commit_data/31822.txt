commit aea3aff66911e31cba9eddd93c02eb591ae483bf
Author: Josh Bleecher Snyder <josharian@gmail.com>
Date:   Fri Mar 17 16:04:46 2017 -0700

    cmd/compile: separate ssa.Frontend and ssa.TypeSource
    
    Prior to this CL, the ssa.Frontend field was responsible
    for providing types to the backend during compilation.
    However, the types needed by the backend are few and static.
    It makes more sense to use a struct for them
    and to hang that struct off the ssa.Config,
    which is the correct home for readonly data.
    Now that Types is a struct, we can clean up the names a bit as well.
    
    This has the added benefit of allowing early construction
    of all types needed by the backend.
    This will be useful for concurrent backend compilation.
    
    Passes toolstash-check -all. No compiler performance change.
    
    Updates #15756
    
    Change-Id: I021658c8cf2836d6a22bbc20cc828ac38c7da08a
    Reviewed-on: https://go-review.googlesource.com/38336
    Reviewed-by: Matthew Dempsky <mdempsky@google.com>
---
 src/cmd/compile/internal/gc/ssa.go                |   35 +-
 src/cmd/compile/internal/ssa/config.go            |   50 +-
 src/cmd/compile/internal/ssa/decompose.go         |   50 +-
 src/cmd/compile/internal/ssa/export_test.go       |   53 +-
 src/cmd/compile/internal/ssa/gen/386.rules        |    4 +-
 src/cmd/compile/internal/ssa/gen/AMD64.rules      |   20 +-
 src/cmd/compile/internal/ssa/gen/ARM.rules        |   38 +-
 src/cmd/compile/internal/ssa/gen/ARM64.rules      |   12 +-
 src/cmd/compile/internal/ssa/gen/MIPS.rules       |   88 +-
 src/cmd/compile/internal/ssa/gen/MIPS64.rules     |  124 +-
 src/cmd/compile/internal/ssa/gen/PPC64.rules      |   98 +-
 src/cmd/compile/internal/ssa/gen/S390X.rules      |    2 +-
 src/cmd/compile/internal/ssa/gen/dec.rules        |   66 +-
 src/cmd/compile/internal/ssa/gen/dec64.rules      |  314 ++---
 src/cmd/compile/internal/ssa/gen/generic.rules    |  308 ++---
 src/cmd/compile/internal/ssa/gen/rulegen.go       |   11 +-
 src/cmd/compile/internal/ssa/loopreschedchecks.go |    5 +-
 src/cmd/compile/internal/ssa/regalloc.go          |    7 +-
 src/cmd/compile/internal/ssa/rewrite386.go        |  116 +-
 src/cmd/compile/internal/ssa/rewriteAMD64.go      |  296 ++---
 src/cmd/compile/internal/ssa/rewriteARM.go        |  604 ++++-----
 src/cmd/compile/internal/ssa/rewriteARM64.go      |  750 +++++------
 src/cmd/compile/internal/ssa/rewriteMIPS.go       | 1028 +++++++--------
 src/cmd/compile/internal/ssa/rewriteMIPS64.go     | 1442 +++++++++++----------
 src/cmd/compile/internal/ssa/rewritePPC64.go      | 1016 +++++++--------
 src/cmd/compile/internal/ssa/rewriteS390X.go      | 1016 +++++++--------
 src/cmd/compile/internal/ssa/rewritedec.go        |   96 +-
 src/cmd/compile/internal/ssa/rewritedec64.go      |  956 +++++++-------
 src/cmd/compile/internal/ssa/rewritegeneric.go    |  680 +++++-----
 src/cmd/compile/internal/ssa/shortcircuit.go      |    4 +-
 src/cmd/compile/internal/ssa/writebarrier.go      |   19 +-
 31 files changed, 4674 insertions(+), 4634 deletions(-)

diff --git a/src/cmd/compile/internal/gc/ssa.go b/src/cmd/compile/internal/gc/ssa.go
index 322ca81d3d..5946da9f8c 100644
--- a/src/cmd/compile/internal/gc/ssa.go
+++ b/src/cmd/compile/internal/gc/ssa.go
@@ -22,7 +22,24 @@ var ssaConfig *ssa.Config
 var ssaCache *ssa.Cache
 
 func initssaconfig() {
-	ssaConfig = ssa.NewConfig(thearch.LinkArch.Name, Ctxt, Debug['N'] == 0)
+	types := ssa.Types{
+		Bool:    Types[TBOOL],
+		Int8:    Types[TINT8],
+		Int16:   Types[TINT16],
+		Int32:   Types[TINT32],
+		Int64:   Types[TINT64],
+		UInt8:   Types[TUINT8],
+		UInt16:  Types[TUINT16],
+		UInt32:  Types[TUINT32],
+		UInt64:  Types[TUINT64],
+		Float32: Types[TFLOAT32],
+		Float64: Types[TFLOAT64],
+		Int:     Types[TINT],
+		Uintptr: Types[TUINTPTR],
+		String:  Types[TSTRING],
+		BytePtr: ptrto(Types[TUINT8]),
+	}
+	ssaConfig = ssa.NewConfig(thearch.LinkArch.Name, types, Ctxt, Debug['N'] == 0)
 	if thearch.LinkArch.Name == "386" {
 		ssaConfig.Set387(thearch.Use387)
 	}
@@ -4673,22 +4690,6 @@ type ssafn struct {
 	log        bool
 }
 
-func (s *ssafn) TypeBool() ssa.Type    { return Types[TBOOL] }
-func (s *ssafn) TypeInt8() ssa.Type    { return Types[TINT8] }
-func (s *ssafn) TypeInt16() ssa.Type   { return Types[TINT16] }
-func (s *ssafn) TypeInt32() ssa.Type   { return Types[TINT32] }
-func (s *ssafn) TypeInt64() ssa.Type   { return Types[TINT64] }
-func (s *ssafn) TypeUInt8() ssa.Type   { return Types[TUINT8] }
-func (s *ssafn) TypeUInt16() ssa.Type  { return Types[TUINT16] }
-func (s *ssafn) TypeUInt32() ssa.Type  { return Types[TUINT32] }
-func (s *ssafn) TypeUInt64() ssa.Type  { return Types[TUINT64] }
-func (s *ssafn) TypeFloat32() ssa.Type { return Types[TFLOAT32] }
-func (s *ssafn) TypeFloat64() ssa.Type { return Types[TFLOAT64] }
-func (s *ssafn) TypeInt() ssa.Type     { return Types[TINT] }
-func (s *ssafn) TypeUintptr() ssa.Type { return Types[TUINTPTR] }
-func (s *ssafn) TypeString() ssa.Type  { return Types[TSTRING] }
-func (s *ssafn) TypeBytePtr() ssa.Type { return ptrto(Types[TUINT8]) }
-
 // StringData returns a symbol (a *Sym wrapped in an interface) which
 // is the data component of a global string constant containing s.
 func (*ssafn) StringData(s string) interface{} {
diff --git a/src/cmd/compile/internal/ssa/config.go b/src/cmd/compile/internal/ssa/config.go
index c764aa3539..7c49abba92 100644
--- a/src/cmd/compile/internal/ssa/config.go
+++ b/src/cmd/compile/internal/ssa/config.go
@@ -15,10 +15,11 @@ import (
 // It is created once, early during compilation,
 // and shared across all compilations.
 type Config struct {
-	arch            string        // "amd64", etc.
-	IntSize         int64         // 4 or 8
-	PtrSize         int64         // 4 or 8
-	RegSize         int64         // 4 or 8
+	arch            string // "amd64", etc.
+	IntSize         int64  // 4 or 8
+	PtrSize         int64  // 4 or 8
+	RegSize         int64  // 4 or 8
+	Types           Types
 	lowerBlock      blockRewriter // lowering function
 	lowerValue      valueRewriter // lowering function
 	registers       []Register    // machine registers
@@ -44,24 +45,22 @@ type (
 	valueRewriter func(*Value) bool
 )
 
-type TypeSource interface {
-	TypeBool() Type
-	TypeInt8() Type
-	TypeInt16() Type
-	TypeInt32() Type
-	TypeInt64() Type
-	TypeUInt8() Type
-	TypeUInt16() Type
-	TypeUInt32() Type
-	TypeUInt64() Type
-	TypeInt() Type
-	TypeFloat32() Type
-	TypeFloat64() Type
-	TypeUintptr() Type
-	TypeString() Type
-	TypeBytePtr() Type // TODO: use unsafe.Pointer instead?
-
-	CanSSA(t Type) bool
+type Types struct {
+	Bool    Type
+	Int8    Type
+	Int16   Type
+	Int32   Type
+	Int64   Type
+	UInt8   Type
+	UInt16  Type
+	UInt32  Type
+	UInt64  Type
+	Int     Type
+	Float32 Type
+	Float64 Type
+	Uintptr Type
+	String  Type
+	BytePtr Type // TODO: use unsafe.Pointer instead?
 }
 
 type Logger interface {
@@ -87,7 +86,8 @@ type Logger interface {
 }
 
 type Frontend interface {
-	TypeSource
+	CanSSA(t Type) bool
+
 	Logger
 
 	// StringData returns a symbol pointing to the given string's contents.
@@ -135,8 +135,8 @@ type GCNode interface {
 }
 
 // NewConfig returns a new configuration object for the given architecture.
-func NewConfig(arch string, ctxt *obj.Link, optimize bool) *Config {
-	c := &Config{arch: arch}
+func NewConfig(arch string, types Types, ctxt *obj.Link, optimize bool) *Config {
+	c := &Config{arch: arch, Types: types}
 	switch arch {
 	case "amd64":
 		c.IntSize = 8
diff --git a/src/cmd/compile/internal/ssa/decompose.go b/src/cmd/compile/internal/ssa/decompose.go
index a0cd5b2c41..56cb46ddac 100644
--- a/src/cmd/compile/internal/ssa/decompose.go
+++ b/src/cmd/compile/internal/ssa/decompose.go
@@ -28,15 +28,15 @@ func decomposeBuiltIn(f *Func) {
 		case t.IsInteger() && t.Size() == 8 && f.Config.IntSize == 4:
 			var elemType Type
 			if t.IsSigned() {
-				elemType = f.fe.TypeInt32()
+				elemType = f.Config.Types.Int32
 			} else {
-				elemType = f.fe.TypeUInt32()
+				elemType = f.Config.Types.UInt32
 			}
 			hiName, loName := f.fe.SplitInt64(name)
 			newNames = append(newNames, hiName, loName)
 			for _, v := range f.NamedValues[name] {
 				hi := v.Block.NewValue1(v.Pos, OpInt64Hi, elemType, v)
-				lo := v.Block.NewValue1(v.Pos, OpInt64Lo, f.fe.TypeUInt32(), v)
+				lo := v.Block.NewValue1(v.Pos, OpInt64Lo, f.Config.Types.UInt32, v)
 				f.NamedValues[hiName] = append(f.NamedValues[hiName], hi)
 				f.NamedValues[loName] = append(f.NamedValues[loName], lo)
 			}
@@ -44,9 +44,9 @@ func decomposeBuiltIn(f *Func) {
 		case t.IsComplex():
 			var elemType Type
 			if t.Size() == 16 {
-				elemType = f.fe.TypeFloat64()
+				elemType = f.Config.Types.Float64
 			} else {
-				elemType = f.fe.TypeFloat32()
+				elemType = f.Config.Types.Float32
 			}
 			rName, iName := f.fe.SplitComplex(name)
 			newNames = append(newNames, rName, iName)
@@ -58,8 +58,8 @@ func decomposeBuiltIn(f *Func) {
 			}
 			delete(f.NamedValues, name)
 		case t.IsString():
-			ptrType := f.fe.TypeBytePtr()
-			lenType := f.fe.TypeInt()
+			ptrType := f.Config.Types.BytePtr
+			lenType := f.Config.Types.Int
 			ptrName, lenName := f.fe.SplitString(name)
 			newNames = append(newNames, ptrName, lenName)
 			for _, v := range f.NamedValues[name] {
@@ -70,8 +70,8 @@ func decomposeBuiltIn(f *Func) {
 			}
 			delete(f.NamedValues, name)
 		case t.IsSlice():
-			ptrType := f.fe.TypeBytePtr()
-			lenType := f.fe.TypeInt()
+			ptrType := f.Config.Types.BytePtr
+			lenType := f.Config.Types.Int
 			ptrName, lenName, capName := f.fe.SplitSlice(name)
 			newNames = append(newNames, ptrName, lenName, capName)
 			for _, v := range f.NamedValues[name] {
@@ -84,7 +84,7 @@ func decomposeBuiltIn(f *Func) {
 			}
 			delete(f.NamedValues, name)
 		case t.IsInterface():
-			ptrType := f.fe.TypeBytePtr()
+			ptrType := f.Config.Types.BytePtr
 			typeName, dataName := f.fe.SplitInterface(name)
 			newNames = append(newNames, typeName, dataName)
 			for _, v := range f.NamedValues[name] {
@@ -129,9 +129,9 @@ func decomposeBuiltInPhi(v *Value) {
 }
 
 func decomposeStringPhi(v *Value) {
-	fe := v.Block.Func.fe
-	ptrType := fe.TypeBytePtr()
-	lenType := fe.TypeInt()
+	types := &v.Block.Func.Config.Types
+	ptrType := types.BytePtr
+	lenType := types.Int
 
 	ptr := v.Block.NewValue0(v.Pos, OpPhi, ptrType)
 	len := v.Block.NewValue0(v.Pos, OpPhi, lenType)
@@ -145,9 +145,9 @@ func decomposeStringPhi(v *Value) {
 }
 
 func decomposeSlicePhi(v *Value) {
-	fe := v.Block.Func.fe
-	ptrType := fe.TypeBytePtr()
-	lenType := fe.TypeInt()
+	types := &v.Block.Func.Config.Types
+	ptrType := types.BytePtr
+	lenType := types.Int
 
 	ptr := v.Block.NewValue0(v.Pos, OpPhi, ptrType)
 	len := v.Block.NewValue0(v.Pos, OpPhi, lenType)
@@ -164,19 +164,19 @@ func decomposeSlicePhi(v *Value) {
 }
 
 func decomposeInt64Phi(v *Value) {
-	fe := v.Block.Func.fe
+	types := &v.Block.Func.Config.Types
 	var partType Type
 	if v.Type.IsSigned() {
-		partType = fe.TypeInt32()
+		partType = types.Int32
 	} else {
-		partType = fe.TypeUInt32()
+		partType = types.UInt32
 	}
 
 	hi := v.Block.NewValue0(v.Pos, OpPhi, partType)
-	lo := v.Block.NewValue0(v.Pos, OpPhi, fe.TypeUInt32())
+	lo := v.Block.NewValue0(v.Pos, OpPhi, types.UInt32)
 	for _, a := range v.Args {
 		hi.AddArg(a.Block.NewValue1(v.Pos, OpInt64Hi, partType, a))
-		lo.AddArg(a.Block.NewValue1(v.Pos, OpInt64Lo, fe.TypeUInt32(), a))
+		lo.AddArg(a.Block.NewValue1(v.Pos, OpInt64Lo, types.UInt32, a))
 	}
 	v.reset(OpInt64Make)
 	v.AddArg(hi)
@@ -184,13 +184,13 @@ func decomposeInt64Phi(v *Value) {
 }
 
 func decomposeComplexPhi(v *Value) {
-	fe := v.Block.Func.fe
+	types := &v.Block.Func.Config.Types
 	var partType Type
 	switch z := v.Type.Size(); z {
 	case 8:
-		partType = fe.TypeFloat32()
+		partType = types.Float32
 	case 16:
-		partType = fe.TypeFloat64()
+		partType = types.Float64
 	default:
 		v.Fatalf("decomposeComplexPhi: bad complex size %d", z)
 	}
@@ -207,7 +207,7 @@ func decomposeComplexPhi(v *Value) {
 }
 
 func decomposeInterfacePhi(v *Value) {
-	ptrType := v.Block.Func.fe.TypeBytePtr()
+	ptrType := v.Block.Func.Config.Types.BytePtr
 
 	itab := v.Block.NewValue0(v.Pos, OpPhi, ptrType)
 	data := v.Block.NewValue0(v.Pos, OpPhi, ptrType)
diff --git a/src/cmd/compile/internal/ssa/export_test.go b/src/cmd/compile/internal/ssa/export_test.go
index eeb1fceb7a..b04702d279 100644
--- a/src/cmd/compile/internal/ssa/export_test.go
+++ b/src/cmd/compile/internal/ssa/export_test.go
@@ -19,11 +19,11 @@ var Copyelim = copyelim
 var TestCtxt = obj.Linknew(&x86.Linkamd64)
 
 func testConfig(t testing.TB) *Config {
-	return NewConfig("amd64", TestCtxt, true)
+	return NewConfig("amd64", dummyTypes, TestCtxt, true)
 }
 
 func testConfigS390X(t testing.TB) *Config {
-	return NewConfig("s390x", obj.Linknew(&s390x.Links390x), true)
+	return NewConfig("s390x", dummyTypes, obj.Linknew(&s390x.Links390x), true)
 }
 
 // DummyFrontend is a test-only frontend.
@@ -52,27 +52,27 @@ func (DummyFrontend) Auto(t Type) GCNode {
 	return &DummyAuto{t: t, s: "aDummyAuto"}
 }
 func (d DummyFrontend) SplitString(s LocalSlot) (LocalSlot, LocalSlot) {
-	return LocalSlot{s.N, d.TypeBytePtr(), s.Off}, LocalSlot{s.N, d.TypeInt(), s.Off + 8}
+	return LocalSlot{s.N, dummyTypes.BytePtr, s.Off}, LocalSlot{s.N, dummyTypes.Int, s.Off + 8}
 }
 func (d DummyFrontend) SplitInterface(s LocalSlot) (LocalSlot, LocalSlot) {
-	return LocalSlot{s.N, d.TypeBytePtr(), s.Off}, LocalSlot{s.N, d.TypeBytePtr(), s.Off + 8}
+	return LocalSlot{s.N, dummyTypes.BytePtr, s.Off}, LocalSlot{s.N, dummyTypes.BytePtr, s.Off + 8}
 }
 func (d DummyFrontend) SplitSlice(s LocalSlot) (LocalSlot, LocalSlot, LocalSlot) {
 	return LocalSlot{s.N, s.Type.ElemType().PtrTo(), s.Off},
-		LocalSlot{s.N, d.TypeInt(), s.Off + 8},
-		LocalSlot{s.N, d.TypeInt(), s.Off + 16}
+		LocalSlot{s.N, dummyTypes.Int, s.Off + 8},
+		LocalSlot{s.N, dummyTypes.Int, s.Off + 16}
 }
 func (d DummyFrontend) SplitComplex(s LocalSlot) (LocalSlot, LocalSlot) {
 	if s.Type.Size() == 16 {
-		return LocalSlot{s.N, d.TypeFloat64(), s.Off}, LocalSlot{s.N, d.TypeFloat64(), s.Off + 8}
+		return LocalSlot{s.N, dummyTypes.Float64, s.Off}, LocalSlot{s.N, dummyTypes.Float64, s.Off + 8}
 	}
-	return LocalSlot{s.N, d.TypeFloat32(), s.Off}, LocalSlot{s.N, d.TypeFloat32(), s.Off + 4}
+	return LocalSlot{s.N, dummyTypes.Float32, s.Off}, LocalSlot{s.N, dummyTypes.Float32, s.Off + 4}
 }
 func (d DummyFrontend) SplitInt64(s LocalSlot) (LocalSlot, LocalSlot) {
 	if s.Type.IsSigned() {
-		return LocalSlot{s.N, d.TypeInt32(), s.Off + 4}, LocalSlot{s.N, d.TypeUInt32(), s.Off}
+		return LocalSlot{s.N, dummyTypes.Int32, s.Off + 4}, LocalSlot{s.N, dummyTypes.UInt32, s.Off}
 	}
-	return LocalSlot{s.N, d.TypeUInt32(), s.Off + 4}, LocalSlot{s.N, d.TypeUInt32(), s.Off}
+	return LocalSlot{s.N, dummyTypes.UInt32, s.Off + 4}, LocalSlot{s.N, dummyTypes.UInt32, s.Off}
 }
 func (d DummyFrontend) SplitStruct(s LocalSlot, i int) LocalSlot {
 	return LocalSlot{s.N, s.Type.FieldType(i), s.Off + s.Type.FieldOff(i)}
@@ -101,21 +101,24 @@ func (d DummyFrontend) Warnl(_ src.XPos, msg string, args ...interface{})  { d.t
 func (d DummyFrontend) Debug_checknil() bool                               { return false }
 func (d DummyFrontend) Debug_wb() bool                                     { return false }
 
-func (d DummyFrontend) TypeBool() Type                               { return TypeBool }
-func (d DummyFrontend) TypeInt8() Type                               { return TypeInt8 }
-func (d DummyFrontend) TypeInt16() Type                              { return TypeInt16 }
-func (d DummyFrontend) TypeInt32() Type                              { return TypeInt32 }
-func (d DummyFrontend) TypeInt64() Type                              { return TypeInt64 }
-func (d DummyFrontend) TypeUInt8() Type                              { return TypeUInt8 }
-func (d DummyFrontend) TypeUInt16() Type                             { return TypeUInt16 }
-func (d DummyFrontend) TypeUInt32() Type                             { return TypeUInt32 }
-func (d DummyFrontend) TypeUInt64() Type                             { return TypeUInt64 }
-func (d DummyFrontend) TypeFloat32() Type                            { return TypeFloat32 }
-func (d DummyFrontend) TypeFloat64() Type                            { return TypeFloat64 }
-func (d DummyFrontend) TypeInt() Type                                { return TypeInt64 }
-func (d DummyFrontend) TypeUintptr() Type                            { return TypeUInt64 }
-func (d DummyFrontend) TypeString() Type                             { panic("unimplemented") }
-func (d DummyFrontend) TypeBytePtr() Type                            { return TypeBytePtr }
+var dummyTypes = Types{
+	Bool:    TypeBool,
+	Int8:    TypeInt8,
+	Int16:   TypeInt16,
+	Int32:   TypeInt32,
+	Int64:   TypeInt64,
+	UInt8:   TypeUInt8,
+	UInt16:  TypeUInt16,
+	UInt32:  TypeUInt32,
+	UInt64:  TypeUInt64,
+	Float32: TypeFloat32,
+	Float64: TypeFloat64,
+	Int:     TypeInt64,
+	Uintptr: TypeUInt64,
+	String:  nil,
+	BytePtr: TypeBytePtr,
+}
+
 func (d DummyFrontend) DerefItab(sym *obj.LSym, off int64) *obj.LSym { return nil }
 
 func (d DummyFrontend) CanSSA(t Type) bool {
diff --git a/src/cmd/compile/internal/ssa/gen/386.rules b/src/cmd/compile/internal/ssa/gen/386.rules
index 4eb7720fd3..13d9bb935f 100644
--- a/src/cmd/compile/internal/ssa/gen/386.rules
+++ b/src/cmd/compile/internal/ssa/gen/386.rules
@@ -68,8 +68,8 @@
 (Neg32  x) -> (NEGL x)
 (Neg16  x) -> (NEGL x)
 (Neg8   x) -> (NEGL x)
-(Neg32F x) && !config.use387 -> (PXOR x (MOVSSconst <fe.TypeFloat32()> [f2i(math.Copysign(0, -1))]))
-(Neg64F x) && !config.use387 -> (PXOR x (MOVSDconst <fe.TypeFloat64()> [f2i(math.Copysign(0, -1))]))
+(Neg32F x) && !config.use387 -> (PXOR x (MOVSSconst <types.Float32> [f2i(math.Copysign(0, -1))]))
+(Neg64F x) && !config.use387 -> (PXOR x (MOVSDconst <types.Float64> [f2i(math.Copysign(0, -1))]))
 (Neg32F x) && config.use387 -> (FCHS x)
 (Neg64F x) && config.use387 -> (FCHS x)
 
diff --git a/src/cmd/compile/internal/ssa/gen/AMD64.rules b/src/cmd/compile/internal/ssa/gen/AMD64.rules
index 9f45154ad3..a1d5b7f2a3 100644
--- a/src/cmd/compile/internal/ssa/gen/AMD64.rules
+++ b/src/cmd/compile/internal/ssa/gen/AMD64.rules
@@ -78,8 +78,8 @@
 (Neg32  x) -> (NEGL x)
 (Neg16  x) -> (NEGL x)
 (Neg8   x) -> (NEGL x)
-(Neg32F x) -> (PXOR x (MOVSSconst <fe.TypeFloat32()> [f2i(math.Copysign(0, -1))]))
-(Neg64F x) -> (PXOR x (MOVSDconst <fe.TypeFloat64()> [f2i(math.Copysign(0, -1))]))
+(Neg32F x) -> (PXOR x (MOVSSconst <types.Float32> [f2i(math.Copysign(0, -1))]))
+(Neg64F x) -> (PXOR x (MOVSDconst <types.Float64> [f2i(math.Copysign(0, -1))]))
 
 (Com64 x) -> (NOTQ x)
 (Com32 x) -> (NOTL x)
@@ -98,10 +98,10 @@
 
 // Lowering other arithmetic
 (Ctz64 <t> x) -> (CMOVQEQ (Select0 <t> (BSFQ x)) (MOVQconst <t> [64]) (Select1 <TypeFlags> (BSFQ x)))
-(Ctz32 x) -> (Select0 (BSFQ (ORQ <fe.TypeUInt64()> (MOVQconst [1<<32]) x)))
+(Ctz32 x) -> (Select0 (BSFQ (ORQ <types.UInt64> (MOVQconst [1<<32]) x)))
 
 (BitLen64 <t> x) -> (ADDQconst [1] (CMOVQEQ <t> (Select0 <t> (BSRQ x)) (MOVQconst <t> [-1]) (Select1 <TypeFlags> (BSRQ x))))
-(BitLen32 x) -> (BitLen64 (MOVLQZX <fe.TypeUInt64()> x))
+(BitLen32 x) -> (BitLen64 (MOVLQZX <types.UInt64> x))
 
 (Bswap64 x) -> (BSWAPQ x)
 (Bswap32 x) -> (BSWAPL x)
@@ -472,10 +472,10 @@
 
 // Atomic stores.  We use XCHG to prevent the hardware reordering a subsequent load.
 // TODO: most runtime uses of atomic stores don't need that property.  Use normal stores for those?
-(AtomicStore32 ptr val mem) -> (Select1 (XCHGL <MakeTuple(fe.TypeUInt32(),TypeMem)> val ptr mem))
-(AtomicStore64 ptr val mem) -> (Select1 (XCHGQ <MakeTuple(fe.TypeUInt64(),TypeMem)> val ptr mem))
-(AtomicStorePtrNoWB ptr val mem) && config.PtrSize == 8 -> (Select1 (XCHGQ <MakeTuple(fe.TypeBytePtr(),TypeMem)> val ptr mem))
-(AtomicStorePtrNoWB ptr val mem) && config.PtrSize == 4 -> (Select1 (XCHGL <MakeTuple(fe.TypeBytePtr(),TypeMem)> val ptr mem))
+(AtomicStore32 ptr val mem) -> (Select1 (XCHGL <MakeTuple(types.UInt32,TypeMem)> val ptr mem))
+(AtomicStore64 ptr val mem) -> (Select1 (XCHGQ <MakeTuple(types.UInt64,TypeMem)> val ptr mem))
+(AtomicStorePtrNoWB ptr val mem) && config.PtrSize == 8 -> (Select1 (XCHGQ <MakeTuple(types.BytePtr,TypeMem)> val ptr mem))
+(AtomicStorePtrNoWB ptr val mem) && config.PtrSize == 4 -> (Select1 (XCHGL <MakeTuple(types.BytePtr,TypeMem)> val ptr mem))
 
 // Atomic exchanges.
 (AtomicExchange32 ptr val mem) -> (XCHGL val ptr mem)
@@ -553,8 +553,8 @@
 (NE (TESTB (SETNEF cmp) (SETNEF cmp)) yes no) -> (NEF  cmp yes no)
 
 // Disabled because it interferes with the pattern match above and makes worse code.
-// (SETNEF x) -> (ORQ (SETNE <fe.TypeInt8()> x) (SETNAN <fe.TypeInt8()> x))
-// (SETEQF x) -> (ANDQ (SETEQ <fe.TypeInt8()> x) (SETORD <fe.TypeInt8()> x))
+// (SETNEF x) -> (ORQ (SETNE <types.Int8> x) (SETNAN <types.Int8> x))
+// (SETEQF x) -> (ANDQ (SETEQ <types.Int8> x) (SETORD <types.Int8> x))
 
 // fold constants into instructions
 (ADDQ x (MOVQconst [c])) && is32Bit(c) -> (ADDQconst [c] x)
diff --git a/src/cmd/compile/internal/ssa/gen/ARM.rules b/src/cmd/compile/internal/ssa/gen/ARM.rules
index 4d37f63989..5ab695c76b 100644
--- a/src/cmd/compile/internal/ssa/gen/ARM.rules
+++ b/src/cmd/compile/internal/ssa/gen/ARM.rules
@@ -34,12 +34,12 @@
 (Mul32uhilo x y) -> (MULLU x y)
 
 (Div32 x y) ->
-	(SUB (XOR <fe.TypeUInt32()>                                                                  // negate the result if one operand is negative
-		(Select0 <fe.TypeUInt32()> (CALLudiv {config.ctxt.Lookup("udiv", 0)}
-			(SUB <fe.TypeUInt32()> (XOR x <fe.TypeUInt32()> (Signmask x)) (Signmask x))   // negate x if negative
-			(SUB <fe.TypeUInt32()> (XOR y <fe.TypeUInt32()> (Signmask y)) (Signmask y)))) // negate y if negative
-		(Signmask (XOR <fe.TypeUInt32()> x y))) (Signmask (XOR <fe.TypeUInt32()> x y)))
-(Div32u x y) -> (Select0 <fe.TypeUInt32()> (CALLudiv {config.ctxt.Lookup("udiv", 0)} x y))
+	(SUB (XOR <types.UInt32>                                                                  // negate the result if one operand is negative
+		(Select0 <types.UInt32> (CALLudiv {config.ctxt.Lookup("udiv", 0)}
+			(SUB <types.UInt32> (XOR x <types.UInt32> (Signmask x)) (Signmask x))   // negate x if negative
+			(SUB <types.UInt32> (XOR y <types.UInt32> (Signmask y)) (Signmask y)))) // negate y if negative
+		(Signmask (XOR <types.UInt32> x y))) (Signmask (XOR <types.UInt32> x y)))
+(Div32u x y) -> (Select0 <types.UInt32> (CALLudiv {config.ctxt.Lookup("udiv", 0)} x y))
 (Div16 x y) -> (Div32 (SignExt16to32 x) (SignExt16to32 y))
 (Div16u x y) -> (Div32u (ZeroExt16to32 x) (ZeroExt16to32 y))
 (Div8 x y) -> (Div32 (SignExt8to32 x) (SignExt8to32 y))
@@ -48,12 +48,12 @@
 (Div64F x y) -> (DIVD x y)
 
 (Mod32 x y) ->
-	(SUB (XOR <fe.TypeUInt32()>                                                                  // negate the result if x is negative
-		(Select1 <fe.TypeUInt32()> (CALLudiv {config.ctxt.Lookup("udiv", 0)}
-			(SUB <fe.TypeUInt32()> (XOR <fe.TypeUInt32()> x (Signmask x)) (Signmask x))   // negate x if negative
-			(SUB <fe.TypeUInt32()> (XOR <fe.TypeUInt32()> y (Signmask y)) (Signmask y)))) // negate y if negative
+	(SUB (XOR <types.UInt32>                                                                  // negate the result if x is negative
+		(Select1 <types.UInt32> (CALLudiv {config.ctxt.Lookup("udiv", 0)}
+			(SUB <types.UInt32> (XOR <types.UInt32> x (Signmask x)) (Signmask x))   // negate x if negative
+			(SUB <types.UInt32> (XOR <types.UInt32> y (Signmask y)) (Signmask y)))) // negate y if negative
 		(Signmask x)) (Signmask x))
-(Mod32u x y) -> (Select1 <fe.TypeUInt32()> (CALLudiv {config.ctxt.Lookup("udiv", 0)} x y))
+(Mod32u x y) -> (Select1 <types.UInt32> (CALLudiv {config.ctxt.Lookup("udiv", 0)} x y))
 (Mod16 x y) -> (Mod32 (SignExt16to32 x) (SignExt16to32 y))
 (Mod16u x y) -> (Mod32u (ZeroExt16to32 x) (ZeroExt16to32 y))
 (Mod8 x y) -> (Mod32 (SignExt8to32 x) (SignExt8to32 y))
@@ -111,7 +111,7 @@
 // boolean ops -- booleans are represented with 0=false, 1=true
 (AndB x y) -> (AND x y)
 (OrB x y) -> (OR x y)
-(EqB x y) -> (XORconst [1] (XOR <fe.TypeBool()> x y))
+(EqB x y) -> (XORconst [1] (XOR <types.Bool> x y))
 (NeqB x y) -> (XOR x y)
 (Not x) -> (XORconst [1] x)
 
@@ -160,11 +160,11 @@
 (Rsh32x64 x (Const64 [c])) && uint64(c) < 32 -> (SRAconst x [c])
 (Rsh32Ux64 x (Const64 [c])) && uint64(c) < 32 -> (SRLconst x [c])
 (Lsh16x64 x (Const64 [c])) && uint64(c) < 16 -> (SLLconst x [c])
-(Rsh16x64 x (Const64 [c])) && uint64(c) < 16 -> (SRAconst (SLLconst <fe.TypeUInt32()> x [16]) [c+16])
-(Rsh16Ux64 x (Const64 [c])) && uint64(c) < 16 -> (SRLconst (SLLconst <fe.TypeUInt32()> x [16]) [c+16])
+(Rsh16x64 x (Const64 [c])) && uint64(c) < 16 -> (SRAconst (SLLconst <types.UInt32> x [16]) [c+16])
+(Rsh16Ux64 x (Const64 [c])) && uint64(c) < 16 -> (SRLconst (SLLconst <types.UInt32> x [16]) [c+16])
 (Lsh8x64 x (Const64 [c])) && uint64(c) < 8 -> (SLLconst x [c])
-(Rsh8x64 x (Const64 [c])) && uint64(c) < 8 -> (SRAconst (SLLconst <fe.TypeUInt32()> x [24]) [c+24])
-(Rsh8Ux64 x (Const64 [c])) && uint64(c) < 8 -> (SRLconst (SLLconst <fe.TypeUInt32()> x [24]) [c+24])
+(Rsh8x64 x (Const64 [c])) && uint64(c) < 8 -> (SRAconst (SLLconst <types.UInt32> x [24]) [c+24])
+(Rsh8Ux64 x (Const64 [c])) && uint64(c) < 8 -> (SRLconst (SLLconst <types.UInt32> x [24]) [c+24])
 
 // large constant shifts
 (Lsh32x64 _ (Const64 [c])) && uint64(c) >= 32 -> (Const32 [0])
@@ -176,8 +176,8 @@
 
 // large constant signed right shift, we leave the sign bit
 (Rsh32x64 x (Const64 [c])) && uint64(c) >= 32 -> (SRAconst x [31])
-(Rsh16x64 x (Const64 [c])) && uint64(c) >= 16 -> (SRAconst (SLLconst <fe.TypeUInt32()> x [16]) [31])
-(Rsh8x64 x (Const64 [c])) && uint64(c) >= 8 -> (SRAconst (SLLconst <fe.TypeUInt32()> x [24]) [31])
+(Rsh16x64 x (Const64 [c])) && uint64(c) >= 16 -> (SRAconst (SLLconst <types.UInt32> x [16]) [31])
+(Rsh8x64 x (Const64 [c])) && uint64(c) >= 8 -> (SRAconst (SLLconst <types.UInt32> x [24]) [31])
 
 // constants
 (Const8 [val]) -> (MOVWconst [val])
@@ -204,7 +204,7 @@
 (SignExt16to32 x) -> (MOVHreg x)
 
 (Signmask x) -> (SRAconst x [31])
-(Zeromask x) -> (SRAconst (RSBshiftRL <fe.TypeInt32()> x x [1]) [31]) // sign bit of uint32(x)>>1 - x
+(Zeromask x) -> (SRAconst (RSBshiftRL <types.Int32> x x [1]) [31]) // sign bit of uint32(x)>>1 - x
 (Slicemask <t> x) -> (SRAconst (RSBconst <t> [0] x) [31])
 
 // float <-> int conversion
diff --git a/src/cmd/compile/internal/ssa/gen/ARM64.rules b/src/cmd/compile/internal/ssa/gen/ARM64.rules
index d1bb9ae624..41661082c7 100644
--- a/src/cmd/compile/internal/ssa/gen/ARM64.rules
+++ b/src/cmd/compile/internal/ssa/gen/ARM64.rules
@@ -27,8 +27,8 @@
 
 (Hmul64 x y) -> (MULH x y)
 (Hmul64u x y) -> (UMULH x y)
-(Hmul32 x y) -> (SRAconst (MULL <fe.TypeInt64()> x y) [32])
-(Hmul32u x y) -> (SRAconst (UMULL <fe.TypeUInt64()> x y) [32])
+(Hmul32 x y) -> (SRAconst (MULL <types.Int64> x y) [32])
+(Hmul32u x y) -> (SRAconst (UMULL <types.UInt64> x y) [32])
 
 (Div64 x y) -> (DIV x y)
 (Div64u x y) -> (UDIV x y)
@@ -86,20 +86,20 @@
 (Ctz64 <t> x) -> (CLZ (RBIT <t> x))
 (Ctz32 <t> x) -> (CLZW (RBITW <t> x))
 
-(BitLen64 x) -> (SUB (MOVDconst [64]) (CLZ <fe.TypeInt()> x))
+(BitLen64 x) -> (SUB (MOVDconst [64]) (CLZ <types.Int> x))
 
 (Bswap64 x) -> (REV x)
 (Bswap32 x) -> (REVW x)
 
 (BitRev64 x) -> (RBIT x)
 (BitRev32 x) -> (RBITW x)
-(BitRev16 x) -> (SRLconst [48] (RBIT <fe.TypeUInt64()> x))
-(BitRev8 x) -> (SRLconst [56] (RBIT <fe.TypeUInt64()> x))
+(BitRev16 x) -> (SRLconst [48] (RBIT <types.UInt64> x))
+(BitRev8 x) -> (SRLconst [56] (RBIT <types.UInt64> x))
 
 // boolean ops -- booleans are represented with 0=false, 1=true
 (AndB x y) -> (AND x y)
 (OrB x y) -> (OR x y)
-(EqB x y) -> (XOR (MOVDconst [1]) (XOR <fe.TypeBool()> x y))
+(EqB x y) -> (XOR (MOVDconst [1]) (XOR <types.Bool> x y))
 (NeqB x y) -> (XOR x y)
 (Not x) -> (XOR (MOVDconst [1]) x)
 
diff --git a/src/cmd/compile/internal/ssa/gen/MIPS.rules b/src/cmd/compile/internal/ssa/gen/MIPS.rules
index d210d5e60c..f1ece56474 100644
--- a/src/cmd/compile/internal/ssa/gen/MIPS.rules
+++ b/src/cmd/compile/internal/ssa/gen/MIPS.rules
@@ -10,7 +10,7 @@
 (Add64F x y) -> (ADDD x y)
 
 (Select0 (Add32carry <t> x y)) -> (ADD <t.FieldType(0)> x y)
-(Select1 (Add32carry <t> x y)) -> (SGTU <fe.TypeBool()> x (ADD <t.FieldType(0)> x y))
+(Select1 (Add32carry <t> x y)) -> (SGTU <types.Bool> x (ADD <t.FieldType(0)> x y))
 (Add32withcarry <t> x y c) -> (ADD c (ADD <t> x y))
 
 (SubPtr x y) -> (SUB x y)
@@ -21,7 +21,7 @@
 (Sub64F x y) -> (SUBD x y)
 
 (Select0 (Sub32carry <t> x y)) -> (SUB <t.FieldType(0)> x y)
-(Select1 (Sub32carry <t> x y)) -> (SGTU <fe.TypeBool()> (SUB <t.FieldType(0)> x y) x)
+(Select1 (Sub32carry <t> x y)) -> (SGTU <types.Bool> (SUB <t.FieldType(0)> x y) x)
 (Sub32withcarry <t> x y c) -> (SUB (SUB <t> x y) c)
 
 (Mul32 x y) -> (MUL x y)
@@ -72,11 +72,11 @@
 (Rsh32x64 x (Const64 [c])) && uint32(c) < 32 -> (SRAconst x [c])
 (Rsh32Ux64 x (Const64 [c])) && uint32(c) < 32 -> (SRLconst x [c])
 (Lsh16x64 x (Const64 [c])) && uint32(c) < 16 -> (SLLconst x [c])
-(Rsh16x64 x (Const64 [c])) && uint32(c) < 16 -> (SRAconst (SLLconst <fe.TypeUInt32()> x [16]) [c+16])
-(Rsh16Ux64 x (Const64 [c])) && uint32(c) < 16 -> (SRLconst (SLLconst <fe.TypeUInt32()> x [16]) [c+16])
+(Rsh16x64 x (Const64 [c])) && uint32(c) < 16 -> (SRAconst (SLLconst <types.UInt32> x [16]) [c+16])
+(Rsh16Ux64 x (Const64 [c])) && uint32(c) < 16 -> (SRLconst (SLLconst <types.UInt32> x [16]) [c+16])
 (Lsh8x64 x (Const64 [c])) && uint32(c) < 8 -> (SLLconst x [c])
-(Rsh8x64 x (Const64 [c])) && uint32(c) < 8 -> (SRAconst (SLLconst <fe.TypeUInt32()> x [24]) [c+24])
-(Rsh8Ux64 x (Const64 [c])) && uint32(c) < 8 -> (SRLconst (SLLconst <fe.TypeUInt32()> x [24]) [c+24])
+(Rsh8x64 x (Const64 [c])) && uint32(c) < 8 -> (SRAconst (SLLconst <types.UInt32> x [24]) [c+24])
+(Rsh8Ux64 x (Const64 [c])) && uint32(c) < 8 -> (SRLconst (SLLconst <types.UInt32> x [24]) [c+24])
 
 // large constant shifts
 (Lsh32x64 _ (Const64 [c])) && uint32(c) >= 32 -> (MOVWconst [0])
@@ -88,8 +88,8 @@
 
 // large constant signed right shift, we leave the sign bit
 (Rsh32x64 x (Const64 [c])) && uint32(c) >= 32 -> (SRAconst x [31])
-(Rsh16x64 x (Const64 [c])) && uint32(c) >= 16 -> (SRAconst (SLLconst <fe.TypeUInt32()> x [16]) [31])
-(Rsh8x64 x (Const64 [c])) && uint32(c) >= 8 -> (SRAconst (SLLconst <fe.TypeUInt32()> x [24]) [31])
+(Rsh16x64 x (Const64 [c])) && uint32(c) >= 16 -> (SRAconst (SLLconst <types.UInt32> x [16]) [31])
+(Rsh8x64 x (Const64 [c])) && uint32(c) >= 8 -> (SRAconst (SLLconst <types.UInt32> x [24]) [31])
 
 // shifts
 // hardware instruction uses only the low 5 bits of the shift
@@ -118,17 +118,17 @@
 (Rsh8Ux16 <t> x y) -> (CMOVZ (SRL <t> (ZeroExt8to32 x) (ZeroExt16to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt16to32 y)))
 (Rsh8Ux8 <t> x y) -> (CMOVZ (SRL <t> (ZeroExt8to32 x) (ZeroExt8to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt8to32 y)))
 
-(Rsh32x32 x y) -> (SRA x ( CMOVZ <fe.TypeUInt32()> y (MOVWconst [-1]) (SGTUconst [32] y)))
-(Rsh32x16 x y) -> (SRA x ( CMOVZ <fe.TypeUInt32()> (ZeroExt16to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt16to32 y))))
-(Rsh32x8 x y) -> (SRA x ( CMOVZ <fe.TypeUInt32()> (ZeroExt8to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt8to32 y))))
+(Rsh32x32 x y) -> (SRA x ( CMOVZ <types.UInt32> y (MOVWconst [-1]) (SGTUconst [32] y)))
+(Rsh32x16 x y) -> (SRA x ( CMOVZ <types.UInt32> (ZeroExt16to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt16to32 y))))
+(Rsh32x8 x y) -> (SRA x ( CMOVZ <types.UInt32> (ZeroExt8to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt8to32 y))))
 
-(Rsh16x32 x y) -> (SRA (SignExt16to32 x) ( CMOVZ <fe.TypeUInt32()> y (MOVWconst [-1]) (SGTUconst [32] y)))
-(Rsh16x16 x y) -> (SRA (SignExt16to32 x) ( CMOVZ <fe.TypeUInt32()> (ZeroExt16to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt16to32 y))))
-(Rsh16x8 x y) -> (SRA (SignExt16to32 x) ( CMOVZ <fe.TypeUInt32()> (ZeroExt8to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt8to32 y))))
+(Rsh16x32 x y) -> (SRA (SignExt16to32 x) ( CMOVZ <types.UInt32> y (MOVWconst [-1]) (SGTUconst [32] y)))
+(Rsh16x16 x y) -> (SRA (SignExt16to32 x) ( CMOVZ <types.UInt32> (ZeroExt16to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt16to32 y))))
+(Rsh16x8 x y) -> (SRA (SignExt16to32 x) ( CMOVZ <types.UInt32> (ZeroExt8to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt8to32 y))))
 
-(Rsh8x32 x y) -> (SRA (SignExt16to32 x) ( CMOVZ <fe.TypeUInt32()> y (MOVWconst [-1]) (SGTUconst [32] y)))
-(Rsh8x16 x y) -> (SRA (SignExt16to32 x) ( CMOVZ <fe.TypeUInt32()> (ZeroExt16to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt16to32 y))))
-(Rsh8x8 x y) -> (SRA (SignExt16to32 x) ( CMOVZ <fe.TypeUInt32()> (ZeroExt8to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt8to32 y))))
+(Rsh8x32 x y) -> (SRA (SignExt16to32 x) ( CMOVZ <types.UInt32> y (MOVWconst [-1]) (SGTUconst [32] y)))
+(Rsh8x16 x y) -> (SRA (SignExt16to32 x) ( CMOVZ <types.UInt32> (ZeroExt16to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt16to32 y))))
+(Rsh8x8 x y) -> (SRA (SignExt16to32 x) ( CMOVZ <types.UInt32> (ZeroExt8to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt8to32 y))))
 
 // unary ops
 (Neg32 x) -> (NEG x)
@@ -153,7 +153,7 @@
 // boolean ops -- booleans are represented with 0=false, 1=true
 (AndB x y) -> (AND x y)
 (OrB x y) -> (OR x y)
-(EqB x y) -> (XORconst [1] (XOR <fe.TypeBool()> x y))
+(EqB x y) -> (XORconst [1] (XOR <types.Bool> x y))
 (NeqB x y) -> (XOR x y)
 (Not x) -> (XORconst [1] x)
 
@@ -393,41 +393,41 @@
 
 // AtomicOr8(ptr,val) -> LoweredAtomicOr(ptr&^3,uint32(val) << ((ptr & 3) * 8))
 (AtomicOr8 ptr val mem) && !config.BigEndian ->
-	(LoweredAtomicOr (AND <fe.TypeUInt32().PtrTo()> (MOVWconst [^3]) ptr)
-		(SLL <fe.TypeUInt32()> (ZeroExt8to32 val)
-			(SLLconst <fe.TypeUInt32()> [3]
-				(ANDconst <fe.TypeUInt32()> [3] ptr))) mem)
+	(LoweredAtomicOr (AND <types.UInt32.PtrTo()> (MOVWconst [^3]) ptr)
+		(SLL <types.UInt32> (ZeroExt8to32 val)
+			(SLLconst <types.UInt32> [3]
+				(ANDconst <types.UInt32> [3] ptr))) mem)
 
 // AtomicAnd8(ptr,val) -> LoweredAtomicAnd(ptr&^3,(uint32(val) << ((ptr & 3) * 8)) | ^(uint32(0xFF) << ((ptr & 3) * 8))))
 (AtomicAnd8  ptr val mem) && !config.BigEndian ->
-	(LoweredAtomicAnd (AND <fe.TypeUInt32().PtrTo()> (MOVWconst [^3]) ptr)
-		(OR <fe.TypeUInt32()> (SLL <fe.TypeUInt32()> (ZeroExt8to32 val)
-			(SLLconst <fe.TypeUInt32()> [3]
-				(ANDconst  <fe.TypeUInt32()> [3] ptr)))
-		(NORconst [0] <fe.TypeUInt32()> (SLL <fe.TypeUInt32()>
-			(MOVWconst [0xff]) (SLLconst <fe.TypeUInt32()> [3]
-				(ANDconst <fe.TypeUInt32()> [3]
-					(XORconst <fe.TypeUInt32()> [3] ptr)))))) mem)
+	(LoweredAtomicAnd (AND <types.UInt32.PtrTo()> (MOVWconst [^3]) ptr)
+		(OR <types.UInt32> (SLL <types.UInt32> (ZeroExt8to32 val)
+			(SLLconst <types.UInt32> [3]
+				(ANDconst  <types.UInt32> [3] ptr)))
+		(NORconst [0] <types.UInt32> (SLL <types.UInt32>
+			(MOVWconst [0xff]) (SLLconst <types.UInt32> [3]
+				(ANDconst <types.UInt32> [3]
+					(XORconst <types.UInt32> [3] ptr)))))) mem)
 
 // AtomicOr8(ptr,val) -> LoweredAtomicOr(ptr&^3,uint32(val) << (((ptr^3) & 3) * 8))
 (AtomicOr8 ptr val mem) && config.BigEndian ->
-	(LoweredAtomicOr (AND <fe.TypeUInt32().PtrTo()> (MOVWconst [^3]) ptr)
-		(SLL <fe.TypeUInt32()> (ZeroExt8to32 val)
-			(SLLconst <fe.TypeUInt32()> [3]
-				(ANDconst <fe.TypeUInt32()> [3]
-					(XORconst <fe.TypeUInt32()> [3] ptr)))) mem)
+	(LoweredAtomicOr (AND <types.UInt32.PtrTo()> (MOVWconst [^3]) ptr)
+		(SLL <types.UInt32> (ZeroExt8to32 val)
+			(SLLconst <types.UInt32> [3]
+				(ANDconst <types.UInt32> [3]
+					(XORconst <types.UInt32> [3] ptr)))) mem)
 
 // AtomicAnd8(ptr,val) -> LoweredAtomicAnd(ptr&^3,(uint32(val) << (((ptr^3) & 3) * 8)) | ^(uint32(0xFF) << (((ptr^3) & 3) * 8))))
 (AtomicAnd8  ptr val mem) && config.BigEndian ->
-	(LoweredAtomicAnd (AND <fe.TypeUInt32().PtrTo()> (MOVWconst [^3]) ptr)
-		(OR <fe.TypeUInt32()> (SLL <fe.TypeUInt32()> (ZeroExt8to32 val)
-			(SLLconst <fe.TypeUInt32()> [3]
-				(ANDconst  <fe.TypeUInt32()> [3]
-					(XORconst <fe.TypeUInt32()> [3] ptr))))
-		(NORconst [0] <fe.TypeUInt32()> (SLL <fe.TypeUInt32()>
-			(MOVWconst [0xff]) (SLLconst <fe.TypeUInt32()> [3]
-				(ANDconst <fe.TypeUInt32()> [3]
-					(XORconst <fe.TypeUInt32()> [3] ptr)))))) mem)
+	(LoweredAtomicAnd (AND <types.UInt32.PtrTo()> (MOVWconst [^3]) ptr)
+		(OR <types.UInt32> (SLL <types.UInt32> (ZeroExt8to32 val)
+			(SLLconst <types.UInt32> [3]
+				(ANDconst  <types.UInt32> [3]
+					(XORconst <types.UInt32> [3] ptr))))
+		(NORconst [0] <types.UInt32> (SLL <types.UInt32>
+			(MOVWconst [0xff]) (SLLconst <types.UInt32> [3]
+				(ANDconst <types.UInt32> [3]
+					(XORconst <types.UInt32> [3] ptr)))))) mem)
 
 
 // checks
diff --git a/src/cmd/compile/internal/ssa/gen/MIPS64.rules b/src/cmd/compile/internal/ssa/gen/MIPS64.rules
index a5dbcadf90..42b0dc51bb 100644
--- a/src/cmd/compile/internal/ssa/gen/MIPS64.rules
+++ b/src/cmd/compile/internal/ssa/gen/MIPS64.rules
@@ -27,8 +27,8 @@
 
 (Hmul64 x y) -> (Select0 (MULV x y))
 (Hmul64u x y) -> (Select0 (MULVU x y))
-(Hmul32 x y) -> (SRAVconst (Select1 <fe.TypeInt64()> (MULV (SignExt32to64 x) (SignExt32to64 y))) [32])
-(Hmul32u x y) -> (SRLVconst (Select1 <fe.TypeUInt64()> (MULVU (ZeroExt32to64 x) (ZeroExt32to64 y))) [32])
+(Hmul32 x y) -> (SRAVconst (Select1 <types.Int64> (MULV (SignExt32to64 x) (SignExt32to64 y))) [32])
+(Hmul32u x y) -> (SRLVconst (Select1 <types.UInt64> (MULVU (ZeroExt32to64 x) (ZeroExt32to64 y))) [32])
 
 (Div64 x y) -> (Select1 (DIVV x y))
 (Div64u x y) -> (Select1 (DIVVU x y))
@@ -71,65 +71,65 @@
 // shifts
 // hardware instruction uses only the low 6 bits of the shift
 // we compare to 64 to ensure Go semantics for large shifts
-(Lsh64x64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SLLV <t> x y))
-(Lsh64x32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
-(Lsh64x16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
-(Lsh64x8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
-
-(Lsh32x64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SLLV <t> x y))
-(Lsh32x32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
-(Lsh32x16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
-(Lsh32x8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
-
-(Lsh16x64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SLLV <t> x y))
-(Lsh16x32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
-(Lsh16x16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
-(Lsh16x8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
-
-(Lsh8x64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SLLV <t> x y))
-(Lsh8x32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
-(Lsh8x16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
-(Lsh8x8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
-
-(Rsh64Ux64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SRLV <t> x y))
-(Rsh64Ux32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SRLV <t> x (ZeroExt32to64 y)))
-(Rsh64Ux16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SRLV <t> x (ZeroExt16to64 y)))
-(Rsh64Ux8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SRLV <t> x (ZeroExt8to64  y)))
-
-(Rsh32Ux64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SRLV <t> (ZeroExt32to64 x) y))
-(Rsh32Ux32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SRLV <t> (ZeroExt32to64 x) (ZeroExt32to64 y)))
-(Rsh32Ux16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SRLV <t> (ZeroExt32to64 x) (ZeroExt16to64 y)))
-(Rsh32Ux8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SRLV <t> (ZeroExt32to64 x) (ZeroExt8to64  y)))
-
-(Rsh16Ux64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SRLV <t> (ZeroExt16to64 x) y))
-(Rsh16Ux32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SRLV <t> (ZeroExt16to64 x) (ZeroExt32to64 y)))
-(Rsh16Ux16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SRLV <t> (ZeroExt16to64 x) (ZeroExt16to64 y)))
-(Rsh16Ux8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SRLV <t> (ZeroExt16to64 x) (ZeroExt8to64  y)))
-
-(Rsh8Ux64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SRLV <t> (ZeroExt8to64 x) y))
-(Rsh8Ux32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SRLV <t> (ZeroExt8to64 x) (ZeroExt32to64 y)))
-(Rsh8Ux16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SRLV <t> (ZeroExt8to64 x) (ZeroExt16to64 y)))
-(Rsh8Ux8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SRLV <t> (ZeroExt8to64 x) (ZeroExt8to64  y)))
-
-(Rsh64x64 <t> x y) -> (SRAV x (OR <t> (NEGV <t> (SGTU y (Const64 <fe.TypeUInt64()> [63]))) y))
-(Rsh64x32 <t> x y) -> (SRAV x (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt32to64 y)))
-(Rsh64x16 <t> x y) -> (SRAV x (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt16to64 y)))
-(Rsh64x8  <t> x y) -> (SRAV x (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt8to64  y)))
-
-(Rsh32x64 <t> x y) -> (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU y (Const64 <fe.TypeUInt64()> [63]))) y))
-(Rsh32x32 <t> x y) -> (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt32to64 y)))
-(Rsh32x16 <t> x y) -> (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt16to64 y)))
-(Rsh32x8  <t> x y) -> (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt8to64  y)))
-
-(Rsh16x64 <t> x y) -> (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU y (Const64 <fe.TypeUInt64()> [63]))) y))
-(Rsh16x32 <t> x y) -> (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt32to64 y)))
-(Rsh16x16 <t> x y) -> (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt16to64 y)))
-(Rsh16x8  <t> x y) -> (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt8to64  y)))
-
-(Rsh8x64 <t> x y) -> (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU y (Const64 <fe.TypeUInt64()> [63]))) y))
-(Rsh8x32 <t> x y) -> (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt32to64 y)))
-(Rsh8x16 <t> x y) -> (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt16to64 y)))
-(Rsh8x8  <t> x y) -> (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt8to64  y)))
+(Lsh64x64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SLLV <t> x y))
+(Lsh64x32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
+(Lsh64x16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
+(Lsh64x8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
+
+(Lsh32x64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SLLV <t> x y))
+(Lsh32x32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
+(Lsh32x16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
+(Lsh32x8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
+
+(Lsh16x64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SLLV <t> x y))
+(Lsh16x32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
+(Lsh16x16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
+(Lsh16x8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
+
+(Lsh8x64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SLLV <t> x y))
+(Lsh8x32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
+(Lsh8x16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
+(Lsh8x8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
+
+(Rsh64Ux64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SRLV <t> x y))
+(Rsh64Ux32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SRLV <t> x (ZeroExt32to64 y)))
+(Rsh64Ux16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SRLV <t> x (ZeroExt16to64 y)))
+(Rsh64Ux8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SRLV <t> x (ZeroExt8to64  y)))
+
+(Rsh32Ux64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SRLV <t> (ZeroExt32to64 x) y))
+(Rsh32Ux32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SRLV <t> (ZeroExt32to64 x) (ZeroExt32to64 y)))
+(Rsh32Ux16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SRLV <t> (ZeroExt32to64 x) (ZeroExt16to64 y)))
+(Rsh32Ux8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SRLV <t> (ZeroExt32to64 x) (ZeroExt8to64  y)))
+
+(Rsh16Ux64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SRLV <t> (ZeroExt16to64 x) y))
+(Rsh16Ux32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SRLV <t> (ZeroExt16to64 x) (ZeroExt32to64 y)))
+(Rsh16Ux16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SRLV <t> (ZeroExt16to64 x) (ZeroExt16to64 y)))
+(Rsh16Ux8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SRLV <t> (ZeroExt16to64 x) (ZeroExt8to64  y)))
+
+(Rsh8Ux64 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SRLV <t> (ZeroExt8to64 x) y))
+(Rsh8Ux32 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SRLV <t> (ZeroExt8to64 x) (ZeroExt32to64 y)))
+(Rsh8Ux16 <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SRLV <t> (ZeroExt8to64 x) (ZeroExt16to64 y)))
+(Rsh8Ux8  <t> x y) -> (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SRLV <t> (ZeroExt8to64 x) (ZeroExt8to64  y)))
+
+(Rsh64x64 <t> x y) -> (SRAV x (OR <t> (NEGV <t> (SGTU y (Const64 <types.UInt64> [63]))) y))
+(Rsh64x32 <t> x y) -> (SRAV x (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt32to64 y)))
+(Rsh64x16 <t> x y) -> (SRAV x (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt16to64 y)))
+(Rsh64x8  <t> x y) -> (SRAV x (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <types.UInt64> [63]))) (ZeroExt8to64  y)))
+
+(Rsh32x64 <t> x y) -> (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU y (Const64 <types.UInt64> [63]))) y))
+(Rsh32x32 <t> x y) -> (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt32to64 y)))
+(Rsh32x16 <t> x y) -> (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt16to64 y)))
+(Rsh32x8  <t> x y) -> (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <types.UInt64> [63]))) (ZeroExt8to64  y)))
+
+(Rsh16x64 <t> x y) -> (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU y (Const64 <types.UInt64> [63]))) y))
+(Rsh16x32 <t> x y) -> (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt32to64 y)))
+(Rsh16x16 <t> x y) -> (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt16to64 y)))
+(Rsh16x8  <t> x y) -> (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <types.UInt64> [63]))) (ZeroExt8to64  y)))
+
+(Rsh8x64 <t> x y) -> (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU y (Const64 <types.UInt64> [63]))) y))
+(Rsh8x32 <t> x y) -> (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt32to64 y)))
+(Rsh8x16 <t> x y) -> (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt16to64 y)))
+(Rsh8x8  <t> x y) -> (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <types.UInt64> [63]))) (ZeroExt8to64  y)))
 
 // unary ops
 (Neg64 x) -> (NEGV x)
@@ -147,7 +147,7 @@
 // boolean ops -- booleans are represented with 0=false, 1=true
 (AndB x y) -> (AND x y)
 (OrB x y) -> (OR x y)
-(EqB x y) -> (XOR (MOVVconst [1]) (XOR <fe.TypeBool()> x y))
+(EqB x y) -> (XOR (MOVVconst [1]) (XOR <types.Bool> x y))
 (NeqB x y) -> (XOR x y)
 (Not x) -> (XORconst [1] x)
 
diff --git a/src/cmd/compile/internal/ssa/gen/PPC64.rules b/src/cmd/compile/internal/ssa/gen/PPC64.rules
index a0b5578d7e..7f56fc33af 100644
--- a/src/cmd/compile/internal/ssa/gen/PPC64.rules
+++ b/src/cmd/compile/internal/ssa/gen/PPC64.rules
@@ -154,72 +154,72 @@
 (Rsh8x32   x (MOVDconst [c])) && uint32(c) < 8  -> (SRAWconst (SignExt8to32  x) [c])
 (Rsh8Ux32  x (MOVDconst [c])) && uint32(c) < 8  -> (SRWconst (ZeroExt8to32  x) [c])
 
-(Rsh64x64 x y)  -> (SRAD x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] y))))
-(Rsh64Ux64 x y) -> (SRD  x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] y))))
-(Lsh64x64 x y)  -> (SLD  x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] y))))
+(Rsh64x64 x y)  -> (SRAD x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] y))))
+(Rsh64Ux64 x y) -> (SRD  x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] y))))
+(Lsh64x64 x y)  -> (SLD  x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] y))))
 
-(Rsh32x64 x y)  -> (SRAW x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] y))))
-(Rsh32Ux64 x y) -> (SRW  x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] y))))
-(Lsh32x64 x y)  -> (SLW  x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] y))))
+(Rsh32x64 x y)  -> (SRAW x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] y))))
+(Rsh32Ux64 x y) -> (SRW  x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] y))))
+(Lsh32x64 x y)  -> (SLW  x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] y))))
 
-(Rsh16x64 x y)  -> (SRAW (SignExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] y))))
-(Rsh16Ux64 x y) -> (SRW  (ZeroExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] y))))
-(Lsh16x64 x y)  -> (SLW  x                 (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] y))))
+(Rsh16x64 x y)  -> (SRAW (SignExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] y))))
+(Rsh16Ux64 x y) -> (SRW  (ZeroExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] y))))
+(Lsh16x64 x y)  -> (SLW  x                 (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] y))))
 
-(Rsh8x64 x y)  -> (SRAW (SignExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] y))))
-(Rsh8Ux64 x y) -> (SRW  (ZeroExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] y))))
-(Lsh8x64 x y)  -> (SLW  x                (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] y))))
+(Rsh8x64 x y)  -> (SRAW (SignExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] y))))
+(Rsh8Ux64 x y) -> (SRW  (ZeroExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] y))))
+(Lsh8x64 x y)  -> (SLW  x                (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] y))))
 
 
-(Rsh64x32 x y)  -> (SRAD x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt32to64 y)))))
-(Rsh64Ux32 x y) -> (SRD x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt32to64 y)))))
-(Lsh64x32 x y)  -> (SLD x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt32to64 y)))))
+(Rsh64x32 x y)  -> (SRAD x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt32to64 y)))))
+(Rsh64Ux32 x y) -> (SRD x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt32to64 y)))))
+(Lsh64x32 x y)  -> (SLD x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt32to64 y)))))
 
-(Rsh32x32 x y)  -> (SRAW x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt32to64 y)))))
-(Rsh32Ux32 x y) -> (SRW x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt32to64 y)))))
-(Lsh32x32 x y)  -> (SLW x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt32to64 y)))))
+(Rsh32x32 x y)  -> (SRAW x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt32to64 y)))))
+(Rsh32Ux32 x y) -> (SRW x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt32to64 y)))))
+(Lsh32x32 x y)  -> (SLW x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt32to64 y)))))
 
-(Rsh16x32 x y)  -> (SRAW (SignExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt32to64 y)))))
-(Rsh16Ux32 x y) -> (SRW  (ZeroExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt32to64 y)))))
-(Lsh16x32 x y)  -> (SLW  x                 (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt32to64 y)))))
+(Rsh16x32 x y)  -> (SRAW (SignExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt32to64 y)))))
+(Rsh16Ux32 x y) -> (SRW  (ZeroExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt32to64 y)))))
+(Lsh16x32 x y)  -> (SLW  x                 (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt32to64 y)))))
 
-(Rsh8x32 x y)  -> (SRAW (SignExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt32to64 y)))))
-(Rsh8Ux32 x y) -> (SRW  (ZeroExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt32to64 y)))))
-(Lsh8x32 x y)  -> (SLW  x                (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt32to64 y)))))
+(Rsh8x32 x y)  -> (SRAW (SignExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt32to64 y)))))
+(Rsh8Ux32 x y) -> (SRW  (ZeroExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt32to64 y)))))
+(Lsh8x32 x y)  -> (SLW  x                (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt32to64 y)))))
 
 
-(Rsh64x16 x y)  -> (SRAD x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt16to64 y)))))
-(Rsh64Ux16 x y) -> (SRD x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt16to64 y)))))
-(Lsh64x16 x y)  -> (SLD x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt16to64 y)))))
+(Rsh64x16 x y)  -> (SRAD x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt16to64 y)))))
+(Rsh64Ux16 x y) -> (SRD x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt16to64 y)))))
+(Lsh64x16 x y)  -> (SLD x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt16to64 y)))))
 
-(Rsh32x16 x y)  -> (SRAW x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt16to64 y)))))
-(Rsh32Ux16 x y) -> (SRW x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt16to64 y)))))
-(Lsh32x16 x y)  -> (SLW x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt16to64 y)))))
+(Rsh32x16 x y)  -> (SRAW x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt16to64 y)))))
+(Rsh32Ux16 x y) -> (SRW x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt16to64 y)))))
+(Lsh32x16 x y)  -> (SLW x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt16to64 y)))))
 
-(Rsh16x16 x y)  -> (SRAW (SignExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt16to64 y)))))
-(Rsh16Ux16 x y) -> (SRW  (ZeroExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt16to64 y)))))
-(Lsh16x16 x y)  -> (SLW  x                 (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt16to64 y)))))
+(Rsh16x16 x y)  -> (SRAW (SignExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt16to64 y)))))
+(Rsh16Ux16 x y) -> (SRW  (ZeroExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt16to64 y)))))
+(Lsh16x16 x y)  -> (SLW  x                 (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt16to64 y)))))
 
-(Rsh8x16 x y)  -> (SRAW (SignExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt16to64 y)))))
-(Rsh8Ux16 x y) -> (SRW  (ZeroExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt16to64 y)))))
-(Lsh8x16 x y)  -> (SLW  x                (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt16to64 y)))))
+(Rsh8x16 x y)  -> (SRAW (SignExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt16to64 y)))))
+(Rsh8Ux16 x y) -> (SRW  (ZeroExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt16to64 y)))))
+(Lsh8x16 x y)  -> (SLW  x                (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt16to64 y)))))
 
 
-(Rsh64x8 x y)  -> (SRAD x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt8to64 y)))))
-(Rsh64Ux8 x y) -> (SRD x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt8to64 y)))))
-(Lsh64x8 x y)  -> (SLD x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt8to64 y)))))
+(Rsh64x8 x y)  -> (SRAD x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt8to64 y)))))
+(Rsh64Ux8 x y) -> (SRD x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt8to64 y)))))
+(Lsh64x8 x y)  -> (SLD x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt8to64 y)))))
 
-(Rsh32x8 x y)  -> (SRAW x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt8to64 y)))))
-(Rsh32Ux8 x y) -> (SRW x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt8to64 y)))))
-(Lsh32x8 x y)  -> (SLW x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt8to64 y)))))
+(Rsh32x8 x y)  -> (SRAW x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt8to64 y)))))
+(Rsh32Ux8 x y) -> (SRW x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt8to64 y)))))
+(Lsh32x8 x y)  -> (SLW x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt8to64 y)))))
 
-(Rsh16x8 x y)  -> (SRAW (SignExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt8to64 y)))))
-(Rsh16Ux8 x y) -> (SRW  (ZeroExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt8to64 y)))))
-(Lsh16x8 x y)  -> (SLW  x                 (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt8to64 y)))))
+(Rsh16x8 x y)  -> (SRAW (SignExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt8to64 y)))))
+(Rsh16Ux8 x y) -> (SRW  (ZeroExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt8to64 y)))))
+(Lsh16x8 x y)  -> (SLW  x                 (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt8to64 y)))))
 
-(Rsh8x8 x y)  -> (SRAW (SignExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt8to64 y)))))
-(Rsh8Ux8 x y) -> (SRW  (ZeroExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt8to64 y)))))
-(Lsh8x8 x y)  -> (SLW  x                (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt8to64 y)))))
+(Rsh8x8 x y)  -> (SRAW (SignExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt8to64 y)))))
+(Rsh8Ux8 x y) -> (SRW  (ZeroExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt8to64 y)))))
+(Lsh8x8 x y)  -> (SLW  x                (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt8to64 y)))))
 
 // Cleaning up shift ops when input is masked
 (MaskIfNotCarry (ADDconstForCarry [c] (ANDconst [d] _))) && c < 0 && d > 0 && c + d < 0 -> (MOVDconst [-1])
@@ -233,7 +233,7 @@
 
 (Addr {sym} base) -> (MOVDaddr {sym} base)
 // (Addr {sym} base) -> (ADDconst {sym} base)
-(OffPtr [off] ptr) -> (ADD (MOVDconst <fe.TypeInt64()> [off]) ptr)
+(OffPtr [off] ptr) -> (ADD (MOVDconst <types.Int64> [off]) ptr)
 
 (And64 x y) -> (AND x y)
 (And32 x y) -> (AND x y)
diff --git a/src/cmd/compile/internal/ssa/gen/S390X.rules b/src/cmd/compile/internal/ssa/gen/S390X.rules
index 41cd0f2465..ed5509bf99 100644
--- a/src/cmd/compile/internal/ssa/gen/S390X.rules
+++ b/src/cmd/compile/internal/ssa/gen/S390X.rules
@@ -437,7 +437,7 @@
 (If (MOVDGTnoinv (MOVDconst [0]) (MOVDconst [1]) cmp) yes no) -> (GTF cmp yes no)
 (If (MOVDGEnoinv (MOVDconst [0]) (MOVDconst [1]) cmp) yes no) -> (GEF cmp yes no)
 
-(If cond yes no) -> (NE (CMPWconst [0] (MOVBZreg <fe.TypeBool()> cond)) yes no)
+(If cond yes no) -> (NE (CMPWconst [0] (MOVBZreg <types.Bool> cond)) yes no)
 
 // ***************************
 // Above: lowering rules
diff --git a/src/cmd/compile/internal/ssa/gen/dec.rules b/src/cmd/compile/internal/ssa/gen/dec.rules
index 5fb5c47ec3..08935d92b4 100644
--- a/src/cmd/compile/internal/ssa/gen/dec.rules
+++ b/src/cmd/compile/internal/ssa/gen/dec.rules
@@ -13,28 +13,28 @@
 
 (Load <t> ptr mem) && t.IsComplex() && t.Size() == 8 ->
   (ComplexMake
-    (Load <fe.TypeFloat32()> ptr mem)
-    (Load <fe.TypeFloat32()>
-      (OffPtr <fe.TypeFloat32().PtrTo()> [4] ptr)
+    (Load <types.Float32> ptr mem)
+    (Load <types.Float32>
+      (OffPtr <types.Float32.PtrTo()> [4] ptr)
       mem)
     )
 (Store {t} dst (ComplexMake real imag) mem) && t.(Type).Size() == 8 ->
-  (Store {fe.TypeFloat32()}
-    (OffPtr <fe.TypeFloat32().PtrTo()> [4] dst)
+  (Store {types.Float32}
+    (OffPtr <types.Float32.PtrTo()> [4] dst)
     imag
-    (Store {fe.TypeFloat32()} dst real mem))
+    (Store {types.Float32} dst real mem))
 (Load <t> ptr mem) && t.IsComplex() && t.Size() == 16 ->
   (ComplexMake
-    (Load <fe.TypeFloat64()> ptr mem)
-    (Load <fe.TypeFloat64()>
-      (OffPtr <fe.TypeFloat64().PtrTo()> [8] ptr)
+    (Load <types.Float64> ptr mem)
+    (Load <types.Float64>
+      (OffPtr <types.Float64.PtrTo()> [8] ptr)
       mem)
     )
 (Store {t} dst (ComplexMake real imag) mem) && t.(Type).Size() == 16 ->
-  (Store {fe.TypeFloat64()}
-    (OffPtr <fe.TypeFloat64().PtrTo()> [8] dst)
+  (Store {types.Float64}
+    (OffPtr <types.Float64.PtrTo()> [8] dst)
     imag
-    (Store {fe.TypeFloat64()} dst real mem))
+    (Store {types.Float64} dst real mem))
 
 // string ops
 (StringPtr (StringMake ptr _)) -> ptr
@@ -42,15 +42,15 @@
 
 (Load <t> ptr mem) && t.IsString() ->
   (StringMake
-    (Load <fe.TypeBytePtr()> ptr mem)
-    (Load <fe.TypeInt()>
-      (OffPtr <fe.TypeInt().PtrTo()> [config.PtrSize] ptr)
+    (Load <types.BytePtr> ptr mem)
+    (Load <types.Int>
+      (OffPtr <types.Int.PtrTo()> [config.PtrSize] ptr)
       mem))
 (Store dst (StringMake ptr len) mem) ->
-  (Store {fe.TypeInt()}
-    (OffPtr <fe.TypeInt().PtrTo()> [config.PtrSize] dst)
+  (Store {types.Int}
+    (OffPtr <types.Int.PtrTo()> [config.PtrSize] dst)
     len
-    (Store {fe.TypeBytePtr()} dst ptr mem))
+    (Store {types.BytePtr} dst ptr mem))
 
 // slice ops
 (SlicePtr (SliceMake ptr _ _ )) -> ptr
@@ -60,20 +60,20 @@
 (Load <t> ptr mem) && t.IsSlice() ->
   (SliceMake
     (Load <t.ElemType().PtrTo()> ptr mem)
-    (Load <fe.TypeInt()>
-      (OffPtr <fe.TypeInt().PtrTo()> [config.PtrSize] ptr)
+    (Load <types.Int>
+      (OffPtr <types.Int.PtrTo()> [config.PtrSize] ptr)
       mem)
-    (Load <fe.TypeInt()>
-      (OffPtr <fe.TypeInt().PtrTo()> [2*config.PtrSize] ptr)
+    (Load <types.Int>
+      (OffPtr <types.Int.PtrTo()> [2*config.PtrSize] ptr)
       mem))
 (Store dst (SliceMake ptr len cap) mem) ->
-  (Store {fe.TypeInt()}
-    (OffPtr <fe.TypeInt().PtrTo()> [2*config.PtrSize] dst)
+  (Store {types.Int}
+    (OffPtr <types.Int.PtrTo()> [2*config.PtrSize] dst)
     cap
-    (Store {fe.TypeInt()}
-      (OffPtr <fe.TypeInt().PtrTo()> [config.PtrSize] dst)
+    (Store {types.Int}
+      (OffPtr <types.Int.PtrTo()> [config.PtrSize] dst)
       len
-      (Store {fe.TypeBytePtr()} dst ptr mem)))
+      (Store {types.BytePtr} dst ptr mem)))
 
 // interface ops
 (ITab (IMake itab _)) -> itab
@@ -81,12 +81,12 @@
 
 (Load <t> ptr mem) && t.IsInterface() ->
   (IMake
-    (Load <fe.TypeBytePtr()> ptr mem)
-    (Load <fe.TypeBytePtr()>
-      (OffPtr <fe.TypeBytePtr().PtrTo()> [config.PtrSize] ptr)
+    (Load <types.BytePtr> ptr mem)
+    (Load <types.BytePtr>
+      (OffPtr <types.BytePtr.PtrTo()> [config.PtrSize] ptr)
       mem))
 (Store dst (IMake itab data) mem) ->
-  (Store {fe.TypeBytePtr()}
-    (OffPtr <fe.TypeBytePtr().PtrTo()> [config.PtrSize] dst)
+  (Store {types.BytePtr}
+    (OffPtr <types.BytePtr.PtrTo()> [config.PtrSize] dst)
     data
-    (Store {fe.TypeUintptr()} dst itab mem))
+    (Store {types.Uintptr} dst itab mem))
diff --git a/src/cmd/compile/internal/ssa/gen/dec64.rules b/src/cmd/compile/internal/ssa/gen/dec64.rules
index 03c14a36fe..19f9755b40 100644
--- a/src/cmd/compile/internal/ssa/gen/dec64.rules
+++ b/src/cmd/compile/internal/ssa/gen/dec64.rules
@@ -12,23 +12,23 @@
 
 (Load <t> ptr mem) && is64BitInt(t) && !config.BigEndian && t.IsSigned() ->
 	(Int64Make
-		(Load <fe.TypeInt32()> (OffPtr <fe.TypeInt32().PtrTo()> [4] ptr) mem)
-		(Load <fe.TypeUInt32()> ptr mem))
+		(Load <types.Int32> (OffPtr <types.Int32.PtrTo()> [4] ptr) mem)
+		(Load <types.UInt32> ptr mem))
 
 (Load <t> ptr mem) && is64BitInt(t) && !config.BigEndian && !t.IsSigned() ->
 	(Int64Make
-		(Load <fe.TypeUInt32()> (OffPtr <fe.TypeUInt32().PtrTo()> [4] ptr) mem)
-		(Load <fe.TypeUInt32()> ptr mem))
+		(Load <types.UInt32> (OffPtr <types.UInt32.PtrTo()> [4] ptr) mem)
+		(Load <types.UInt32> ptr mem))
 
 (Load <t> ptr mem) && is64BitInt(t) && config.BigEndian && t.IsSigned() ->
 	(Int64Make
-		(Load <fe.TypeInt32()> ptr mem)
-		(Load <fe.TypeUInt32()> (OffPtr <fe.TypeUInt32().PtrTo()> [4] ptr) mem))
+		(Load <types.Int32> ptr mem)
+		(Load <types.UInt32> (OffPtr <types.UInt32.PtrTo()> [4] ptr) mem))
 
 (Load <t> ptr mem) && is64BitInt(t) && config.BigEndian && !t.IsSigned() ->
 	(Int64Make
-		(Load <fe.TypeUInt32()> ptr mem)
-		(Load <fe.TypeUInt32()> (OffPtr <fe.TypeUInt32().PtrTo()> [4] ptr) mem))
+		(Load <types.UInt32> ptr mem)
+		(Load <types.UInt32> (OffPtr <types.UInt32.PtrTo()> [4] ptr) mem))
 
 (Store {t} dst (Int64Make hi lo) mem) && t.(Type).Size() == 8 && !config.BigEndian ->
 	(Store {hi.Type}
@@ -44,94 +44,94 @@
 
 (Arg {n} [off]) && is64BitInt(v.Type) && !config.BigEndian && v.Type.IsSigned() ->
   (Int64Make
-    (Arg <fe.TypeInt32()> {n} [off+4])
-    (Arg <fe.TypeUInt32()> {n} [off]))
+    (Arg <types.Int32> {n} [off+4])
+    (Arg <types.UInt32> {n} [off]))
 (Arg {n} [off]) && is64BitInt(v.Type) && !config.BigEndian && !v.Type.IsSigned() ->
   (Int64Make
-    (Arg <fe.TypeUInt32()> {n} [off+4])
-    (Arg <fe.TypeUInt32()> {n} [off]))
+    (Arg <types.UInt32> {n} [off+4])
+    (Arg <types.UInt32> {n} [off]))
 
 (Arg {n} [off]) && is64BitInt(v.Type) && config.BigEndian && v.Type.IsSigned() ->
   (Int64Make
-    (Arg <fe.TypeInt32()> {n} [off])
-    (Arg <fe.TypeUInt32()> {n} [off+4]))
+    (Arg <types.Int32> {n} [off])
+    (Arg <types.UInt32> {n} [off+4]))
 (Arg {n} [off]) && is64BitInt(v.Type) && config.BigEndian && !v.Type.IsSigned() ->
   (Int64Make
-    (Arg <fe.TypeUInt32()> {n} [off])
-    (Arg <fe.TypeUInt32()> {n} [off+4]))
+    (Arg <types.UInt32> {n} [off])
+    (Arg <types.UInt32> {n} [off+4]))
 
 (Add64 x y) ->
 	(Int64Make
-		(Add32withcarry <fe.TypeInt32()>
+		(Add32withcarry <types.Int32>
 			(Int64Hi x)
 			(Int64Hi y)
 			(Select1 <TypeFlags> (Add32carry (Int64Lo x) (Int64Lo y))))
-		(Select0 <fe.TypeUInt32()> (Add32carry (Int64Lo x) (Int64Lo y))))
+		(Select0 <types.UInt32> (Add32carry (Int64Lo x) (Int64Lo y))))
 
 (Sub64 x y) ->
 	(Int64Make
-		(Sub32withcarry <fe.TypeInt32()>
+		(Sub32withcarry <types.Int32>
 			(Int64Hi x)
 			(Int64Hi y)
 			(Select1 <TypeFlags> (Sub32carry (Int64Lo x) (Int64Lo y))))
-		(Select0 <fe.TypeUInt32()> (Sub32carry (Int64Lo x) (Int64Lo y))))
+		(Select0 <types.UInt32> (Sub32carry (Int64Lo x) (Int64Lo y))))
 
 (Mul64 x y) ->
 	(Int64Make
-		(Add32 <fe.TypeUInt32()>
-			(Mul32 <fe.TypeUInt32()> (Int64Lo x) (Int64Hi y))
-			(Add32 <fe.TypeUInt32()>
-				(Mul32 <fe.TypeUInt32()> (Int64Hi x) (Int64Lo y))
-				(Select0 <fe.TypeUInt32()> (Mul32uhilo (Int64Lo x) (Int64Lo y)))))
-		(Select1 <fe.TypeUInt32()> (Mul32uhilo (Int64Lo x) (Int64Lo y))))
+		(Add32 <types.UInt32>
+			(Mul32 <types.UInt32> (Int64Lo x) (Int64Hi y))
+			(Add32 <types.UInt32>
+				(Mul32 <types.UInt32> (Int64Hi x) (Int64Lo y))
+				(Select0 <types.UInt32> (Mul32uhilo (Int64Lo x) (Int64Lo y)))))
+		(Select1 <types.UInt32> (Mul32uhilo (Int64Lo x) (Int64Lo y))))
 
 (And64 x y) ->
 	(Int64Make
-		(And32 <fe.TypeUInt32()> (Int64Hi x) (Int64Hi y))
-		(And32 <fe.TypeUInt32()> (Int64Lo x) (Int64Lo y)))
+		(And32 <types.UInt32> (Int64Hi x) (Int64Hi y))
+		(And32 <types.UInt32> (Int64Lo x) (Int64Lo y)))
 
 (Or64 x y) ->
 	(Int64Make
-		(Or32 <fe.TypeUInt32()> (Int64Hi x) (Int64Hi y))
-		(Or32 <fe.TypeUInt32()> (Int64Lo x) (Int64Lo y)))
+		(Or32 <types.UInt32> (Int64Hi x) (Int64Hi y))
+		(Or32 <types.UInt32> (Int64Lo x) (Int64Lo y)))
 
 (Xor64 x y) ->
 	(Int64Make
-		(Xor32 <fe.TypeUInt32()> (Int64Hi x) (Int64Hi y))
-		(Xor32 <fe.TypeUInt32()> (Int64Lo x) (Int64Lo y)))
+		(Xor32 <types.UInt32> (Int64Hi x) (Int64Hi y))
+		(Xor32 <types.UInt32> (Int64Lo x) (Int64Lo y)))
 
 (Neg64 <t> x) -> (Sub64 (Const64 <t> [0]) x)
 
 (Com64 x) ->
 	(Int64Make
-		(Com32 <fe.TypeUInt32()> (Int64Hi x))
-		(Com32 <fe.TypeUInt32()> (Int64Lo x)))
+		(Com32 <types.UInt32> (Int64Hi x))
+		(Com32 <types.UInt32> (Int64Lo x)))
 
 (Ctz64 x) ->
-	(Add32 <fe.TypeUInt32()>
-		(Ctz32 <fe.TypeUInt32()> (Int64Lo x))
-		(And32 <fe.TypeUInt32()>
-			(Com32 <fe.TypeUInt32()> (Zeromask (Int64Lo x)))
-			(Ctz32 <fe.TypeUInt32()> (Int64Hi x))))
+	(Add32 <types.UInt32>
+		(Ctz32 <types.UInt32> (Int64Lo x))
+		(And32 <types.UInt32>
+			(Com32 <types.UInt32> (Zeromask (Int64Lo x)))
+			(Ctz32 <types.UInt32> (Int64Hi x))))
 
 (BitLen64 x) ->
-	(Add32 <fe.TypeInt()>
-		(BitLen32 <fe.TypeInt()> (Int64Hi x))
-		(BitLen32 <fe.TypeInt()>
-			(Or32 <fe.TypeUInt32()>
+	(Add32 <types.Int>
+		(BitLen32 <types.Int> (Int64Hi x))
+		(BitLen32 <types.Int>
+			(Or32 <types.UInt32>
 				(Int64Lo x)
 				(Zeromask (Int64Hi x)))))
 
 (Bswap64 x) ->
 	(Int64Make
-		(Bswap32 <fe.TypeUInt32()> (Int64Lo x))
-		(Bswap32 <fe.TypeUInt32()> (Int64Hi x)))
+		(Bswap32 <types.UInt32> (Int64Lo x))
+		(Bswap32 <types.UInt32> (Int64Hi x)))
 
 (SignExt32to64 x) -> (Int64Make (Signmask x) x)
 (SignExt16to64 x) -> (SignExt32to64 (SignExt16to32 x))
 (SignExt8to64 x) -> (SignExt32to64 (SignExt8to32 x))
 
-(ZeroExt32to64 x) -> (Int64Make (Const32 <fe.TypeUInt32()> [0]) x)
+(ZeroExt32to64 x) -> (Int64Make (Const32 <types.UInt32> [0]) x)
 (ZeroExt16to64 x) -> (ZeroExt32to64 (ZeroExt16to32 x))
 (ZeroExt8to64 x) -> (ZeroExt32to64 (ZeroExt8to32 x))
 
@@ -170,160 +170,160 @@
 // turn x64 non-constant shifts to x32 shifts
 // if high 32-bit of the shift is nonzero, make a huge shift
 (Lsh64x64 x (Int64Make hi lo)) && hi.Op != OpConst32 ->
-	(Lsh64x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	(Lsh64x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 (Rsh64x64 x (Int64Make hi lo)) && hi.Op != OpConst32 ->
-	(Rsh64x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	(Rsh64x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 (Rsh64Ux64 x (Int64Make hi lo)) && hi.Op != OpConst32 ->
-	(Rsh64Ux32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	(Rsh64Ux32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 (Lsh32x64 x (Int64Make hi lo)) && hi.Op != OpConst32 ->
-	(Lsh32x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	(Lsh32x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 (Rsh32x64 x (Int64Make hi lo)) && hi.Op != OpConst32 ->
-	(Rsh32x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	(Rsh32x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 (Rsh32Ux64 x (Int64Make hi lo)) && hi.Op != OpConst32 ->
-	(Rsh32Ux32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	(Rsh32Ux32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 (Lsh16x64 x (Int64Make hi lo)) && hi.Op != OpConst32 ->
-	(Lsh16x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	(Lsh16x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 (Rsh16x64 x (Int64Make hi lo)) && hi.Op != OpConst32 ->
-	(Rsh16x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	(Rsh16x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 (Rsh16Ux64 x (Int64Make hi lo)) && hi.Op != OpConst32 ->
-	(Rsh16Ux32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	(Rsh16Ux32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 (Lsh8x64 x (Int64Make hi lo)) && hi.Op != OpConst32 ->
-	(Lsh8x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	(Lsh8x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 (Rsh8x64 x (Int64Make hi lo)) && hi.Op != OpConst32 ->
-	(Rsh8x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	(Rsh8x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 (Rsh8Ux64 x (Int64Make hi lo)) && hi.Op != OpConst32 ->
-	(Rsh8Ux32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	(Rsh8Ux32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 
 // 64x left shift
 // result.hi = hi<<s | lo>>(32-s) | lo<<(s-32) // >> is unsigned, large shifts result 0
 // result.lo = lo<<s
 (Lsh64x32 (Int64Make hi lo) s) ->
 	(Int64Make
-		(Or32 <fe.TypeUInt32()>
-			(Or32 <fe.TypeUInt32()>
-				(Lsh32x32 <fe.TypeUInt32()> hi s)
-				(Rsh32Ux32 <fe.TypeUInt32()>
+		(Or32 <types.UInt32>
+			(Or32 <types.UInt32>
+				(Lsh32x32 <types.UInt32> hi s)
+				(Rsh32Ux32 <types.UInt32>
 					lo
-					(Sub32 <fe.TypeUInt32()> (Const32 <fe.TypeUInt32()> [32]) s)))
-			(Lsh32x32 <fe.TypeUInt32()>
+					(Sub32 <types.UInt32> (Const32 <types.UInt32> [32]) s)))
+			(Lsh32x32 <types.UInt32>
 				lo
-				(Sub32 <fe.TypeUInt32()> s (Const32 <fe.TypeUInt32()> [32]))))
-		(Lsh32x32 <fe.TypeUInt32()> lo s))
+				(Sub32 <types.UInt32> s (Const32 <types.UInt32> [32]))))
+		(Lsh32x32 <types.UInt32> lo s))
 (Lsh64x16 (Int64Make hi lo) s) ->
 	(Int64Make
-		(Or32 <fe.TypeUInt32()>
-			(Or32 <fe.TypeUInt32()>
-				(Lsh32x16 <fe.TypeUInt32()> hi s)
-				(Rsh32Ux16 <fe.TypeUInt32()>
+		(Or32 <types.UInt32>
+			(Or32 <types.UInt32>
+				(Lsh32x16 <types.UInt32> hi s)
+				(Rsh32Ux16 <types.UInt32>
 					lo
-					(Sub16 <fe.TypeUInt16()> (Const16 <fe.TypeUInt16()> [32]) s)))
-			(Lsh32x16 <fe.TypeUInt32()>
+					(Sub16 <types.UInt16> (Const16 <types.UInt16> [32]) s)))
+			(Lsh32x16 <types.UInt32>
 				lo
-				(Sub16 <fe.TypeUInt16()> s (Const16 <fe.TypeUInt16()> [32]))))
-		(Lsh32x16 <fe.TypeUInt32()> lo s))
+				(Sub16 <types.UInt16> s (Const16 <types.UInt16> [32]))))
+		(Lsh32x16 <types.UInt32> lo s))
 (Lsh64x8 (Int64Make hi lo) s) ->
 	(Int64Make
-		(Or32 <fe.TypeUInt32()>
-			(Or32 <fe.TypeUInt32()>
-				(Lsh32x8 <fe.TypeUInt32()> hi s)
-				(Rsh32Ux8 <fe.TypeUInt32()>
+		(Or32 <types.UInt32>
+			(Or32 <types.UInt32>
+				(Lsh32x8 <types.UInt32> hi s)
+				(Rsh32Ux8 <types.UInt32>
 					lo
-					(Sub8 <fe.TypeUInt8()> (Const8 <fe.TypeUInt8()> [32]) s)))
-			(Lsh32x8 <fe.TypeUInt32()>
+					(Sub8 <types.UInt8> (Const8 <types.UInt8> [32]) s)))
+			(Lsh32x8 <types.UInt32>
 				lo
-				(Sub8 <fe.TypeUInt8()> s (Const8 <fe.TypeUInt8()> [32]))))
-		(Lsh32x8 <fe.TypeUInt32()> lo s))
+				(Sub8 <types.UInt8> s (Const8 <types.UInt8> [32]))))
+		(Lsh32x8 <types.UInt32> lo s))
 
 // 64x unsigned right shift
 // result.hi = hi>>s
 // result.lo = lo>>s | hi<<(32-s) | hi>>(s-32) // >> is unsigned, large shifts result 0
 (Rsh64Ux32 (Int64Make hi lo) s) ->
 	(Int64Make
-		(Rsh32Ux32 <fe.TypeUInt32()> hi s)
-		(Or32 <fe.TypeUInt32()>
-			(Or32 <fe.TypeUInt32()>
-				(Rsh32Ux32 <fe.TypeUInt32()> lo s)
-				(Lsh32x32 <fe.TypeUInt32()>
+		(Rsh32Ux32 <types.UInt32> hi s)
+		(Or32 <types.UInt32>
+			(Or32 <types.UInt32>
+				(Rsh32Ux32 <types.UInt32> lo s)
+				(Lsh32x32 <types.UInt32>
 					hi
-					(Sub32 <fe.TypeUInt32()> (Const32 <fe.TypeUInt32()> [32]) s)))
-			(Rsh32Ux32 <fe.TypeUInt32()>
+					(Sub32 <types.UInt32> (Const32 <types.UInt32> [32]) s)))
+			(Rsh32Ux32 <types.UInt32>
 				hi
-				(Sub32 <fe.TypeUInt32()> s (Const32 <fe.TypeUInt32()> [32])))))
+				(Sub32 <types.UInt32> s (Const32 <types.UInt32> [32])))))
 (Rsh64Ux16 (Int64Make hi lo) s) ->
 	(Int64Make
-		(Rsh32Ux16 <fe.TypeUInt32()> hi s)
-		(Or32 <fe.TypeUInt32()>
-			(Or32 <fe.TypeUInt32()>
-				(Rsh32Ux16 <fe.TypeUInt32()> lo s)
-				(Lsh32x16 <fe.TypeUInt32()>
+		(Rsh32Ux16 <types.UInt32> hi s)
+		(Or32 <types.UInt32>
+			(Or32 <types.UInt32>
+				(Rsh32Ux16 <types.UInt32> lo s)
+				(Lsh32x16 <types.UInt32>
 					hi
-					(Sub16 <fe.TypeUInt16()> (Const16 <fe.TypeUInt16()> [32]) s)))
-			(Rsh32Ux16 <fe.TypeUInt32()>
+					(Sub16 <types.UInt16> (Const16 <types.UInt16> [32]) s)))
+			(Rsh32Ux16 <types.UInt32>
 				hi
-				(Sub16 <fe.TypeUInt16()> s (Const16 <fe.TypeUInt16()> [32])))))
+				(Sub16 <types.UInt16> s (Const16 <types.UInt16> [32])))))
 (Rsh64Ux8 (Int64Make hi lo) s) ->
 	(Int64Make
-		(Rsh32Ux8 <fe.TypeUInt32()> hi s)
-		(Or32 <fe.TypeUInt32()>
-			(Or32 <fe.TypeUInt32()>
-				(Rsh32Ux8 <fe.TypeUInt32()> lo s)
-				(Lsh32x8 <fe.TypeUInt32()>
+		(Rsh32Ux8 <types.UInt32> hi s)
+		(Or32 <types.UInt32>
+			(Or32 <types.UInt32>
+				(Rsh32Ux8 <types.UInt32> lo s)
+				(Lsh32x8 <types.UInt32>
 					hi
-					(Sub8 <fe.TypeUInt8()> (Const8 <fe.TypeUInt8()> [32]) s)))
-			(Rsh32Ux8 <fe.TypeUInt32()>
+					(Sub8 <types.UInt8> (Const8 <types.UInt8> [32]) s)))
+			(Rsh32Ux8 <types.UInt32>
 				hi
-				(Sub8 <fe.TypeUInt8()> s (Const8 <fe.TypeUInt8()> [32])))))
+				(Sub8 <types.UInt8> s (Const8 <types.UInt8> [32])))))
 
 // 64x signed right shift
 // result.hi = hi>>s
 // result.lo = lo>>s | hi<<(32-s) | (hi>>(s-32))&zeromask(s>>5) // hi>>(s-32) is signed, large shifts result 0/-1
 (Rsh64x32 (Int64Make hi lo) s) ->
 	(Int64Make
-		(Rsh32x32 <fe.TypeUInt32()> hi s)
-		(Or32 <fe.TypeUInt32()>
-			(Or32 <fe.TypeUInt32()>
-				(Rsh32Ux32 <fe.TypeUInt32()> lo s)
-				(Lsh32x32 <fe.TypeUInt32()>
+		(Rsh32x32 <types.UInt32> hi s)
+		(Or32 <types.UInt32>
+			(Or32 <types.UInt32>
+				(Rsh32Ux32 <types.UInt32> lo s)
+				(Lsh32x32 <types.UInt32>
 					hi
-					(Sub32 <fe.TypeUInt32()> (Const32 <fe.TypeUInt32()> [32]) s)))
-			(And32 <fe.TypeUInt32()>
-				(Rsh32x32 <fe.TypeUInt32()>
+					(Sub32 <types.UInt32> (Const32 <types.UInt32> [32]) s)))
+			(And32 <types.UInt32>
+				(Rsh32x32 <types.UInt32>
 					hi
-					(Sub32 <fe.TypeUInt32()> s (Const32 <fe.TypeUInt32()> [32])))
+					(Sub32 <types.UInt32> s (Const32 <types.UInt32> [32])))
 				(Zeromask
-					(Rsh32Ux32 <fe.TypeUInt32()> s (Const32 <fe.TypeUInt32()> [5]))))))
+					(Rsh32Ux32 <types.UInt32> s (Const32 <types.UInt32> [5]))))))
 (Rsh64x16 (Int64Make hi lo) s) ->
 	(Int64Make
-		(Rsh32x16 <fe.TypeUInt32()> hi s)
-		(Or32 <fe.TypeUInt32()>
-			(Or32 <fe.TypeUInt32()>
-				(Rsh32Ux16 <fe.TypeUInt32()> lo s)
-				(Lsh32x16 <fe.TypeUInt32()>
+		(Rsh32x16 <types.UInt32> hi s)
+		(Or32 <types.UInt32>
+			(Or32 <types.UInt32>
+				(Rsh32Ux16 <types.UInt32> lo s)
+				(Lsh32x16 <types.UInt32>
 					hi
-					(Sub16 <fe.TypeUInt16()> (Const16 <fe.TypeUInt16()> [32]) s)))
-			(And32 <fe.TypeUInt32()>
-				(Rsh32x16 <fe.TypeUInt32()>
+					(Sub16 <types.UInt16> (Const16 <types.UInt16> [32]) s)))
+			(And32 <types.UInt32>
+				(Rsh32x16 <types.UInt32>
 					hi
-					(Sub16 <fe.TypeUInt16()> s (Const16 <fe.TypeUInt16()> [32])))
+					(Sub16 <types.UInt16> s (Const16 <types.UInt16> [32])))
 				(Zeromask
 					(ZeroExt16to32
-						(Rsh16Ux32 <fe.TypeUInt16()> s (Const32 <fe.TypeUInt32()> [5])))))))
+						(Rsh16Ux32 <types.UInt16> s (Const32 <types.UInt32> [5])))))))
 (Rsh64x8 (Int64Make hi lo) s) ->
 	(Int64Make
-		(Rsh32x8 <fe.TypeUInt32()> hi s)
-		(Or32 <fe.TypeUInt32()>
-			(Or32 <fe.TypeUInt32()>
-				(Rsh32Ux8 <fe.TypeUInt32()> lo s)
-				(Lsh32x8 <fe.TypeUInt32()>
+		(Rsh32x8 <types.UInt32> hi s)
+		(Or32 <types.UInt32>
+			(Or32 <types.UInt32>
+				(Rsh32Ux8 <types.UInt32> lo s)
+				(Lsh32x8 <types.UInt32>
 					hi
-					(Sub8 <fe.TypeUInt8()> (Const8 <fe.TypeUInt8()> [32]) s)))
-			(And32 <fe.TypeUInt32()>
-				(Rsh32x8 <fe.TypeUInt32()>
+					(Sub8 <types.UInt8> (Const8 <types.UInt8> [32]) s)))
+			(And32 <types.UInt32>
+				(Rsh32x8 <types.UInt32>
 					hi
-					(Sub8 <fe.TypeUInt8()> s (Const8 <fe.TypeUInt8()> [32])))
+					(Sub8 <types.UInt8> s (Const8 <types.UInt8> [32])))
 				(Zeromask
 					(ZeroExt8to32
-						(Rsh8Ux32 <fe.TypeUInt8()> s (Const32 <fe.TypeUInt32()> [5])))))))
+						(Rsh8Ux32 <types.UInt8> s (Const32 <types.UInt32> [5])))))))
 
 // 64xConst32 shifts
 // we probably do not need them -- lateopt may take care of them just fine
@@ -333,48 +333,48 @@
 //
 //(Lsh64x32 x (Const32 [c])) && c < 64 && c > 32 ->
 //	(Int64Make
-//		(Lsh32x32 <fe.TypeUInt32()> (Int64Lo x) (Const32 <fe.TypeUInt32()> [c-32]))
-//		(Const32 <fe.TypeUInt32()> [0]))
+//		(Lsh32x32 <types.UInt32> (Int64Lo x) (Const32 <types.UInt32> [c-32]))
+//		(Const32 <types.UInt32> [0]))
 //(Rsh64x32 x (Const32 [c])) && c < 64 && c > 32 ->
 //	(Int64Make
 //		(Signmask (Int64Hi x))
-//		(Rsh32x32 <fe.TypeInt32()> (Int64Hi x) (Const32 <fe.TypeUInt32()> [c-32])))
+//		(Rsh32x32 <types.Int32> (Int64Hi x) (Const32 <types.UInt32> [c-32])))
 //(Rsh64Ux32 x (Const32 [c])) && c < 64 && c > 32 ->
 //	(Int64Make
-//		(Const32 <fe.TypeUInt32()> [0])
-//		(Rsh32Ux32 <fe.TypeUInt32()> (Int64Hi x) (Const32 <fe.TypeUInt32()> [c-32])))
+//		(Const32 <types.UInt32> [0])
+//		(Rsh32Ux32 <types.UInt32> (Int64Hi x) (Const32 <types.UInt32> [c-32])))
 //
-//(Lsh64x32 x (Const32 [32])) -> (Int64Make (Int64Lo x) (Const32 <fe.TypeUInt32()> [0]))
+//(Lsh64x32 x (Const32 [32])) -> (Int64Make (Int64Lo x) (Const32 <types.UInt32> [0]))
 //(Rsh64x32 x (Const32 [32])) -> (Int64Make (Signmask (Int64Hi x)) (Int64Hi x))
-//(Rsh64Ux32 x (Const32 [32])) -> (Int64Make (Const32 <fe.TypeUInt32()> [0]) (Int64Hi x))
+//(Rsh64Ux32 x (Const32 [32])) -> (Int64Make (Const32 <types.UInt32> [0]) (Int64Hi x))
 //
 //(Lsh64x32 x (Const32 [c])) && c < 32 && c > 0 ->
 //	(Int64Make
-//		(Or32 <fe.TypeUInt32()>
-//			(Lsh32x32 <fe.TypeUInt32()> (Int64Hi x) (Const32 <fe.TypeUInt32()> [c]))
-//			(Rsh32Ux32 <fe.TypeUInt32()> (Int64Lo x) (Const32 <fe.TypeUInt32()> [32-c])))
-//		(Lsh32x32 <fe.TypeUInt32()> (Int64Lo x) (Const32 <fe.TypeUInt32()> [c])))
+//		(Or32 <types.UInt32>
+//			(Lsh32x32 <types.UInt32> (Int64Hi x) (Const32 <types.UInt32> [c]))
+//			(Rsh32Ux32 <types.UInt32> (Int64Lo x) (Const32 <types.UInt32> [32-c])))
+//		(Lsh32x32 <types.UInt32> (Int64Lo x) (Const32 <types.UInt32> [c])))
 //(Rsh64x32 x (Const32 [c])) && c < 32 && c > 0 ->
 //	(Int64Make
-//		(Rsh32x32 <fe.TypeInt32()> (Int64Hi x) (Const32 <fe.TypeUInt32()> [c]))
-//		(Or32 <fe.TypeUInt32()>
-//			(Rsh32Ux32 <fe.TypeUInt32()> (Int64Lo x) (Const32 <fe.TypeUInt32()> [c]))
-//			(Lsh32x32 <fe.TypeUInt32()> (Int64Hi x) (Const32 <fe.TypeUInt32()> [32-c]))))
+//		(Rsh32x32 <types.Int32> (Int64Hi x) (Const32 <types.UInt32> [c]))
+//		(Or32 <types.UInt32>
+//			(Rsh32Ux32 <types.UInt32> (Int64Lo x) (Const32 <types.UInt32> [c]))
+//			(Lsh32x32 <types.UInt32> (Int64Hi x) (Const32 <types.UInt32> [32-c]))))
 //(Rsh64Ux32 x (Const32 [c])) && c < 32 && c > 0 ->
 //	(Int64Make
-//		(Rsh32Ux32 <fe.TypeUInt32()> (Int64Hi x) (Const32 <fe.TypeUInt32()> [c]))
-//		(Or32 <fe.TypeUInt32()>
-//			(Rsh32Ux32 <fe.TypeUInt32()> (Int64Lo x) (Const32 <fe.TypeUInt32()> [c]))
-//			(Lsh32x32 <fe.TypeUInt32()> (Int64Hi x) (Const32 <fe.TypeUInt32()> [32-c]))))
+//		(Rsh32Ux32 <types.UInt32> (Int64Hi x) (Const32 <types.UInt32> [c]))
+//		(Or32 <types.UInt32>
+//			(Rsh32Ux32 <types.UInt32> (Int64Lo x) (Const32 <types.UInt32> [c]))
+//			(Lsh32x32 <types.UInt32> (Int64Hi x) (Const32 <types.UInt32> [32-c]))))
 //
 //(Lsh64x32 x (Const32 [0])) -> x
 //(Rsh64x32 x (Const32 [0])) -> x
 //(Rsh64Ux32 x (Const32 [0])) -> x
 
 (Const64 <t> [c]) && t.IsSigned() ->
-	(Int64Make (Const32 <fe.TypeInt32()> [c>>32]) (Const32 <fe.TypeUInt32()> [int64(int32(c))]))
+	(Int64Make (Const32 <types.Int32> [c>>32]) (Const32 <types.UInt32> [int64(int32(c))]))
 (Const64 <t> [c]) && !t.IsSigned() ->
-	(Int64Make (Const32 <fe.TypeUInt32()> [c>>32]) (Const32 <fe.TypeUInt32()> [int64(int32(c))]))
+	(Int64Make (Const32 <types.UInt32> [c>>32]) (Const32 <types.UInt32> [int64(int32(c))]))
 
 (Eq64 x y) ->
 	(AndB
diff --git a/src/cmd/compile/internal/ssa/gen/generic.rules b/src/cmd/compile/internal/ssa/gen/generic.rules
index 6be686134e..5bb5610c25 100644
--- a/src/cmd/compile/internal/ssa/gen/generic.rules
+++ b/src/cmd/compile/internal/ssa/gen/generic.rules
@@ -155,14 +155,14 @@
 (Mul64 (Const64 [-1]) x) -> (Neg64 x)
 
 // Convert multiplication by a power of two to a shift.
-(Mul8  <t> n (Const8  [c])) && isPowerOfTwo(c) -> (Lsh8x64  <t> n (Const64 <fe.TypeUInt64()> [log2(c)]))
-(Mul16 <t> n (Const16 [c])) && isPowerOfTwo(c) -> (Lsh16x64 <t> n (Const64 <fe.TypeUInt64()> [log2(c)]))
-(Mul32 <t> n (Const32 [c])) && isPowerOfTwo(c) -> (Lsh32x64 <t> n (Const64 <fe.TypeUInt64()> [log2(c)]))
-(Mul64 <t> n (Const64 [c])) && isPowerOfTwo(c) -> (Lsh64x64 <t> n (Const64 <fe.TypeUInt64()> [log2(c)]))
-(Mul8  <t> n (Const8  [c])) && t.IsSigned() && isPowerOfTwo(-c) -> (Neg8  (Lsh8x64  <t> n (Const64 <fe.TypeUInt64()> [log2(-c)])))
-(Mul16 <t> n (Const16 [c])) && t.IsSigned() && isPowerOfTwo(-c) -> (Neg16 (Lsh16x64 <t> n (Const64 <fe.TypeUInt64()> [log2(-c)])))
-(Mul32 <t> n (Const32 [c])) && t.IsSigned() && isPowerOfTwo(-c) -> (Neg32 (Lsh32x64 <t> n (Const64 <fe.TypeUInt64()> [log2(-c)])))
-(Mul64 <t> n (Const64 [c])) && t.IsSigned() && isPowerOfTwo(-c) -> (Neg64 (Lsh64x64 <t> n (Const64 <fe.TypeUInt64()> [log2(-c)])))
+(Mul8  <t> n (Const8  [c])) && isPowerOfTwo(c) -> (Lsh8x64  <t> n (Const64 <types.UInt64> [log2(c)]))
+(Mul16 <t> n (Const16 [c])) && isPowerOfTwo(c) -> (Lsh16x64 <t> n (Const64 <types.UInt64> [log2(c)]))
+(Mul32 <t> n (Const32 [c])) && isPowerOfTwo(c) -> (Lsh32x64 <t> n (Const64 <types.UInt64> [log2(c)]))
+(Mul64 <t> n (Const64 [c])) && isPowerOfTwo(c) -> (Lsh64x64 <t> n (Const64 <types.UInt64> [log2(c)]))
+(Mul8  <t> n (Const8  [c])) && t.IsSigned() && isPowerOfTwo(-c) -> (Neg8  (Lsh8x64  <t> n (Const64 <types.UInt64> [log2(-c)])))
+(Mul16 <t> n (Const16 [c])) && t.IsSigned() && isPowerOfTwo(-c) -> (Neg16 (Lsh16x64 <t> n (Const64 <types.UInt64> [log2(-c)])))
+(Mul32 <t> n (Const32 [c])) && t.IsSigned() && isPowerOfTwo(-c) -> (Neg32 (Lsh32x64 <t> n (Const64 <types.UInt64> [log2(-c)])))
+(Mul64 <t> n (Const64 [c])) && t.IsSigned() && isPowerOfTwo(-c) -> (Neg64 (Lsh64x64 <t> n (Const64 <types.UInt64> [log2(-c)])))
 
 (Mod8  (Const8  [c]) (Const8  [d])) && d != 0 -> (Const8  [int64(int8(c % d))])
 (Mod16 (Const16 [c]) (Const16 [d])) && d != 0 -> (Const16 [int64(int16(c % d))])
@@ -481,46 +481,46 @@
 // ((x >> c1) << c2) >> c3
 (Rsh64Ux64 (Lsh64x64 (Rsh64Ux64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
   && uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-  -> (Rsh64Ux64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+  -> (Rsh64Ux64 x (Const64 <types.UInt64> [c1-c2+c3]))
 (Rsh32Ux64 (Lsh32x64 (Rsh32Ux64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
   && uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-  -> (Rsh32Ux64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+  -> (Rsh32Ux64 x (Const64 <types.UInt64> [c1-c2+c3]))
 (Rsh16Ux64 (Lsh16x64 (Rsh16Ux64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
   && uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-  -> (Rsh16Ux64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+  -> (Rsh16Ux64 x (Const64 <types.UInt64> [c1-c2+c3]))
 (Rsh8Ux64 (Lsh8x64 (Rsh8Ux64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
   && uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-  -> (Rsh8Ux64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+  -> (Rsh8Ux64 x (Const64 <types.UInt64> [c1-c2+c3]))
 
 // ((x << c1) >> c2) << c3
 (Lsh64x64 (Rsh64Ux64 (Lsh64x64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
   && uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-  -> (Lsh64x64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+  -> (Lsh64x64 x (Const64 <types.UInt64> [c1-c2+c3]))
 (Lsh32x64 (Rsh32Ux64 (Lsh32x64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
   && uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-  -> (Lsh32x64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+  -> (Lsh32x64 x (Const64 <types.UInt64> [c1-c2+c3]))
 (Lsh16x64 (Rsh16Ux64 (Lsh16x64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
   && uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-  -> (Lsh16x64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+  -> (Lsh16x64 x (Const64 <types.UInt64> [c1-c2+c3]))
 (Lsh8x64 (Rsh8Ux64 (Lsh8x64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
   && uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-  -> (Lsh8x64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+  -> (Lsh8x64 x (Const64 <types.UInt64> [c1-c2+c3]))
 
 // replace shifts with zero extensions
-(Rsh16Ux64 (Lsh16x64 x (Const64  [8])) (Const64  [8])) -> (ZeroExt8to16  (Trunc16to8  <fe.TypeUInt8()>  x))
-(Rsh32Ux64 (Lsh32x64 x (Const64 [24])) (Const64 [24])) -> (ZeroExt8to32  (Trunc32to8  <fe.TypeUInt8()>  x))
-(Rsh64Ux64 (Lsh64x64 x (Const64 [56])) (Const64 [56])) -> (ZeroExt8to64  (Trunc64to8  <fe.TypeUInt8()>  x))
-(Rsh32Ux64 (Lsh32x64 x (Const64 [16])) (Const64 [16])) -> (ZeroExt16to32 (Trunc32to16 <fe.TypeUInt16()> x))
-(Rsh64Ux64 (Lsh64x64 x (Const64 [48])) (Const64 [48])) -> (ZeroExt16to64 (Trunc64to16 <fe.TypeUInt16()> x))
-(Rsh64Ux64 (Lsh64x64 x (Const64 [32])) (Const64 [32])) -> (ZeroExt32to64 (Trunc64to32 <fe.TypeUInt32()> x))
+(Rsh16Ux64 (Lsh16x64 x (Const64  [8])) (Const64  [8])) -> (ZeroExt8to16  (Trunc16to8  <types.UInt8>  x))
+(Rsh32Ux64 (Lsh32x64 x (Const64 [24])) (Const64 [24])) -> (ZeroExt8to32  (Trunc32to8  <types.UInt8>  x))
+(Rsh64Ux64 (Lsh64x64 x (Const64 [56])) (Const64 [56])) -> (ZeroExt8to64  (Trunc64to8  <types.UInt8>  x))
+(Rsh32Ux64 (Lsh32x64 x (Const64 [16])) (Const64 [16])) -> (ZeroExt16to32 (Trunc32to16 <types.UInt16> x))
+(Rsh64Ux64 (Lsh64x64 x (Const64 [48])) (Const64 [48])) -> (ZeroExt16to64 (Trunc64to16 <types.UInt16> x))
+(Rsh64Ux64 (Lsh64x64 x (Const64 [32])) (Const64 [32])) -> (ZeroExt32to64 (Trunc64to32 <types.UInt32> x))
 
 // replace shifts with sign extensions
-(Rsh16x64 (Lsh16x64 x (Const64  [8])) (Const64  [8])) -> (SignExt8to16  (Trunc16to8  <fe.TypeInt8()>  x))
-(Rsh32x64 (Lsh32x64 x (Const64 [24])) (Const64 [24])) -> (SignExt8to32  (Trunc32to8  <fe.TypeInt8()>  x))
-(Rsh64x64 (Lsh64x64 x (Const64 [56])) (Const64 [56])) -> (SignExt8to64  (Trunc64to8  <fe.TypeInt8()>  x))
-(Rsh32x64 (Lsh32x64 x (Const64 [16])) (Const64 [16])) -> (SignExt16to32 (Trunc32to16 <fe.TypeInt16()> x))
-(Rsh64x64 (Lsh64x64 x (Const64 [48])) (Const64 [48])) -> (SignExt16to64 (Trunc64to16 <fe.TypeInt16()> x))
-(Rsh64x64 (Lsh64x64 x (Const64 [32])) (Const64 [32])) -> (SignExt32to64 (Trunc64to32 <fe.TypeInt32()> x))
+(Rsh16x64 (Lsh16x64 x (Const64  [8])) (Const64  [8])) -> (SignExt8to16  (Trunc16to8  <types.Int8>  x))
+(Rsh32x64 (Lsh32x64 x (Const64 [24])) (Const64 [24])) -> (SignExt8to32  (Trunc32to8  <types.Int8>  x))
+(Rsh64x64 (Lsh64x64 x (Const64 [56])) (Const64 [56])) -> (SignExt8to64  (Trunc64to8  <types.Int8>  x))
+(Rsh32x64 (Lsh32x64 x (Const64 [16])) (Const64 [16])) -> (SignExt16to32 (Trunc32to16 <types.Int16> x))
+(Rsh64x64 (Lsh64x64 x (Const64 [48])) (Const64 [48])) -> (SignExt16to64 (Trunc64to16 <types.Int16> x))
+(Rsh64x64 (Lsh64x64 x (Const64 [32])) (Const64 [32])) -> (SignExt32to64 (Trunc64to32 <types.Int32> x))
 
 // constant comparisons
 (Eq64 (Const64 [c]) (Const64 [d])) -> (ConstBool [b2i(c == d)])
@@ -754,8 +754,8 @@
 
 // indexing operations
 // Note: bounds check has already been done
-(PtrIndex <t> ptr idx) && config.PtrSize == 4 -> (AddPtr ptr (Mul32 <fe.TypeInt()> idx (Const32 <fe.TypeInt()> [t.ElemType().Size()])))
-(PtrIndex <t> ptr idx) && config.PtrSize == 8 -> (AddPtr ptr (Mul64 <fe.TypeInt()> idx (Const64 <fe.TypeInt()> [t.ElemType().Size()])))
+(PtrIndex <t> ptr idx) && config.PtrSize == 4 -> (AddPtr ptr (Mul32 <types.Int> idx (Const32 <types.Int> [t.ElemType().Size()])))
+(PtrIndex <t> ptr idx) && config.PtrSize == 8 -> (AddPtr ptr (Mul64 <types.Int> idx (Const64 <types.Int> [t.ElemType().Size()])))
 
 // struct operations
 (StructSelect (StructMake1 x)) -> x
@@ -862,19 +862,19 @@
 (StringPtr (StringMake (Const64 <t> [c]) _)) -> (Const64 <t> [c])
 (StringLen (StringMake _ (Const64 <t> [c]))) -> (Const64 <t> [c])
 (ConstString {s}) && config.PtrSize == 4 && s.(string) == "" ->
-  (StringMake (ConstNil) (Const32 <fe.TypeInt()> [0]))
+  (StringMake (ConstNil) (Const32 <types.Int> [0]))
 (ConstString {s}) && config.PtrSize == 8 && s.(string) == "" ->
-  (StringMake (ConstNil) (Const64 <fe.TypeInt()> [0]))
+  (StringMake (ConstNil) (Const64 <types.Int> [0]))
 (ConstString {s}) && config.PtrSize == 4 && s.(string) != "" ->
   (StringMake
-    (Addr <fe.TypeBytePtr()> {fe.StringData(s.(string))}
+    (Addr <types.BytePtr> {fe.StringData(s.(string))}
       (SB))
-    (Const32 <fe.TypeInt()> [int64(len(s.(string)))]))
+    (Const32 <types.Int> [int64(len(s.(string)))]))
 (ConstString {s}) && config.PtrSize == 8 && s.(string) != "" ->
   (StringMake
-    (Addr <fe.TypeBytePtr()> {fe.StringData(s.(string))}
+    (Addr <types.BytePtr> {fe.StringData(s.(string))}
       (SB))
-    (Const64 <fe.TypeInt()> [int64(len(s.(string)))]))
+    (Const64 <types.Int> [int64(len(s.(string)))]))
 
 // slice ops
 // Only a few slice rules are provided here.  See dec.rules for
@@ -890,19 +890,19 @@
 (ConstSlice) && config.PtrSize == 4 ->
   (SliceMake
     (ConstNil <v.Type.ElemType().PtrTo()>)
-    (Const32 <fe.TypeInt()> [0])
-    (Const32 <fe.TypeInt()> [0]))
+    (Const32 <types.Int> [0])
+    (Const32 <types.Int> [0]))
 (ConstSlice) && config.PtrSize == 8 ->
   (SliceMake
     (ConstNil <v.Type.ElemType().PtrTo()>)
-    (Const64 <fe.TypeInt()> [0])
-    (Const64 <fe.TypeInt()> [0]))
+    (Const64 <types.Int> [0])
+    (Const64 <types.Int> [0]))
 
 // interface ops
 (ConstInterface) ->
   (IMake
-    (ConstNil <fe.TypeBytePtr()>)
-    (ConstNil <fe.TypeBytePtr()>))
+    (ConstNil <types.BytePtr>)
+    (ConstNil <types.BytePtr>))
 
 (NilCheck (GetG mem) mem) -> mem
 
@@ -918,29 +918,29 @@
 // Decompose compound argument values
 (Arg {n} [off]) && v.Type.IsString() ->
   (StringMake
-    (Arg <fe.TypeBytePtr()> {n} [off])
-    (Arg <fe.TypeInt()> {n} [off+config.PtrSize]))
+    (Arg <types.BytePtr> {n} [off])
+    (Arg <types.Int> {n} [off+config.PtrSize]))
 
 (Arg {n} [off]) && v.Type.IsSlice() ->
   (SliceMake
     (Arg <v.Type.ElemType().PtrTo()> {n} [off])
-    (Arg <fe.TypeInt()> {n} [off+config.PtrSize])
-    (Arg <fe.TypeInt()> {n} [off+2*config.PtrSize]))
+    (Arg <types.Int> {n} [off+config.PtrSize])
+    (Arg <types.Int> {n} [off+2*config.PtrSize]))
 
 (Arg {n} [off]) && v.Type.IsInterface() ->
   (IMake
-    (Arg <fe.TypeBytePtr()> {n} [off])
-    (Arg <fe.TypeBytePtr()> {n} [off+config.PtrSize]))
+    (Arg <types.BytePtr> {n} [off])
+    (Arg <types.BytePtr> {n} [off+config.PtrSize]))
 
 (Arg {n} [off]) && v.Type.IsComplex() && v.Type.Size() == 16 ->
   (ComplexMake
-    (Arg <fe.TypeFloat64()> {n} [off])
-    (Arg <fe.TypeFloat64()> {n} [off+8]))
+    (Arg <types.Float64> {n} [off])
+    (Arg <types.Float64> {n} [off+8]))
 
 (Arg {n} [off]) && v.Type.IsComplex() && v.Type.Size() == 8 ->
   (ComplexMake
-    (Arg <fe.TypeFloat32()> {n} [off])
-    (Arg <fe.TypeFloat32()> {n} [off+4]))
+    (Arg <types.Float32> {n} [off])
+    (Arg <types.Float32> {n} [off+4]))
 
 (Arg <t>) && t.IsStruct() && t.NumFields() == 0 && fe.CanSSA(t) ->
   (StructMake0)
@@ -972,125 +972,125 @@
 // See ../magic.go for a detailed description of these algorithms.
 
 // Unsigned divide by power of 2.  Strength reduce to a shift.
-(Div8u  n (Const8  [c])) && isPowerOfTwo(c&0xff)       -> (Rsh8Ux64 n  (Const64 <fe.TypeUInt64()> [log2(c&0xff)]))
-(Div16u n (Const16 [c])) && isPowerOfTwo(c&0xffff)     -> (Rsh16Ux64 n (Const64 <fe.TypeUInt64()> [log2(c&0xffff)]))
-(Div32u n (Const32 [c])) && isPowerOfTwo(c&0xffffffff) -> (Rsh32Ux64 n (Const64 <fe.TypeUInt64()> [log2(c&0xffffffff)]))
-(Div64u n (Const64 [c])) && isPowerOfTwo(c)            -> (Rsh64Ux64 n (Const64 <fe.TypeUInt64()> [log2(c)]))
+(Div8u  n (Const8  [c])) && isPowerOfTwo(c&0xff)       -> (Rsh8Ux64 n  (Const64 <types.UInt64> [log2(c&0xff)]))
+(Div16u n (Const16 [c])) && isPowerOfTwo(c&0xffff)     -> (Rsh16Ux64 n (Const64 <types.UInt64> [log2(c&0xffff)]))
+(Div32u n (Const32 [c])) && isPowerOfTwo(c&0xffffffff) -> (Rsh32Ux64 n (Const64 <types.UInt64> [log2(c&0xffffffff)]))
+(Div64u n (Const64 [c])) && isPowerOfTwo(c)            -> (Rsh64Ux64 n (Const64 <types.UInt64> [log2(c)]))
 
 // Unsigned divide, not a power of 2.  Strength reduce to a multiply.
 // For 8-bit divides, we just do a direct 9-bit by 8-bit multiply.
 (Div8u x (Const8 [c])) && umagicOK(8, c) ->
   (Trunc32to8
-    (Rsh32Ux64 <fe.TypeUInt32()>
-      (Mul32 <fe.TypeUInt32()>
-        (Const32 <fe.TypeUInt32()> [int64(1<<8+umagic(8,c).m)])
+    (Rsh32Ux64 <types.UInt32>
+      (Mul32 <types.UInt32>
+        (Const32 <types.UInt32> [int64(1<<8+umagic(8,c).m)])
         (ZeroExt8to32 x))
-      (Const64 <fe.TypeUInt64()> [8+umagic(8,c).s])))
+      (Const64 <types.UInt64> [8+umagic(8,c).s])))
 
 // For 16-bit divides on 64-bit machines, we do a direct 17-bit by 16-bit multiply.
 (Div16u x (Const16 [c])) && umagicOK(16, c) && config.RegSize == 8 ->
   (Trunc64to16
-    (Rsh64Ux64 <fe.TypeUInt64()>
-      (Mul64 <fe.TypeUInt64()>
-        (Const64 <fe.TypeUInt64()> [int64(1<<16+umagic(16,c).m)])
+    (Rsh64Ux64 <types.UInt64>
+      (Mul64 <types.UInt64>
+        (Const64 <types.UInt64> [int64(1<<16+umagic(16,c).m)])
         (ZeroExt16to64 x))
-      (Const64 <fe.TypeUInt64()> [16+umagic(16,c).s])))
+      (Const64 <types.UInt64> [16+umagic(16,c).s])))
 
 // For 16-bit divides on 32-bit machines
 (Div16u x (Const16 [c])) && umagicOK(16, c) && config.RegSize == 4 && umagic(16,c).m&1 == 0 ->
   (Trunc32to16
-    (Rsh32Ux64 <fe.TypeUInt32()>
-      (Mul32 <fe.TypeUInt32()>
-        (Const32 <fe.TypeUInt32()> [int64(1<<15+umagic(16,c).m/2)])
+    (Rsh32Ux64 <types.UInt32>
+      (Mul32 <types.UInt32>
+        (Const32 <types.UInt32> [int64(1<<15+umagic(16,c).m/2)])
         (ZeroExt16to32 x))
-      (Const64 <fe.TypeUInt64()> [16+umagic(16,c).s-1])))
+      (Const64 <types.UInt64> [16+umagic(16,c).s-1])))
 (Div16u x (Const16 [c])) && umagicOK(16, c) && config.RegSize == 4 && c&1 == 0 ->
   (Trunc32to16
-    (Rsh32Ux64 <fe.TypeUInt32()>
-      (Mul32 <fe.TypeUInt32()>
-        (Const32 <fe.TypeUInt32()> [int64(1<<15+(umagic(16,c).m+1)/2)])
-        (Rsh32Ux64 <fe.TypeUInt32()> (ZeroExt16to32 x) (Const64 <fe.TypeUInt64()> [1])))
-      (Const64 <fe.TypeUInt64()> [16+umagic(16,c).s-2])))
+    (Rsh32Ux64 <types.UInt32>
+      (Mul32 <types.UInt32>
+        (Const32 <types.UInt32> [int64(1<<15+(umagic(16,c).m+1)/2)])
+        (Rsh32Ux64 <types.UInt32> (ZeroExt16to32 x) (Const64 <types.UInt64> [1])))
+      (Const64 <types.UInt64> [16+umagic(16,c).s-2])))
 (Div16u x (Const16 [c])) && umagicOK(16, c) && config.RegSize == 4 ->
   (Trunc32to16
-    (Rsh32Ux64 <fe.TypeUInt32()>
+    (Rsh32Ux64 <types.UInt32>
       (Avg32u
-        (Lsh32x64 <fe.TypeUInt32()> (ZeroExt16to32 x) (Const64 <fe.TypeUInt64()> [16]))
-        (Mul32 <fe.TypeUInt32()>
-          (Const32 <fe.TypeUInt32()> [int64(umagic(16,c).m)])
+        (Lsh32x64 <types.UInt32> (ZeroExt16to32 x) (Const64 <types.UInt64> [16]))
+        (Mul32 <types.UInt32>
+          (Const32 <types.UInt32> [int64(umagic(16,c).m)])
           (ZeroExt16to32 x)))
-      (Const64 <fe.TypeUInt64()> [16+umagic(16,c).s-1])))
+      (Const64 <types.UInt64> [16+umagic(16,c).s-1])))
 
 // For 32-bit divides on 32-bit machines
 (Div32u x (Const32 [c])) && umagicOK(32, c) && config.RegSize == 4 && umagic(32,c).m&1 == 0 ->
-  (Rsh32Ux64 <fe.TypeUInt32()>
-    (Hmul32u <fe.TypeUInt32()>
-      (Const32 <fe.TypeUInt32()> [int64(int32(1<<31+umagic(32,c).m/2))])
+  (Rsh32Ux64 <types.UInt32>
+    (Hmul32u <types.UInt32>
+      (Const32 <types.UInt32> [int64(int32(1<<31+umagic(32,c).m/2))])
       x)
-    (Const64 <fe.TypeUInt64()> [umagic(32,c).s-1]))
+    (Const64 <types.UInt64> [umagic(32,c).s-1]))
 (Div32u x (Const32 [c])) && umagicOK(32, c) && config.RegSize == 4 && c&1 == 0 ->
-  (Rsh32Ux64 <fe.TypeUInt32()>
-    (Hmul32u <fe.TypeUInt32()>
-      (Const32 <fe.TypeUInt32()> [int64(int32(1<<31+(umagic(32,c).m+1)/2))])
-      (Rsh32Ux64 <fe.TypeUInt32()> x (Const64 <fe.TypeUInt64()> [1])))
-    (Const64 <fe.TypeUInt64()> [umagic(32,c).s-2]))
+  (Rsh32Ux64 <types.UInt32>
+    (Hmul32u <types.UInt32>
+      (Const32 <types.UInt32> [int64(int32(1<<31+(umagic(32,c).m+1)/2))])
+      (Rsh32Ux64 <types.UInt32> x (Const64 <types.UInt64> [1])))
+    (Const64 <types.UInt64> [umagic(32,c).s-2]))
 (Div32u x (Const32 [c])) && umagicOK(32, c) && config.RegSize == 4 ->
-  (Rsh32Ux64 <fe.TypeUInt32()>
+  (Rsh32Ux64 <types.UInt32>
     (Avg32u
       x
-      (Hmul32u <fe.TypeUInt32()>
-        (Const32 <fe.TypeUInt32()> [int64(int32(umagic(32,c).m))])
+      (Hmul32u <types.UInt32>
+        (Const32 <types.UInt32> [int64(int32(umagic(32,c).m))])
         x))
-    (Const64 <fe.TypeUInt64()> [umagic(32,c).s-1]))
+    (Const64 <types.UInt64> [umagic(32,c).s-1]))
 
 // For 32-bit divides on 64-bit machines
 // We'll use a regular (non-hi) multiply for this case.
 (Div32u x (Const32 [c])) && umagicOK(32, c) && config.RegSize == 8 && umagic(32,c).m&1 == 0 ->
   (Trunc64to32
-    (Rsh64Ux64 <fe.TypeUInt64()>
-      (Mul64 <fe.TypeUInt64()>
-        (Const64 <fe.TypeUInt64()> [int64(1<<31+umagic(32,c).m/2)])
+    (Rsh64Ux64 <types.UInt64>
+      (Mul64 <types.UInt64>
+        (Const64 <types.UInt64> [int64(1<<31+umagic(32,c).m/2)])
         (ZeroExt32to64 x))
-      (Const64 <fe.TypeUInt64()> [32+umagic(32,c).s-1])))
+      (Const64 <types.UInt64> [32+umagic(32,c).s-1])))
 (Div32u x (Const32 [c])) && umagicOK(32, c) && config.RegSize == 8 && c&1 == 0 ->
   (Trunc64to32
-    (Rsh64Ux64 <fe.TypeUInt64()>
-      (Mul64 <fe.TypeUInt64()>
-        (Const64 <fe.TypeUInt64()> [int64(1<<31+(umagic(32,c).m+1)/2)])
-        (Rsh64Ux64 <fe.TypeUInt64()> (ZeroExt32to64 x) (Const64 <fe.TypeUInt64()> [1])))
-      (Const64 <fe.TypeUInt64()> [32+umagic(32,c).s-2])))
+    (Rsh64Ux64 <types.UInt64>
+      (Mul64 <types.UInt64>
+        (Const64 <types.UInt64> [int64(1<<31+(umagic(32,c).m+1)/2)])
+        (Rsh64Ux64 <types.UInt64> (ZeroExt32to64 x) (Const64 <types.UInt64> [1])))
+      (Const64 <types.UInt64> [32+umagic(32,c).s-2])))
 (Div32u x (Const32 [c])) && umagicOK(32, c) && config.RegSize == 8 ->
   (Trunc64to32
-    (Rsh64Ux64 <fe.TypeUInt64()>
+    (Rsh64Ux64 <types.UInt64>
       (Avg64u
-        (Lsh64x64 <fe.TypeUInt64()> (ZeroExt32to64 x) (Const64 <fe.TypeUInt64()> [32]))
-        (Mul64 <fe.TypeUInt64()>
-          (Const64 <fe.TypeUInt32()> [int64(umagic(32,c).m)])
+        (Lsh64x64 <types.UInt64> (ZeroExt32to64 x) (Const64 <types.UInt64> [32]))
+        (Mul64 <types.UInt64>
+          (Const64 <types.UInt32> [int64(umagic(32,c).m)])
           (ZeroExt32to64 x)))
-      (Const64 <fe.TypeUInt64()> [32+umagic(32,c).s-1])))
+      (Const64 <types.UInt64> [32+umagic(32,c).s-1])))
 
 // For 64-bit divides on 64-bit machines
 // (64-bit divides on 32-bit machines are lowered to a runtime call by the walk pass.)
 (Div64u x (Const64 [c])) && umagicOK(64, c) && config.RegSize == 8 && umagic(64,c).m&1 == 0 ->
-  (Rsh64Ux64 <fe.TypeUInt64()>
-    (Hmul64u <fe.TypeUInt64()>
-      (Const64 <fe.TypeUInt64()> [int64(1<<63+umagic(64,c).m/2)])
+  (Rsh64Ux64 <types.UInt64>
+    (Hmul64u <types.UInt64>
+      (Const64 <types.UInt64> [int64(1<<63+umagic(64,c).m/2)])
       x)
-    (Const64 <fe.TypeUInt64()> [umagic(64,c).s-1]))
+    (Const64 <types.UInt64> [umagic(64,c).s-1]))
 (Div64u x (Const64 [c])) && umagicOK(64, c) && config.RegSize == 8 && c&1 == 0 ->
-  (Rsh64Ux64 <fe.TypeUInt64()>
-    (Hmul64u <fe.TypeUInt64()>
-      (Const64 <fe.TypeUInt64()> [int64(1<<63+(umagic(64,c).m+1)/2)])
-      (Rsh64Ux64 <fe.TypeUInt64()> x (Const64 <fe.TypeUInt64()> [1])))
-    (Const64 <fe.TypeUInt64()> [umagic(64,c).s-2]))
+  (Rsh64Ux64 <types.UInt64>
+    (Hmul64u <types.UInt64>
+      (Const64 <types.UInt64> [int64(1<<63+(umagic(64,c).m+1)/2)])
+      (Rsh64Ux64 <types.UInt64> x (Const64 <types.UInt64> [1])))
+    (Const64 <types.UInt64> [umagic(64,c).s-2]))
 (Div64u x (Const64 [c])) && umagicOK(64, c) && config.RegSize == 8 ->
-  (Rsh64Ux64 <fe.TypeUInt64()>
+  (Rsh64Ux64 <types.UInt64>
     (Avg64u
       x
-      (Hmul64u <fe.TypeUInt64()>
-        (Const64 <fe.TypeUInt64()> [int64(umagic(64,c).m)])
+      (Hmul64u <types.UInt64>
+        (Const64 <types.UInt64> [int64(umagic(64,c).m)])
         x))
-    (Const64 <fe.TypeUInt64()> [umagic(64,c).s-1]))
+    (Const64 <types.UInt64> [umagic(64,c).s-1]))
 
 // Signed divide by a negative constant.  Rewrite to divide by a positive constant.
 (Div8  <t> n (Const8  [c])) && c < 0 && c != -1<<7  -> (Neg8  (Div8  <t> n (Const8  <t> [-c])))
@@ -1101,10 +1101,10 @@
 // Dividing by the most-negative number.  Result is always 0 except
 // if the input is also the most-negative number.
 // We can detect that using the sign bit of x & -x.
-(Div8  <t> x (Const8  [-1<<7 ])) -> (Rsh8Ux64  (And8  <t> x (Neg8  <t> x)) (Const64 <fe.TypeUInt64()> [7 ]))
-(Div16 <t> x (Const16 [-1<<15])) -> (Rsh16Ux64 (And16 <t> x (Neg16 <t> x)) (Const64 <fe.TypeUInt64()> [15]))
-(Div32 <t> x (Const32 [-1<<31])) -> (Rsh32Ux64 (And32 <t> x (Neg32 <t> x)) (Const64 <fe.TypeUInt64()> [31]))
-(Div64 <t> x (Const64 [-1<<63])) -> (Rsh64Ux64 (And64 <t> x (Neg64 <t> x)) (Const64 <fe.TypeUInt64()> [63]))
+(Div8  <t> x (Const8  [-1<<7 ])) -> (Rsh8Ux64  (And8  <t> x (Neg8  <t> x)) (Const64 <types.UInt64> [7 ]))
+(Div16 <t> x (Const16 [-1<<15])) -> (Rsh16Ux64 (And16 <t> x (Neg16 <t> x)) (Const64 <types.UInt64> [15]))
+(Div32 <t> x (Const32 [-1<<31])) -> (Rsh32Ux64 (And32 <t> x (Neg32 <t> x)) (Const64 <types.UInt64> [31]))
+(Div64 <t> x (Const64 [-1<<63])) -> (Rsh64Ux64 (And64 <t> x (Neg64 <t> x)) (Const64 <types.UInt64> [63]))
 
 // Signed divide by power of 2.
 // n / c =       n >> log(c) if n >= 0
@@ -1112,96 +1112,96 @@
 // We conditionally add c-1 by adding n>>63>>(64-log(c)) (first shift signed, second shift unsigned).
 (Div8  <t> n (Const8  [c])) && isPowerOfTwo(c) ->
   (Rsh8x64
-    (Add8  <t> n (Rsh8Ux64  <t> (Rsh8x64  <t> n (Const64 <fe.TypeUInt64()> [ 7])) (Const64 <fe.TypeUInt64()> [ 8-log2(c)])))
-    (Const64 <fe.TypeUInt64()> [log2(c)]))
+    (Add8  <t> n (Rsh8Ux64  <t> (Rsh8x64  <t> n (Const64 <types.UInt64> [ 7])) (Const64 <types.UInt64> [ 8-log2(c)])))
+    (Const64 <types.UInt64> [log2(c)]))
 (Div16 <t> n (Const16 [c])) && isPowerOfTwo(c) ->
   (Rsh16x64
-    (Add16 <t> n (Rsh16Ux64 <t> (Rsh16x64 <t> n (Const64 <fe.TypeUInt64()> [15])) (Const64 <fe.TypeUInt64()> [16-log2(c)])))
-    (Const64 <fe.TypeUInt64()> [log2(c)]))
+    (Add16 <t> n (Rsh16Ux64 <t> (Rsh16x64 <t> n (Const64 <types.UInt64> [15])) (Const64 <types.UInt64> [16-log2(c)])))
+    (Const64 <types.UInt64> [log2(c)]))
 (Div32 <t> n (Const32 [c])) && isPowerOfTwo(c) ->
   (Rsh32x64
-    (Add32 <t> n (Rsh32Ux64 <t> (Rsh32x64 <t> n (Const64 <fe.TypeUInt64()> [31])) (Const64 <fe.TypeUInt64()> [32-log2(c)])))
-    (Const64 <fe.TypeUInt64()> [log2(c)]))
+    (Add32 <t> n (Rsh32Ux64 <t> (Rsh32x64 <t> n (Const64 <types.UInt64> [31])) (Const64 <types.UInt64> [32-log2(c)])))
+    (Const64 <types.UInt64> [log2(c)]))
 (Div64 <t> n (Const64 [c])) && isPowerOfTwo(c) ->
   (Rsh64x64
-    (Add64 <t> n (Rsh64Ux64 <t> (Rsh64x64 <t> n (Const64 <fe.TypeUInt64()> [63])) (Const64 <fe.TypeUInt64()> [64-log2(c)])))
-    (Const64 <fe.TypeUInt64()> [log2(c)]))
+    (Add64 <t> n (Rsh64Ux64 <t> (Rsh64x64 <t> n (Const64 <types.UInt64> [63])) (Const64 <types.UInt64> [64-log2(c)])))
+    (Const64 <types.UInt64> [log2(c)]))
 
 // Signed divide, not a power of 2.  Strength reduce to a multiply.
 (Div8 <t> x (Const8 [c])) && smagicOK(8,c) ->
   (Sub8 <t>
     (Rsh32x64 <t>
-      (Mul32 <fe.TypeUInt32()>
-        (Const32 <fe.TypeUInt32()> [int64(smagic(8,c).m)])
+      (Mul32 <types.UInt32>
+        (Const32 <types.UInt32> [int64(smagic(8,c).m)])
         (SignExt8to32 x))
-      (Const64 <fe.TypeUInt64()> [8+smagic(8,c).s]))
+      (Const64 <types.UInt64> [8+smagic(8,c).s]))
     (Rsh32x64 <t>
       (SignExt8to32 x)
-      (Const64 <fe.TypeUInt64()> [31])))
+      (Const64 <types.UInt64> [31])))
 (Div16 <t> x (Const16 [c])) && smagicOK(16,c) ->
   (Sub16 <t>
     (Rsh32x64 <t>
-      (Mul32 <fe.TypeUInt32()>
-        (Const32 <fe.TypeUInt32()> [int64(smagic(16,c).m)])
+      (Mul32 <types.UInt32>
+        (Const32 <types.UInt32> [int64(smagic(16,c).m)])
         (SignExt16to32 x))
-      (Const64 <fe.TypeUInt64()> [16+smagic(16,c).s]))
+      (Const64 <types.UInt64> [16+smagic(16,c).s]))
     (Rsh32x64 <t>
       (SignExt16to32 x)
-      (Const64 <fe.TypeUInt64()> [31])))
+      (Const64 <types.UInt64> [31])))
 (Div32 <t> x (Const32 [c])) && smagicOK(32,c) && config.RegSize == 8 ->
   (Sub32 <t>
     (Rsh64x64 <t>
-      (Mul64 <fe.TypeUInt64()>
-        (Const64 <fe.TypeUInt64()> [int64(smagic(32,c).m)])
+      (Mul64 <types.UInt64>
+        (Const64 <types.UInt64> [int64(smagic(32,c).m)])
         (SignExt32to64 x))
-      (Const64 <fe.TypeUInt64()> [32+smagic(32,c).s]))
+      (Const64 <types.UInt64> [32+smagic(32,c).s]))
     (Rsh64x64 <t>
       (SignExt32to64 x)
-      (Const64 <fe.TypeUInt64()> [63])))
+      (Const64 <types.UInt64> [63])))
 (Div32 <t> x (Const32 [c])) && smagicOK(32,c) && config.RegSize == 4 && smagic(32,c).m&1 == 0 ->
   (Sub32 <t>
     (Rsh32x64 <t>
       (Hmul32 <t>
-        (Const32 <fe.TypeUInt32()> [int64(int32(smagic(32,c).m/2))])
+        (Const32 <types.UInt32> [int64(int32(smagic(32,c).m/2))])
         x)
-      (Const64 <fe.TypeUInt64()> [smagic(32,c).s-1]))
+      (Const64 <types.UInt64> [smagic(32,c).s-1]))
     (Rsh32x64 <t>
       x
-      (Const64 <fe.TypeUInt64()> [31])))
+      (Const64 <types.UInt64> [31])))
 (Div32 <t> x (Const32 [c])) && smagicOK(32,c) && config.RegSize == 4 && smagic(32,c).m&1 != 0 ->
   (Sub32 <t>
     (Rsh32x64 <t>
       (Add32 <t>
         (Hmul32 <t>
-          (Const32 <fe.TypeUInt32()> [int64(int32(smagic(32,c).m))])
+          (Const32 <types.UInt32> [int64(int32(smagic(32,c).m))])
           x)
         x)
-      (Const64 <fe.TypeUInt64()> [smagic(32,c).s]))
+      (Const64 <types.UInt64> [smagic(32,c).s]))
     (Rsh32x64 <t>
       x
-      (Const64 <fe.TypeUInt64()> [31])))
+      (Const64 <types.UInt64> [31])))
 (Div64 <t> x (Const64 [c])) && smagicOK(64,c) && smagic(64,c).m&1 == 0 ->
   (Sub64 <t>
     (Rsh64x64 <t>
       (Hmul64 <t>
-        (Const64 <fe.TypeUInt64()> [int64(smagic(64,c).m/2)])
+        (Const64 <types.UInt64> [int64(smagic(64,c).m/2)])
         x)
-      (Const64 <fe.TypeUInt64()> [smagic(64,c).s-1]))
+      (Const64 <types.UInt64> [smagic(64,c).s-1]))
     (Rsh64x64 <t>
       x
-      (Const64 <fe.TypeUInt64()> [63])))
+      (Const64 <types.UInt64> [63])))
 (Div64 <t> x (Const64 [c])) && smagicOK(64,c) && smagic(64,c).m&1 != 0 ->
   (Sub64 <t>
     (Rsh64x64 <t>
       (Add64 <t>
         (Hmul64 <t>
-          (Const64 <fe.TypeUInt64()> [int64(smagic(64,c).m)])
+          (Const64 <types.UInt64> [int64(smagic(64,c).m)])
           x)
         x)
-      (Const64 <fe.TypeUInt64()> [smagic(64,c).s]))
+      (Const64 <types.UInt64> [smagic(64,c).s]))
     (Rsh64x64 <t>
       x
-      (Const64 <fe.TypeUInt64()> [63])))
+      (Const64 <types.UInt64> [63])))
 
 // Unsigned mod by power of 2 constant.
 (Mod8u  <t> n (Const8  [c])) && isPowerOfTwo(c&0xff)       -> (And8 n (Const8 <t> [(c&0xff)-1]))
diff --git a/src/cmd/compile/internal/ssa/gen/rulegen.go b/src/cmd/compile/internal/ssa/gen/rulegen.go
index bb3291d55f..beabca97d0 100644
--- a/src/cmd/compile/internal/ssa/gen/rulegen.go
+++ b/src/cmd/compile/internal/ssa/gen/rulegen.go
@@ -204,12 +204,13 @@ func genRules(arch arch) {
 		}
 
 		body := buf.String()
-		// Do a rough match to predict whether we need b, config, and/or fe.
+		// Do a rough match to predict whether we need b, config, fe, and/or types.
 		// It's not precise--thus the blank assignments--but it's good enough
 		// to avoid generating needless code and doing pointless nil checks.
 		hasb := strings.Contains(body, "b.")
 		hasconfig := strings.Contains(body, "config.") || strings.Contains(body, "config)")
 		hasfe := strings.Contains(body, "fe.")
+		hasts := strings.Contains(body, "types.")
 		fmt.Fprintf(w, "func rewriteValue%s_%s(v *Value) bool {\n", arch.name, op)
 		if hasb || hasconfig || hasfe {
 			fmt.Fprintln(w, "b := v.Block")
@@ -223,6 +224,10 @@ func genRules(arch arch) {
 			fmt.Fprintln(w, "fe := b.Func.fe")
 			fmt.Fprintln(w, "_ = fe")
 		}
+		if hasts {
+			fmt.Fprintln(w, "types := &b.Func.Config.Types")
+			fmt.Fprintln(w, "_ = types")
+		}
 		fmt.Fprint(w, body)
 		fmt.Fprintf(w, "}\n")
 	}
@@ -234,6 +239,8 @@ func genRules(arch arch) {
 	fmt.Fprintln(w, "_ = config")
 	fmt.Fprintln(w, "fe := b.Func.fe")
 	fmt.Fprintln(w, "_ = fe")
+	fmt.Fprintln(w, "types := &config.Types")
+	fmt.Fprintln(w, "_ = types")
 	fmt.Fprintf(w, "switch b.Kind {\n")
 	ops = nil
 	for op := range blockrules {
@@ -719,7 +726,7 @@ func typeName(typ string) string {
 	case "Flags", "Mem", "Void", "Int128":
 		return "Type" + typ
 	default:
-		return "fe.Type" + typ + "()"
+		return "types." + typ
 	}
 }
 
diff --git a/src/cmd/compile/internal/ssa/loopreschedchecks.go b/src/cmd/compile/internal/ssa/loopreschedchecks.go
index b6dfebd612..863fc9ccb7 100644
--- a/src/cmd/compile/internal/ssa/loopreschedchecks.go
+++ b/src/cmd/compile/internal/ssa/loopreschedchecks.go
@@ -197,7 +197,8 @@ func insertLoopReschedChecks(f *Func) {
 		// if sp < g.limit { goto sched }
 		// goto header
 
-		pt := f.fe.TypeUintptr()
+		types := &f.Config.Types
+		pt := types.Uintptr
 		g := test.NewValue1(bb.Pos, OpGetG, pt, mem0)
 		sp := test.NewValue0(bb.Pos, OpSP, pt)
 		cmpOp := OpLess64U
@@ -206,7 +207,7 @@ func insertLoopReschedChecks(f *Func) {
 		}
 		limaddr := test.NewValue1I(bb.Pos, OpOffPtr, pt, 2*pt.Size(), g)
 		lim := test.NewValue2(bb.Pos, OpLoad, pt, limaddr, mem0)
-		cmp := test.NewValue2(bb.Pos, cmpOp, f.fe.TypeBool(), sp, lim)
+		cmp := test.NewValue2(bb.Pos, cmpOp, types.Bool, sp, lim)
 		test.SetControl(cmp)
 
 		// if true, goto sched
diff --git a/src/cmd/compile/internal/ssa/regalloc.go b/src/cmd/compile/internal/ssa/regalloc.go
index 03f1f7ce32..a1b07433ae 100644
--- a/src/cmd/compile/internal/ssa/regalloc.go
+++ b/src/cmd/compile/internal/ssa/regalloc.go
@@ -2055,10 +2055,11 @@ func (e *edgeState) erase(loc Location) {
 func (e *edgeState) findRegFor(typ Type) Location {
 	// Which registers are possibilities.
 	var m regMask
+	types := &e.s.f.Config.Types
 	if typ.IsFloat() {
-		m = e.s.compatRegs(e.s.f.fe.TypeFloat64())
+		m = e.s.compatRegs(types.Float64)
 	} else {
-		m = e.s.compatRegs(e.s.f.fe.TypeInt64())
+		m = e.s.compatRegs(types.Int64)
 	}
 
 	// Pick a register. In priority order:
@@ -2082,7 +2083,7 @@ func (e *edgeState) findRegFor(typ Type) Location {
 	// No register is available. Allocate a temp location to spill a register to.
 	// The type of the slot is immaterial - it will not be live across
 	// any safepoint. Just use a type big enough to hold any register.
-	typ = e.s.f.fe.TypeInt64()
+	typ = types.Int64
 	t := LocalSlot{e.s.f.fe.Auto(typ), typ, 0}
 	// TODO: reuse these slots.
 
diff --git a/src/cmd/compile/internal/ssa/rewrite386.go b/src/cmd/compile/internal/ssa/rewrite386.go
index f96b0c63d5..ba5288de2a 100644
--- a/src/cmd/compile/internal/ssa/rewrite386.go
+++ b/src/cmd/compile/internal/ssa/rewrite386.go
@@ -4354,8 +4354,8 @@ func rewriteValue386_Op386MOVSDconst(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (MOVSDconst [c])
 	// cond: config.ctxt.Flag_shared
 	// result: (MOVSDconst2 (MOVSDconst1 [c]))
@@ -4365,7 +4365,7 @@ func rewriteValue386_Op386MOVSDconst(v *Value) bool {
 			break
 		}
 		v.reset(Op386MOVSDconst2)
-		v0 := b.NewValue0(v.Pos, Op386MOVSDconst1, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, Op386MOVSDconst1, types.UInt32)
 		v0.AuxInt = c
 		v.AddArg(v0)
 		return true
@@ -4843,8 +4843,8 @@ func rewriteValue386_Op386MOVSSconst(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (MOVSSconst [c])
 	// cond: config.ctxt.Flag_shared
 	// result: (MOVSSconst2 (MOVSSconst1 [c]))
@@ -4854,7 +4854,7 @@ func rewriteValue386_Op386MOVSSconst(v *Value) bool {
 			break
 		}
 		v.reset(Op386MOVSSconst2)
-		v0 := b.NewValue0(v.Pos, Op386MOVSSconst1, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, Op386MOVSSconst1, types.UInt32)
 		v0.AuxInt = c
 		v.AddArg(v0)
 		return true
@@ -7212,8 +7212,8 @@ func rewriteValue386_Op386NOTL(v *Value) bool {
 func rewriteValue386_Op386ORL(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (ORL x (MOVLconst [c]))
 	// cond:
 	// result: (ORLconst [c] x)
@@ -7479,7 +7479,7 @@ func rewriteValue386_Op386ORL(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1)
-		v0 := b.NewValue0(v.Pos, Op386MOVWload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, Op386MOVWload, types.UInt16)
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v0.AuxInt = i
@@ -7554,7 +7554,7 @@ func rewriteValue386_Op386ORL(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1, x2)
-		v0 := b.NewValue0(v.Pos, Op386MOVLload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, Op386MOVLload, types.UInt32)
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v0.AuxInt = i
@@ -9890,8 +9890,8 @@ func rewriteValue386_OpDiv64F(v *Value) bool {
 func rewriteValue386_OpDiv8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8   x y)
 	// cond:
 	// result: (DIVW  (SignExt8to16 x) (SignExt8to16 y))
@@ -9899,10 +9899,10 @@ func rewriteValue386_OpDiv8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(Op386DIVW)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to16, fe.TypeInt16())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to16, types.Int16)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to16, fe.TypeInt16())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to16, types.Int16)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -9911,8 +9911,8 @@ func rewriteValue386_OpDiv8(v *Value) bool {
 func rewriteValue386_OpDiv8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8u  x y)
 	// cond:
 	// result: (DIVWU (ZeroExt8to16 x) (ZeroExt8to16 y))
@@ -9920,10 +9920,10 @@ func rewriteValue386_OpDiv8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(Op386DIVWU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to16, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to16, types.UInt16)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to16, fe.TypeUInt16())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to16, types.UInt16)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -11163,8 +11163,8 @@ func rewriteValue386_OpMod32u(v *Value) bool {
 func rewriteValue386_OpMod8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8   x y)
 	// cond:
 	// result: (MODW  (SignExt8to16 x) (SignExt8to16 y))
@@ -11172,10 +11172,10 @@ func rewriteValue386_OpMod8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(Op386MODW)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to16, fe.TypeInt16())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to16, types.Int16)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to16, fe.TypeInt16())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to16, types.Int16)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -11184,8 +11184,8 @@ func rewriteValue386_OpMod8(v *Value) bool {
 func rewriteValue386_OpMod8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8u  x y)
 	// cond:
 	// result: (MODWU (ZeroExt8to16 x) (ZeroExt8to16 y))
@@ -11193,10 +11193,10 @@ func rewriteValue386_OpMod8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(Op386MODWU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to16, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to16, types.UInt16)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to16, fe.TypeUInt16())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to16, types.UInt16)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -11207,8 +11207,8 @@ func rewriteValue386_OpMove(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Move [0] _ _ mem)
 	// cond:
 	// result: mem
@@ -11234,7 +11234,7 @@ func rewriteValue386_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(Op386MOVBstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, Op386MOVBload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, Op386MOVBload, types.UInt8)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -11253,7 +11253,7 @@ func rewriteValue386_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(Op386MOVWstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, Op386MOVWload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, Op386MOVWload, types.UInt16)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -11272,7 +11272,7 @@ func rewriteValue386_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(Op386MOVLstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, Op386MOVLload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, Op386MOVLload, types.UInt32)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -11292,14 +11292,14 @@ func rewriteValue386_OpMove(v *Value) bool {
 		v.reset(Op386MOVBstore)
 		v.AuxInt = 2
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, Op386MOVBload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, Op386MOVBload, types.UInt8)
 		v0.AuxInt = 2
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, Op386MOVWstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, Op386MOVWload, fe.TypeUInt16())
+		v2 := b.NewValue0(v.Pos, Op386MOVWload, types.UInt16)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -11320,14 +11320,14 @@ func rewriteValue386_OpMove(v *Value) bool {
 		v.reset(Op386MOVBstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, Op386MOVBload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, Op386MOVBload, types.UInt8)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, Op386MOVLstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, Op386MOVLload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, Op386MOVLload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -11348,14 +11348,14 @@ func rewriteValue386_OpMove(v *Value) bool {
 		v.reset(Op386MOVWstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, Op386MOVWload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, Op386MOVWload, types.UInt16)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, Op386MOVLstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, Op386MOVLload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, Op386MOVLload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -11376,14 +11376,14 @@ func rewriteValue386_OpMove(v *Value) bool {
 		v.reset(Op386MOVLstore)
 		v.AuxInt = 3
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, Op386MOVLload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, Op386MOVLload, types.UInt32)
 		v0.AuxInt = 3
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, Op386MOVLstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, Op386MOVLload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, Op386MOVLload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -11404,14 +11404,14 @@ func rewriteValue386_OpMove(v *Value) bool {
 		v.reset(Op386MOVLstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, Op386MOVLload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, Op386MOVLload, types.UInt32)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, Op386MOVLstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, Op386MOVLload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, Op386MOVLload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -11442,7 +11442,7 @@ func rewriteValue386_OpMove(v *Value) bool {
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, Op386MOVLstore, TypeMem)
 		v2.AddArg(dst)
-		v3 := b.NewValue0(v.Pos, Op386MOVLload, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, Op386MOVLload, types.UInt32)
 		v3.AddArg(src)
 		v3.AddArg(mem)
 		v2.AddArg(v3)
@@ -11482,7 +11482,7 @@ func rewriteValue386_OpMove(v *Value) bool {
 		v.reset(Op386REPMOVSL)
 		v.AddArg(dst)
 		v.AddArg(src)
-		v0 := b.NewValue0(v.Pos, Op386MOVLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, Op386MOVLconst, types.UInt32)
 		v0.AuxInt = s / 4
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -11595,11 +11595,11 @@ func rewriteValue386_OpNeg32F(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neg32F x)
 	// cond: !config.use387
-	// result: (PXOR x (MOVSSconst <fe.TypeFloat32()> [f2i(math.Copysign(0, -1))]))
+	// result: (PXOR x (MOVSSconst <types.Float32> [f2i(math.Copysign(0, -1))]))
 	for {
 		x := v.Args[0]
 		if !(!config.use387) {
@@ -11607,7 +11607,7 @@ func rewriteValue386_OpNeg32F(v *Value) bool {
 		}
 		v.reset(Op386PXOR)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, Op386MOVSSconst, fe.TypeFloat32())
+		v0 := b.NewValue0(v.Pos, Op386MOVSSconst, types.Float32)
 		v0.AuxInt = f2i(math.Copysign(0, -1))
 		v.AddArg(v0)
 		return true
@@ -11631,11 +11631,11 @@ func rewriteValue386_OpNeg64F(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neg64F x)
 	// cond: !config.use387
-	// result: (PXOR x (MOVSDconst <fe.TypeFloat64()> [f2i(math.Copysign(0, -1))]))
+	// result: (PXOR x (MOVSDconst <types.Float64> [f2i(math.Copysign(0, -1))]))
 	for {
 		x := v.Args[0]
 		if !(!config.use387) {
@@ -11643,7 +11643,7 @@ func rewriteValue386_OpNeg64F(v *Value) bool {
 		}
 		v.reset(Op386PXOR)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, Op386MOVSDconst, fe.TypeFloat64())
+		v0 := b.NewValue0(v.Pos, Op386MOVSDconst, types.Float64)
 		v0.AuxInt = f2i(math.Copysign(0, -1))
 		v.AddArg(v0)
 		return true
@@ -12955,8 +12955,8 @@ func rewriteValue386_OpZero(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Zero [0] _ mem)
 	// cond:
 	// result: mem
@@ -13103,7 +13103,7 @@ func rewriteValue386_OpZero(v *Value) bool {
 		}
 		v.reset(OpZero)
 		v.AuxInt = s - s%4
-		v0 := b.NewValue0(v.Pos, Op386ADDLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, Op386ADDLconst, types.UInt32)
 		v0.AuxInt = s % 4
 		v0.AddArg(destptr)
 		v.AddArg(v0)
@@ -13196,7 +13196,7 @@ func rewriteValue386_OpZero(v *Value) bool {
 		v.reset(Op386DUFFZERO)
 		v.AuxInt = 1 * (128 - s/4)
 		v.AddArg(destptr)
-		v0 := b.NewValue0(v.Pos, Op386MOVLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, Op386MOVLconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -13214,10 +13214,10 @@ func rewriteValue386_OpZero(v *Value) bool {
 		}
 		v.reset(Op386REPSTOSL)
 		v.AddArg(destptr)
-		v0 := b.NewValue0(v.Pos, Op386MOVLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, Op386MOVLconst, types.UInt32)
 		v0.AuxInt = s / 4
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, Op386MOVLconst, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, Op386MOVLconst, types.UInt32)
 		v1.AuxInt = 0
 		v.AddArg(v1)
 		v.AddArg(mem)
@@ -13283,6 +13283,8 @@ func rewriteBlock386(b *Block) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &config.Types
+	_ = types
 	switch b.Kind {
 	case Block386EQ:
 		// match: (EQ (InvertFlags cmp) yes no)
diff --git a/src/cmd/compile/internal/ssa/rewriteAMD64.go b/src/cmd/compile/internal/ssa/rewriteAMD64.go
index 223b470cad..d14edbb74e 100644
--- a/src/cmd/compile/internal/ssa/rewriteAMD64.go
+++ b/src/cmd/compile/internal/ssa/rewriteAMD64.go
@@ -6786,8 +6786,8 @@ func rewriteValueAMD64_OpAMD64MOVLstore(v *Value) bool {
 func rewriteValueAMD64_OpAMD64MOVLstoreconst(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (MOVLstoreconst [sc] {s} (ADDQconst [off] ptr) mem)
 	// cond: ValAndOff(sc).canAdd(off)
 	// result: (MOVLstoreconst [ValAndOff(sc).add(off)] {s} ptr mem)
@@ -6934,7 +6934,7 @@ func rewriteValueAMD64_OpAMD64MOVLstoreconst(v *Value) bool {
 		v.AuxInt = ValAndOff(a).Off()
 		v.Aux = s
 		v.AddArg(p)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, types.UInt64)
 		v0.AuxInt = ValAndOff(a).Val()&0xffffffff | ValAndOff(c).Val()<<32
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -6992,8 +6992,8 @@ func rewriteValueAMD64_OpAMD64MOVLstoreconst(v *Value) bool {
 func rewriteValueAMD64_OpAMD64MOVLstoreconstidx1(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (MOVLstoreconstidx1 [c] {sym} ptr (SHLQconst [2] idx) mem)
 	// cond:
 	// result: (MOVLstoreconstidx4 [c] {sym} ptr idx mem)
@@ -7093,7 +7093,7 @@ func rewriteValueAMD64_OpAMD64MOVLstoreconstidx1(v *Value) bool {
 		v.Aux = s
 		v.AddArg(p)
 		v.AddArg(i)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, types.UInt64)
 		v0.AuxInt = ValAndOff(a).Val()&0xffffffff | ValAndOff(c).Val()<<32
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -7104,8 +7104,8 @@ func rewriteValueAMD64_OpAMD64MOVLstoreconstidx1(v *Value) bool {
 func rewriteValueAMD64_OpAMD64MOVLstoreconstidx4(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (MOVLstoreconstidx4 [x] {sym} (ADDQconst [c] ptr) idx mem)
 	// cond:
 	// result: (MOVLstoreconstidx4 [ValAndOff(x).add(c)] {sym} ptr idx mem)
@@ -7184,7 +7184,7 @@ func rewriteValueAMD64_OpAMD64MOVLstoreconstidx4(v *Value) bool {
 		v0.AuxInt = 2
 		v0.AddArg(i)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpAMD64MOVQconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpAMD64MOVQconst, types.UInt64)
 		v1.AuxInt = ValAndOff(a).Val()&0xffffffff | ValAndOff(c).Val()<<32
 		v.AddArg(v1)
 		v.AddArg(mem)
@@ -12037,8 +12037,8 @@ func rewriteValueAMD64_OpAMD64NOTQ(v *Value) bool {
 func rewriteValueAMD64_OpAMD64ORL(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (ORL x (MOVLconst [c]))
 	// cond:
 	// result: (ORLconst [c] x)
@@ -12304,7 +12304,7 @@ func rewriteValueAMD64_OpAMD64ORL(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVWload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVWload, types.UInt16)
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v0.AuxInt = i
@@ -12379,7 +12379,7 @@ func rewriteValueAMD64_OpAMD64ORL(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1, x2)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, types.UInt32)
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v0.AuxInt = i
@@ -12567,7 +12567,7 @@ func rewriteValueAMD64_OpAMD64ORL(v *Value) bool {
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v0.AuxInt = 8
-		v1 := b.NewValue0(v.Pos, OpAMD64MOVWload, fe.TypeUInt16())
+		v1 := b.NewValue0(v.Pos, OpAMD64MOVWload, types.UInt16)
 		v1.AuxInt = i - 1
 		v1.Aux = s
 		v1.AddArg(p)
@@ -12707,7 +12707,7 @@ func rewriteValueAMD64_OpAMD64ORL(v *Value) bool {
 		v0 := b.NewValue0(v.Pos, OpAMD64BSWAPL, v.Type)
 		v.reset(OpCopy)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpAMD64MOVLload, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpAMD64MOVLload, types.UInt32)
 		v1.AuxInt = i1 - 2
 		v1.Aux = s
 		v1.AddArg(p)
@@ -12903,8 +12903,8 @@ func rewriteValueAMD64_OpAMD64ORLconst(v *Value) bool {
 func rewriteValueAMD64_OpAMD64ORQ(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (ORQ x (MOVQconst [c]))
 	// cond: is32Bit(c)
 	// result: (ORQconst [c] x)
@@ -13222,7 +13222,7 @@ func rewriteValueAMD64_OpAMD64ORQ(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1, x2, x3, x4, x5, x6, x7)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, types.UInt64)
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v0.AuxInt = i
@@ -13668,7 +13668,7 @@ func rewriteValueAMD64_OpAMD64ORQ(v *Value) bool {
 		v0 := b.NewValue0(v.Pos, OpAMD64BSWAPQ, v.Type)
 		v.reset(OpCopy)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpAMD64MOVQload, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpAMD64MOVQload, types.UInt64)
 		v1.AuxInt = i - 7
 		v1.Aux = s
 		v1.AddArg(p)
@@ -17355,8 +17355,8 @@ func rewriteValueAMD64_OpAndB(v *Value) bool {
 func rewriteValueAMD64_OpAtomicAdd32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (AtomicAdd32 ptr val mem)
 	// cond:
 	// result: (AddTupleFirst32 (XADDLlock val ptr mem) val)
@@ -17365,7 +17365,7 @@ func rewriteValueAMD64_OpAtomicAdd32(v *Value) bool {
 		val := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpAMD64AddTupleFirst32)
-		v0 := b.NewValue0(v.Pos, OpAMD64XADDLlock, MakeTuple(fe.TypeUInt32(), TypeMem))
+		v0 := b.NewValue0(v.Pos, OpAMD64XADDLlock, MakeTuple(types.UInt32, TypeMem))
 		v0.AddArg(val)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
@@ -17377,8 +17377,8 @@ func rewriteValueAMD64_OpAtomicAdd32(v *Value) bool {
 func rewriteValueAMD64_OpAtomicAdd64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (AtomicAdd64 ptr val mem)
 	// cond:
 	// result: (AddTupleFirst64 (XADDQlock val ptr mem) val)
@@ -17387,7 +17387,7 @@ func rewriteValueAMD64_OpAtomicAdd64(v *Value) bool {
 		val := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpAMD64AddTupleFirst64)
-		v0 := b.NewValue0(v.Pos, OpAMD64XADDQlock, MakeTuple(fe.TypeUInt64(), TypeMem))
+		v0 := b.NewValue0(v.Pos, OpAMD64XADDQlock, MakeTuple(types.UInt64, TypeMem))
 		v0.AddArg(val)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
@@ -17554,17 +17554,17 @@ func rewriteValueAMD64_OpAtomicOr8(v *Value) bool {
 func rewriteValueAMD64_OpAtomicStore32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (AtomicStore32 ptr val mem)
 	// cond:
-	// result: (Select1 (XCHGL <MakeTuple(fe.TypeUInt32(),TypeMem)> val ptr mem))
+	// result: (Select1 (XCHGL <MakeTuple(types.UInt32,TypeMem)> val ptr mem))
 	for {
 		ptr := v.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpAMD64XCHGL, MakeTuple(fe.TypeUInt32(), TypeMem))
+		v0 := b.NewValue0(v.Pos, OpAMD64XCHGL, MakeTuple(types.UInt32, TypeMem))
 		v0.AddArg(val)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
@@ -17575,17 +17575,17 @@ func rewriteValueAMD64_OpAtomicStore32(v *Value) bool {
 func rewriteValueAMD64_OpAtomicStore64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (AtomicStore64 ptr val mem)
 	// cond:
-	// result: (Select1 (XCHGQ <MakeTuple(fe.TypeUInt64(),TypeMem)> val ptr mem))
+	// result: (Select1 (XCHGQ <MakeTuple(types.UInt64,TypeMem)> val ptr mem))
 	for {
 		ptr := v.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpAMD64XCHGQ, MakeTuple(fe.TypeUInt64(), TypeMem))
+		v0 := b.NewValue0(v.Pos, OpAMD64XCHGQ, MakeTuple(types.UInt64, TypeMem))
 		v0.AddArg(val)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
@@ -17598,11 +17598,11 @@ func rewriteValueAMD64_OpAtomicStorePtrNoWB(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (AtomicStorePtrNoWB ptr val mem)
 	// cond: config.PtrSize == 8
-	// result: (Select1 (XCHGQ <MakeTuple(fe.TypeBytePtr(),TypeMem)> val ptr mem))
+	// result: (Select1 (XCHGQ <MakeTuple(types.BytePtr,TypeMem)> val ptr mem))
 	for {
 		ptr := v.Args[0]
 		val := v.Args[1]
@@ -17611,7 +17611,7 @@ func rewriteValueAMD64_OpAtomicStorePtrNoWB(v *Value) bool {
 			break
 		}
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpAMD64XCHGQ, MakeTuple(fe.TypeBytePtr(), TypeMem))
+		v0 := b.NewValue0(v.Pos, OpAMD64XCHGQ, MakeTuple(types.BytePtr, TypeMem))
 		v0.AddArg(val)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
@@ -17620,7 +17620,7 @@ func rewriteValueAMD64_OpAtomicStorePtrNoWB(v *Value) bool {
 	}
 	// match: (AtomicStorePtrNoWB ptr val mem)
 	// cond: config.PtrSize == 4
-	// result: (Select1 (XCHGL <MakeTuple(fe.TypeBytePtr(),TypeMem)> val ptr mem))
+	// result: (Select1 (XCHGL <MakeTuple(types.BytePtr,TypeMem)> val ptr mem))
 	for {
 		ptr := v.Args[0]
 		val := v.Args[1]
@@ -17629,7 +17629,7 @@ func rewriteValueAMD64_OpAtomicStorePtrNoWB(v *Value) bool {
 			break
 		}
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpAMD64XCHGL, MakeTuple(fe.TypeBytePtr(), TypeMem))
+		v0 := b.NewValue0(v.Pos, OpAMD64XCHGL, MakeTuple(types.BytePtr, TypeMem))
 		v0.AddArg(val)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
@@ -17654,15 +17654,15 @@ func rewriteValueAMD64_OpAvg64u(v *Value) bool {
 func rewriteValueAMD64_OpBitLen32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (BitLen32 x)
 	// cond:
-	// result: (BitLen64 (MOVLQZX <fe.TypeUInt64()> x))
+	// result: (BitLen64 (MOVLQZX <types.UInt64> x))
 	for {
 		x := v.Args[0]
 		v.reset(OpBitLen64)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVLQZX, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVLQZX, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -17671,8 +17671,8 @@ func rewriteValueAMD64_OpBitLen32(v *Value) bool {
 func rewriteValueAMD64_OpBitLen64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (BitLen64 <t> x)
 	// cond:
 	// result: (ADDQconst [1] (CMOVQEQ <t> (Select0 <t> (BSRQ x)) (MOVQconst <t> [-1]) (Select1 <TypeFlags> (BSRQ x))))
@@ -17683,7 +17683,7 @@ func rewriteValueAMD64_OpBitLen64(v *Value) bool {
 		v.AuxInt = 1
 		v0 := b.NewValue0(v.Pos, OpAMD64CMOVQEQ, t)
 		v1 := b.NewValue0(v.Pos, OpSelect0, t)
-		v2 := b.NewValue0(v.Pos, OpAMD64BSRQ, MakeTuple(fe.TypeUInt64(), TypeFlags))
+		v2 := b.NewValue0(v.Pos, OpAMD64BSRQ, MakeTuple(types.UInt64, TypeFlags))
 		v2.AddArg(x)
 		v1.AddArg(v2)
 		v0.AddArg(v1)
@@ -17691,7 +17691,7 @@ func rewriteValueAMD64_OpBitLen64(v *Value) bool {
 		v3.AuxInt = -1
 		v0.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpSelect1, TypeFlags)
-		v5 := b.NewValue0(v.Pos, OpAMD64BSRQ, MakeTuple(fe.TypeUInt64(), TypeFlags))
+		v5 := b.NewValue0(v.Pos, OpAMD64BSRQ, MakeTuple(types.UInt64, TypeFlags))
 		v5.AddArg(x)
 		v4.AddArg(v5)
 		v0.AddArg(v4)
@@ -17930,17 +17930,17 @@ func rewriteValueAMD64_OpConvert(v *Value) bool {
 func rewriteValueAMD64_OpCtz32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Ctz32 x)
 	// cond:
-	// result: (Select0 (BSFQ (ORQ <fe.TypeUInt64()> (MOVQconst [1<<32]) x)))
+	// result: (Select0 (BSFQ (ORQ <types.UInt64> (MOVQconst [1<<32]) x)))
 	for {
 		x := v.Args[0]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpAMD64BSFQ, MakeTuple(fe.TypeUInt64(), TypeFlags))
-		v1 := b.NewValue0(v.Pos, OpAMD64ORQ, fe.TypeUInt64())
-		v2 := b.NewValue0(v.Pos, OpAMD64MOVQconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpAMD64BSFQ, MakeTuple(types.UInt64, TypeFlags))
+		v1 := b.NewValue0(v.Pos, OpAMD64ORQ, types.UInt64)
+		v2 := b.NewValue0(v.Pos, OpAMD64MOVQconst, types.UInt64)
 		v2.AuxInt = 1 << 32
 		v1.AddArg(v2)
 		v1.AddArg(x)
@@ -17952,8 +17952,8 @@ func rewriteValueAMD64_OpCtz32(v *Value) bool {
 func rewriteValueAMD64_OpCtz64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Ctz64 <t> x)
 	// cond:
 	// result: (CMOVQEQ (Select0 <t> (BSFQ x)) (MOVQconst <t> [64]) (Select1 <TypeFlags> (BSFQ x)))
@@ -17962,7 +17962,7 @@ func rewriteValueAMD64_OpCtz64(v *Value) bool {
 		x := v.Args[0]
 		v.reset(OpAMD64CMOVQEQ)
 		v0 := b.NewValue0(v.Pos, OpSelect0, t)
-		v1 := b.NewValue0(v.Pos, OpAMD64BSFQ, MakeTuple(fe.TypeUInt64(), TypeFlags))
+		v1 := b.NewValue0(v.Pos, OpAMD64BSFQ, MakeTuple(types.UInt64, TypeFlags))
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -17970,7 +17970,7 @@ func rewriteValueAMD64_OpCtz64(v *Value) bool {
 		v2.AuxInt = 64
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpSelect1, TypeFlags)
-		v4 := b.NewValue0(v.Pos, OpAMD64BSFQ, MakeTuple(fe.TypeUInt64(), TypeFlags))
+		v4 := b.NewValue0(v.Pos, OpAMD64BSFQ, MakeTuple(types.UInt64, TypeFlags))
 		v4.AddArg(x)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -18105,8 +18105,8 @@ func rewriteValueAMD64_OpDiv128u(v *Value) bool {
 func rewriteValueAMD64_OpDiv16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16  x y)
 	// cond:
 	// result: (Select0 (DIVW  x y))
@@ -18114,7 +18114,7 @@ func rewriteValueAMD64_OpDiv16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVW, MakeTuple(fe.TypeInt16(), fe.TypeInt16()))
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVW, MakeTuple(types.Int16, types.Int16))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -18124,8 +18124,8 @@ func rewriteValueAMD64_OpDiv16(v *Value) bool {
 func rewriteValueAMD64_OpDiv16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16u x y)
 	// cond:
 	// result: (Select0 (DIVWU x y))
@@ -18133,7 +18133,7 @@ func rewriteValueAMD64_OpDiv16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVWU, MakeTuple(fe.TypeUInt16(), fe.TypeUInt16()))
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVWU, MakeTuple(types.UInt16, types.UInt16))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -18143,8 +18143,8 @@ func rewriteValueAMD64_OpDiv16u(v *Value) bool {
 func rewriteValueAMD64_OpDiv32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div32  x y)
 	// cond:
 	// result: (Select0 (DIVL  x y))
@@ -18152,7 +18152,7 @@ func rewriteValueAMD64_OpDiv32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVL, MakeTuple(fe.TypeInt32(), fe.TypeInt32()))
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVL, MakeTuple(types.Int32, types.Int32))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -18175,8 +18175,8 @@ func rewriteValueAMD64_OpDiv32F(v *Value) bool {
 func rewriteValueAMD64_OpDiv32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div32u x y)
 	// cond:
 	// result: (Select0 (DIVLU x y))
@@ -18184,7 +18184,7 @@ func rewriteValueAMD64_OpDiv32u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVLU, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVLU, MakeTuple(types.UInt32, types.UInt32))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -18194,8 +18194,8 @@ func rewriteValueAMD64_OpDiv32u(v *Value) bool {
 func rewriteValueAMD64_OpDiv64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div64  x y)
 	// cond:
 	// result: (Select0 (DIVQ  x y))
@@ -18203,7 +18203,7 @@ func rewriteValueAMD64_OpDiv64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVQ, MakeTuple(fe.TypeInt64(), fe.TypeInt64()))
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVQ, MakeTuple(types.Int64, types.Int64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -18226,8 +18226,8 @@ func rewriteValueAMD64_OpDiv64F(v *Value) bool {
 func rewriteValueAMD64_OpDiv64u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div64u x y)
 	// cond:
 	// result: (Select0 (DIVQU x y))
@@ -18235,7 +18235,7 @@ func rewriteValueAMD64_OpDiv64u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVQU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVQU, MakeTuple(types.UInt64, types.UInt64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -18245,8 +18245,8 @@ func rewriteValueAMD64_OpDiv64u(v *Value) bool {
 func rewriteValueAMD64_OpDiv8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8   x y)
 	// cond:
 	// result: (Select0 (DIVW  (SignExt8to16 x) (SignExt8to16 y)))
@@ -18254,11 +18254,11 @@ func rewriteValueAMD64_OpDiv8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVW, MakeTuple(fe.TypeInt16(), fe.TypeInt16()))
-		v1 := b.NewValue0(v.Pos, OpSignExt8to16, fe.TypeInt16())
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVW, MakeTuple(types.Int16, types.Int16))
+		v1 := b.NewValue0(v.Pos, OpSignExt8to16, types.Int16)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to16, fe.TypeInt16())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to16, types.Int16)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -18268,8 +18268,8 @@ func rewriteValueAMD64_OpDiv8(v *Value) bool {
 func rewriteValueAMD64_OpDiv8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8u  x y)
 	// cond:
 	// result: (Select0 (DIVWU (ZeroExt8to16 x) (ZeroExt8to16 y)))
@@ -18277,11 +18277,11 @@ func rewriteValueAMD64_OpDiv8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVWU, MakeTuple(fe.TypeUInt16(), fe.TypeUInt16()))
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to16, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVWU, MakeTuple(types.UInt16, types.UInt16))
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to16, types.UInt16)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to16, fe.TypeUInt16())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to16, types.UInt16)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -19780,8 +19780,8 @@ func rewriteValueAMD64_OpLsh8x8(v *Value) bool {
 func rewriteValueAMD64_OpMod16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16  x y)
 	// cond:
 	// result: (Select1 (DIVW  x y))
@@ -19789,7 +19789,7 @@ func rewriteValueAMD64_OpMod16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVW, MakeTuple(fe.TypeInt16(), fe.TypeInt16()))
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVW, MakeTuple(types.Int16, types.Int16))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -19799,8 +19799,8 @@ func rewriteValueAMD64_OpMod16(v *Value) bool {
 func rewriteValueAMD64_OpMod16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16u x y)
 	// cond:
 	// result: (Select1 (DIVWU x y))
@@ -19808,7 +19808,7 @@ func rewriteValueAMD64_OpMod16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVWU, MakeTuple(fe.TypeUInt16(), fe.TypeUInt16()))
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVWU, MakeTuple(types.UInt16, types.UInt16))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -19818,8 +19818,8 @@ func rewriteValueAMD64_OpMod16u(v *Value) bool {
 func rewriteValueAMD64_OpMod32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod32  x y)
 	// cond:
 	// result: (Select1 (DIVL  x y))
@@ -19827,7 +19827,7 @@ func rewriteValueAMD64_OpMod32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVL, MakeTuple(fe.TypeInt32(), fe.TypeInt32()))
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVL, MakeTuple(types.Int32, types.Int32))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -19837,8 +19837,8 @@ func rewriteValueAMD64_OpMod32(v *Value) bool {
 func rewriteValueAMD64_OpMod32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod32u x y)
 	// cond:
 	// result: (Select1 (DIVLU x y))
@@ -19846,7 +19846,7 @@ func rewriteValueAMD64_OpMod32u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVLU, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVLU, MakeTuple(types.UInt32, types.UInt32))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -19856,8 +19856,8 @@ func rewriteValueAMD64_OpMod32u(v *Value) bool {
 func rewriteValueAMD64_OpMod64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod64  x y)
 	// cond:
 	// result: (Select1 (DIVQ  x y))
@@ -19865,7 +19865,7 @@ func rewriteValueAMD64_OpMod64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVQ, MakeTuple(fe.TypeInt64(), fe.TypeInt64()))
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVQ, MakeTuple(types.Int64, types.Int64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -19875,8 +19875,8 @@ func rewriteValueAMD64_OpMod64(v *Value) bool {
 func rewriteValueAMD64_OpMod64u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod64u x y)
 	// cond:
 	// result: (Select1 (DIVQU x y))
@@ -19884,7 +19884,7 @@ func rewriteValueAMD64_OpMod64u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVQU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVQU, MakeTuple(types.UInt64, types.UInt64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -19894,8 +19894,8 @@ func rewriteValueAMD64_OpMod64u(v *Value) bool {
 func rewriteValueAMD64_OpMod8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8   x y)
 	// cond:
 	// result: (Select1 (DIVW  (SignExt8to16 x) (SignExt8to16 y)))
@@ -19903,11 +19903,11 @@ func rewriteValueAMD64_OpMod8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVW, MakeTuple(fe.TypeInt16(), fe.TypeInt16()))
-		v1 := b.NewValue0(v.Pos, OpSignExt8to16, fe.TypeInt16())
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVW, MakeTuple(types.Int16, types.Int16))
+		v1 := b.NewValue0(v.Pos, OpSignExt8to16, types.Int16)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to16, fe.TypeInt16())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to16, types.Int16)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -19917,8 +19917,8 @@ func rewriteValueAMD64_OpMod8(v *Value) bool {
 func rewriteValueAMD64_OpMod8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8u  x y)
 	// cond:
 	// result: (Select1 (DIVWU (ZeroExt8to16 x) (ZeroExt8to16 y)))
@@ -19926,11 +19926,11 @@ func rewriteValueAMD64_OpMod8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpAMD64DIVWU, MakeTuple(fe.TypeUInt16(), fe.TypeUInt16()))
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to16, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpAMD64DIVWU, MakeTuple(types.UInt16, types.UInt16))
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to16, types.UInt16)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to16, fe.TypeUInt16())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to16, types.UInt16)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -19942,8 +19942,8 @@ func rewriteValueAMD64_OpMove(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Move [0] _ _ mem)
 	// cond:
 	// result: mem
@@ -19969,7 +19969,7 @@ func rewriteValueAMD64_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVBstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVBload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVBload, types.UInt8)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -19988,7 +19988,7 @@ func rewriteValueAMD64_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVWstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVWload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVWload, types.UInt16)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -20007,7 +20007,7 @@ func rewriteValueAMD64_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVLstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, types.UInt32)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -20026,7 +20026,7 @@ func rewriteValueAMD64_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVQstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, types.UInt64)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -20065,14 +20065,14 @@ func rewriteValueAMD64_OpMove(v *Value) bool {
 		v.reset(OpAMD64MOVBstore)
 		v.AuxInt = 2
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVBload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVBload, types.UInt8)
 		v0.AuxInt = 2
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpAMD64MOVWstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpAMD64MOVWload, fe.TypeUInt16())
+		v2 := b.NewValue0(v.Pos, OpAMD64MOVWload, types.UInt16)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -20093,14 +20093,14 @@ func rewriteValueAMD64_OpMove(v *Value) bool {
 		v.reset(OpAMD64MOVBstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVBload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVBload, types.UInt8)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpAMD64MOVLstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpAMD64MOVLload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpAMD64MOVLload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -20121,14 +20121,14 @@ func rewriteValueAMD64_OpMove(v *Value) bool {
 		v.reset(OpAMD64MOVWstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVWload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVWload, types.UInt16)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpAMD64MOVLstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpAMD64MOVLload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpAMD64MOVLload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -20149,14 +20149,14 @@ func rewriteValueAMD64_OpMove(v *Value) bool {
 		v.reset(OpAMD64MOVLstore)
 		v.AuxInt = 3
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, types.UInt32)
 		v0.AuxInt = 3
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpAMD64MOVLstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpAMD64MOVLload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpAMD64MOVLload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -20178,14 +20178,14 @@ func rewriteValueAMD64_OpMove(v *Value) bool {
 		v.reset(OpAMD64MOVQstore)
 		v.AuxInt = s - 8
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, types.UInt64)
 		v0.AuxInt = s - 8
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpAMD64MOVQstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpAMD64MOVQload, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpAMD64MOVQload, types.UInt64)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -20216,7 +20216,7 @@ func rewriteValueAMD64_OpMove(v *Value) bool {
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpAMD64MOVQstore, TypeMem)
 		v2.AddArg(dst)
-		v3 := b.NewValue0(v.Pos, OpAMD64MOVQload, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpAMD64MOVQload, types.UInt64)
 		v3.AddArg(src)
 		v3.AddArg(mem)
 		v2.AddArg(v3)
@@ -20287,7 +20287,7 @@ func rewriteValueAMD64_OpMove(v *Value) bool {
 		v.reset(OpAMD64REPMOVSQ)
 		v.AddArg(dst)
 		v.AddArg(src)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, types.UInt64)
 		v0.AuxInt = s / 8
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -20411,16 +20411,16 @@ func rewriteValueAMD64_OpNeg32(v *Value) bool {
 func rewriteValueAMD64_OpNeg32F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neg32F x)
 	// cond:
-	// result: (PXOR x (MOVSSconst <fe.TypeFloat32()> [f2i(math.Copysign(0, -1))]))
+	// result: (PXOR x (MOVSSconst <types.Float32> [f2i(math.Copysign(0, -1))]))
 	for {
 		x := v.Args[0]
 		v.reset(OpAMD64PXOR)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVSSconst, fe.TypeFloat32())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVSSconst, types.Float32)
 		v0.AuxInt = f2i(math.Copysign(0, -1))
 		v.AddArg(v0)
 		return true
@@ -20440,16 +20440,16 @@ func rewriteValueAMD64_OpNeg64(v *Value) bool {
 func rewriteValueAMD64_OpNeg64F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neg64F x)
 	// cond:
-	// result: (PXOR x (MOVSDconst <fe.TypeFloat64()> [f2i(math.Copysign(0, -1))]))
+	// result: (PXOR x (MOVSDconst <types.Float64> [f2i(math.Copysign(0, -1))]))
 	for {
 		x := v.Args[0]
 		v.reset(OpAMD64PXOR)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVSDconst, fe.TypeFloat64())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVSDconst, types.Float64)
 		v0.AuxInt = f2i(math.Copysign(0, -1))
 		v.AddArg(v0)
 		return true
@@ -20654,8 +20654,8 @@ func rewriteValueAMD64_OpOffPtr(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (OffPtr [off] ptr)
 	// cond: config.PtrSize == 8 && is32Bit(off)
 	// result: (ADDQconst [off] ptr)
@@ -20680,7 +20680,7 @@ func rewriteValueAMD64_OpOffPtr(v *Value) bool {
 			break
 		}
 		v.reset(OpAMD64ADDQ)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, types.UInt64)
 		v0.AuxInt = off
 		v.AddArg(v0)
 		v.AddArg(ptr)
@@ -22133,8 +22133,8 @@ func rewriteValueAMD64_OpZero(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Zero [0] _ mem)
 	// cond:
 	// result: mem
@@ -22394,7 +22394,7 @@ func rewriteValueAMD64_OpZero(v *Value) bool {
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpAMD64MOVQstore, TypeMem)
 		v1.AddArg(destptr)
-		v2 := b.NewValue0(v.Pos, OpAMD64MOVQconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpAMD64MOVQconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -22432,10 +22432,10 @@ func rewriteValueAMD64_OpZero(v *Value) bool {
 		}
 		v.reset(OpAMD64REPSTOSQ)
 		v.AddArg(destptr)
-		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, types.UInt64)
 		v0.AuxInt = s / 8
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpAMD64MOVQconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpAMD64MOVQconst, types.UInt64)
 		v1.AuxInt = 0
 		v.AddArg(v1)
 		v.AddArg(mem)
@@ -22514,6 +22514,8 @@ func rewriteBlockAMD64(b *Block) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &config.Types
+	_ = types
 	switch b.Kind {
 	case BlockAMD64EQ:
 		// match: (EQ (TESTL (SHLL (MOVLconst [1]) x) y))
diff --git a/src/cmd/compile/internal/ssa/rewriteARM.go b/src/cmd/compile/internal/ssa/rewriteARM.go
index 1a8e915bf7..0b554d79a4 100644
--- a/src/cmd/compile/internal/ssa/rewriteARM.go
+++ b/src/cmd/compile/internal/ssa/rewriteARM.go
@@ -13209,8 +13209,8 @@ func rewriteValueARM_OpCvt64Fto32U(v *Value) bool {
 func rewriteValueARM_OpDiv16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16 x y)
 	// cond:
 	// result: (Div32 (SignExt16to32 x) (SignExt16to32 y))
@@ -13218,10 +13218,10 @@ func rewriteValueARM_OpDiv16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpDiv32)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -13230,8 +13230,8 @@ func rewriteValueARM_OpDiv16(v *Value) bool {
 func rewriteValueARM_OpDiv16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16u x y)
 	// cond:
 	// result: (Div32u (ZeroExt16to32 x) (ZeroExt16to32 y))
@@ -13239,10 +13239,10 @@ func rewriteValueARM_OpDiv16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpDiv32u)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -13253,52 +13253,52 @@ func rewriteValueARM_OpDiv32(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div32 x y)
 	// cond:
-	// result: (SUB (XOR <fe.TypeUInt32()> 		(Select0 <fe.TypeUInt32()> (CALLudiv {config.ctxt.Lookup("udiv", 0)} 			(SUB <fe.TypeUInt32()> (XOR x <fe.TypeUInt32()> (Signmask x)) (Signmask x)) 			(SUB <fe.TypeUInt32()> (XOR y <fe.TypeUInt32()> (Signmask y)) (Signmask y)))) 		(Signmask (XOR <fe.TypeUInt32()> x y))) (Signmask (XOR <fe.TypeUInt32()> x y)))
+	// result: (SUB (XOR <types.UInt32> 		(Select0 <types.UInt32> (CALLudiv {config.ctxt.Lookup("udiv", 0)} 			(SUB <types.UInt32> (XOR x <types.UInt32> (Signmask x)) (Signmask x)) 			(SUB <types.UInt32> (XOR y <types.UInt32> (Signmask y)) (Signmask y)))) 		(Signmask (XOR <types.UInt32> x y))) (Signmask (XOR <types.UInt32> x y)))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARMSUB)
-		v0 := b.NewValue0(v.Pos, OpARMXOR, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpSelect0, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpARMCALLudiv, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
+		v0 := b.NewValue0(v.Pos, OpARMXOR, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpSelect0, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpARMCALLudiv, MakeTuple(types.UInt32, types.UInt32))
 		v2.Aux = config.ctxt.Lookup("udiv", 0)
-		v3 := b.NewValue0(v.Pos, OpARMSUB, fe.TypeUInt32())
-		v4 := b.NewValue0(v.Pos, OpARMXOR, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpARMSUB, types.UInt32)
+		v4 := b.NewValue0(v.Pos, OpARMXOR, types.UInt32)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
+		v5 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
 		v3.AddArg(v4)
-		v6 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
+		v6 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
 		v6.AddArg(x)
 		v3.AddArg(v6)
 		v2.AddArg(v3)
-		v7 := b.NewValue0(v.Pos, OpARMSUB, fe.TypeUInt32())
-		v8 := b.NewValue0(v.Pos, OpARMXOR, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpARMSUB, types.UInt32)
+		v8 := b.NewValue0(v.Pos, OpARMXOR, types.UInt32)
 		v8.AddArg(y)
-		v9 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
+		v9 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
 		v9.AddArg(y)
 		v8.AddArg(v9)
 		v7.AddArg(v8)
-		v10 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
+		v10 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
 		v10.AddArg(y)
 		v7.AddArg(v10)
 		v2.AddArg(v7)
 		v1.AddArg(v2)
 		v0.AddArg(v1)
-		v11 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
-		v12 := b.NewValue0(v.Pos, OpARMXOR, fe.TypeUInt32())
+		v11 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
+		v12 := b.NewValue0(v.Pos, OpARMXOR, types.UInt32)
 		v12.AddArg(x)
 		v12.AddArg(y)
 		v11.AddArg(v12)
 		v0.AddArg(v11)
 		v.AddArg(v0)
-		v13 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
-		v14 := b.NewValue0(v.Pos, OpARMXOR, fe.TypeUInt32())
+		v13 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
+		v14 := b.NewValue0(v.Pos, OpARMXOR, types.UInt32)
 		v14.AddArg(x)
 		v14.AddArg(y)
 		v13.AddArg(v14)
@@ -13324,17 +13324,17 @@ func rewriteValueARM_OpDiv32u(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div32u x y)
 	// cond:
-	// result: (Select0 <fe.TypeUInt32()> (CALLudiv {config.ctxt.Lookup("udiv", 0)} x y))
+	// result: (Select0 <types.UInt32> (CALLudiv {config.ctxt.Lookup("udiv", 0)} x y))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v.Type = fe.TypeUInt32()
-		v0 := b.NewValue0(v.Pos, OpARMCALLudiv, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
+		v.Type = types.UInt32
+		v0 := b.NewValue0(v.Pos, OpARMCALLudiv, MakeTuple(types.UInt32, types.UInt32))
 		v0.Aux = config.ctxt.Lookup("udiv", 0)
 		v0.AddArg(x)
 		v0.AddArg(y)
@@ -13358,8 +13358,8 @@ func rewriteValueARM_OpDiv64F(v *Value) bool {
 func rewriteValueARM_OpDiv8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8 x y)
 	// cond:
 	// result: (Div32 (SignExt8to32 x) (SignExt8to32 y))
@@ -13367,10 +13367,10 @@ func rewriteValueARM_OpDiv8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpDiv32)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -13379,8 +13379,8 @@ func rewriteValueARM_OpDiv8(v *Value) bool {
 func rewriteValueARM_OpDiv8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8u x y)
 	// cond:
 	// result: (Div32u (ZeroExt8to32 x) (ZeroExt8to32 y))
@@ -13388,10 +13388,10 @@ func rewriteValueARM_OpDiv8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpDiv32u)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -13400,8 +13400,8 @@ func rewriteValueARM_OpDiv8u(v *Value) bool {
 func rewriteValueARM_OpEq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq16 x y)
 	// cond:
 	// result: (Equal (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -13410,10 +13410,10 @@ func rewriteValueARM_OpEq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMEqual)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13474,8 +13474,8 @@ func rewriteValueARM_OpEq64F(v *Value) bool {
 func rewriteValueARM_OpEq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq8 x y)
 	// cond:
 	// result: (Equal (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -13484,10 +13484,10 @@ func rewriteValueARM_OpEq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMEqual)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13497,17 +13497,17 @@ func rewriteValueARM_OpEq8(v *Value) bool {
 func rewriteValueARM_OpEqB(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (EqB x y)
 	// cond:
-	// result: (XORconst [1] (XOR <fe.TypeBool()> x y))
+	// result: (XORconst [1] (XOR <types.Bool> x y))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARMXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpARMXOR, fe.TypeBool())
+		v0 := b.NewValue0(v.Pos, OpARMXOR, types.Bool)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -13534,8 +13534,8 @@ func rewriteValueARM_OpEqPtr(v *Value) bool {
 func rewriteValueARM_OpGeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq16 x y)
 	// cond:
 	// result: (GreaterEqual (CMP (SignExt16to32 x) (SignExt16to32 y)))
@@ -13544,10 +13544,10 @@ func rewriteValueARM_OpGeq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMGreaterEqual)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13557,8 +13557,8 @@ func rewriteValueARM_OpGeq16(v *Value) bool {
 func rewriteValueARM_OpGeq16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq16U x y)
 	// cond:
 	// result: (GreaterEqualU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -13567,10 +13567,10 @@ func rewriteValueARM_OpGeq16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMGreaterEqualU)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13648,8 +13648,8 @@ func rewriteValueARM_OpGeq64F(v *Value) bool {
 func rewriteValueARM_OpGeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq8 x y)
 	// cond:
 	// result: (GreaterEqual (CMP (SignExt8to32 x) (SignExt8to32 y)))
@@ -13658,10 +13658,10 @@ func rewriteValueARM_OpGeq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMGreaterEqual)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13671,8 +13671,8 @@ func rewriteValueARM_OpGeq8(v *Value) bool {
 func rewriteValueARM_OpGeq8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq8U x y)
 	// cond:
 	// result: (GreaterEqualU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -13681,10 +13681,10 @@ func rewriteValueARM_OpGeq8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMGreaterEqualU)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13703,8 +13703,8 @@ func rewriteValueARM_OpGetClosurePtr(v *Value) bool {
 func rewriteValueARM_OpGreater16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater16 x y)
 	// cond:
 	// result: (GreaterThan (CMP (SignExt16to32 x) (SignExt16to32 y)))
@@ -13713,10 +13713,10 @@ func rewriteValueARM_OpGreater16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMGreaterThan)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13726,8 +13726,8 @@ func rewriteValueARM_OpGreater16(v *Value) bool {
 func rewriteValueARM_OpGreater16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater16U x y)
 	// cond:
 	// result: (GreaterThanU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -13736,10 +13736,10 @@ func rewriteValueARM_OpGreater16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMGreaterThanU)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13817,8 +13817,8 @@ func rewriteValueARM_OpGreater64F(v *Value) bool {
 func rewriteValueARM_OpGreater8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater8 x y)
 	// cond:
 	// result: (GreaterThan (CMP (SignExt8to32 x) (SignExt8to32 y)))
@@ -13827,10 +13827,10 @@ func rewriteValueARM_OpGreater8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMGreaterThan)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13840,8 +13840,8 @@ func rewriteValueARM_OpGreater8(v *Value) bool {
 func rewriteValueARM_OpGreater8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater8U x y)
 	// cond:
 	// result: (GreaterThanU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -13850,10 +13850,10 @@ func rewriteValueARM_OpGreater8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMGreaterThanU)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13954,8 +13954,8 @@ func rewriteValueARM_OpIsSliceInBounds(v *Value) bool {
 func rewriteValueARM_OpLeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq16 x y)
 	// cond:
 	// result: (LessEqual (CMP (SignExt16to32 x) (SignExt16to32 y)))
@@ -13964,10 +13964,10 @@ func rewriteValueARM_OpLeq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMLessEqual)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13977,8 +13977,8 @@ func rewriteValueARM_OpLeq16(v *Value) bool {
 func rewriteValueARM_OpLeq16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq16U x y)
 	// cond:
 	// result: (LessEqualU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -13987,10 +13987,10 @@ func rewriteValueARM_OpLeq16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMLessEqualU)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -14068,8 +14068,8 @@ func rewriteValueARM_OpLeq64F(v *Value) bool {
 func rewriteValueARM_OpLeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq8 x y)
 	// cond:
 	// result: (LessEqual (CMP (SignExt8to32 x) (SignExt8to32 y)))
@@ -14078,10 +14078,10 @@ func rewriteValueARM_OpLeq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMLessEqual)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -14091,8 +14091,8 @@ func rewriteValueARM_OpLeq8(v *Value) bool {
 func rewriteValueARM_OpLeq8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq8U x y)
 	// cond:
 	// result: (LessEqualU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -14101,10 +14101,10 @@ func rewriteValueARM_OpLeq8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMLessEqualU)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -14114,8 +14114,8 @@ func rewriteValueARM_OpLeq8U(v *Value) bool {
 func rewriteValueARM_OpLess16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less16 x y)
 	// cond:
 	// result: (LessThan (CMP (SignExt16to32 x) (SignExt16to32 y)))
@@ -14124,10 +14124,10 @@ func rewriteValueARM_OpLess16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMLessThan)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -14137,8 +14137,8 @@ func rewriteValueARM_OpLess16(v *Value) bool {
 func rewriteValueARM_OpLess16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less16U x y)
 	// cond:
 	// result: (LessThanU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -14147,10 +14147,10 @@ func rewriteValueARM_OpLess16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMLessThanU)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -14228,8 +14228,8 @@ func rewriteValueARM_OpLess64F(v *Value) bool {
 func rewriteValueARM_OpLess8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less8 x y)
 	// cond:
 	// result: (LessThan (CMP (SignExt8to32 x) (SignExt8to32 y)))
@@ -14238,10 +14238,10 @@ func rewriteValueARM_OpLess8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMLessThan)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -14251,8 +14251,8 @@ func rewriteValueARM_OpLess8(v *Value) bool {
 func rewriteValueARM_OpLess8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less8U x y)
 	// cond:
 	// result: (LessThanU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -14261,10 +14261,10 @@ func rewriteValueARM_OpLess8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMLessThanU)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -14397,8 +14397,8 @@ func rewriteValueARM_OpLoad(v *Value) bool {
 func rewriteValueARM_OpLsh16x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x16 x y)
 	// cond:
 	// result: (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
@@ -14409,13 +14409,13 @@ func rewriteValueARM_OpLsh16x16(v *Value) bool {
 		v.AuxInt = 0
 		v0 := b.NewValue0(v.Pos, OpARMSLL, x.Type)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v2 := b.NewValue0(v.Pos, OpARMCMPconst, TypeFlags)
 		v2.AuxInt = 256
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v.AddArg(v2)
@@ -14484,8 +14484,8 @@ func rewriteValueARM_OpLsh16x64(v *Value) bool {
 func rewriteValueARM_OpLsh16x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x8  x y)
 	// cond:
 	// result: (SLL x (ZeroExt8to32 y))
@@ -14494,7 +14494,7 @@ func rewriteValueARM_OpLsh16x8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMSLL)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(y)
 		v.AddArg(v0)
 		return true
@@ -14503,8 +14503,8 @@ func rewriteValueARM_OpLsh16x8(v *Value) bool {
 func rewriteValueARM_OpLsh32x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x16 x y)
 	// cond:
 	// result: (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
@@ -14515,13 +14515,13 @@ func rewriteValueARM_OpLsh32x16(v *Value) bool {
 		v.AuxInt = 0
 		v0 := b.NewValue0(v.Pos, OpARMSLL, x.Type)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v2 := b.NewValue0(v.Pos, OpARMCMPconst, TypeFlags)
 		v2.AuxInt = 256
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v.AddArg(v2)
@@ -14590,8 +14590,8 @@ func rewriteValueARM_OpLsh32x64(v *Value) bool {
 func rewriteValueARM_OpLsh32x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x8  x y)
 	// cond:
 	// result: (SLL x (ZeroExt8to32 y))
@@ -14600,7 +14600,7 @@ func rewriteValueARM_OpLsh32x8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMSLL)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(y)
 		v.AddArg(v0)
 		return true
@@ -14609,8 +14609,8 @@ func rewriteValueARM_OpLsh32x8(v *Value) bool {
 func rewriteValueARM_OpLsh8x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x16 x y)
 	// cond:
 	// result: (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
@@ -14621,13 +14621,13 @@ func rewriteValueARM_OpLsh8x16(v *Value) bool {
 		v.AuxInt = 0
 		v0 := b.NewValue0(v.Pos, OpARMSLL, x.Type)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v2 := b.NewValue0(v.Pos, OpARMCMPconst, TypeFlags)
 		v2.AuxInt = 256
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v.AddArg(v2)
@@ -14696,8 +14696,8 @@ func rewriteValueARM_OpLsh8x64(v *Value) bool {
 func rewriteValueARM_OpLsh8x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x8  x y)
 	// cond:
 	// result: (SLL x (ZeroExt8to32 y))
@@ -14706,7 +14706,7 @@ func rewriteValueARM_OpLsh8x8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMSLL)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(y)
 		v.AddArg(v0)
 		return true
@@ -14715,8 +14715,8 @@ func rewriteValueARM_OpLsh8x8(v *Value) bool {
 func rewriteValueARM_OpMod16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16 x y)
 	// cond:
 	// result: (Mod32 (SignExt16to32 x) (SignExt16to32 y))
@@ -14724,10 +14724,10 @@ func rewriteValueARM_OpMod16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMod32)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -14736,8 +14736,8 @@ func rewriteValueARM_OpMod16(v *Value) bool {
 func rewriteValueARM_OpMod16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16u x y)
 	// cond:
 	// result: (Mod32u (ZeroExt16to32 x) (ZeroExt16to32 y))
@@ -14745,10 +14745,10 @@ func rewriteValueARM_OpMod16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMod32u)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -14759,48 +14759,48 @@ func rewriteValueARM_OpMod32(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod32 x y)
 	// cond:
-	// result: (SUB (XOR <fe.TypeUInt32()> 		(Select1 <fe.TypeUInt32()> (CALLudiv {config.ctxt.Lookup("udiv", 0)} 			(SUB <fe.TypeUInt32()> (XOR <fe.TypeUInt32()> x (Signmask x)) (Signmask x)) 			(SUB <fe.TypeUInt32()> (XOR <fe.TypeUInt32()> y (Signmask y)) (Signmask y)))) 		(Signmask x)) (Signmask x))
+	// result: (SUB (XOR <types.UInt32> 		(Select1 <types.UInt32> (CALLudiv {config.ctxt.Lookup("udiv", 0)} 			(SUB <types.UInt32> (XOR <types.UInt32> x (Signmask x)) (Signmask x)) 			(SUB <types.UInt32> (XOR <types.UInt32> y (Signmask y)) (Signmask y)))) 		(Signmask x)) (Signmask x))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARMSUB)
-		v0 := b.NewValue0(v.Pos, OpARMXOR, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpSelect1, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpARMCALLudiv, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
+		v0 := b.NewValue0(v.Pos, OpARMXOR, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpSelect1, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpARMCALLudiv, MakeTuple(types.UInt32, types.UInt32))
 		v2.Aux = config.ctxt.Lookup("udiv", 0)
-		v3 := b.NewValue0(v.Pos, OpARMSUB, fe.TypeUInt32())
-		v4 := b.NewValue0(v.Pos, OpARMXOR, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpARMSUB, types.UInt32)
+		v4 := b.NewValue0(v.Pos, OpARMXOR, types.UInt32)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
+		v5 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
 		v3.AddArg(v4)
-		v6 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
+		v6 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
 		v6.AddArg(x)
 		v3.AddArg(v6)
 		v2.AddArg(v3)
-		v7 := b.NewValue0(v.Pos, OpARMSUB, fe.TypeUInt32())
-		v8 := b.NewValue0(v.Pos, OpARMXOR, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpARMSUB, types.UInt32)
+		v8 := b.NewValue0(v.Pos, OpARMXOR, types.UInt32)
 		v8.AddArg(y)
-		v9 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
+		v9 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
 		v9.AddArg(y)
 		v8.AddArg(v9)
 		v7.AddArg(v8)
-		v10 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
+		v10 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
 		v10.AddArg(y)
 		v7.AddArg(v10)
 		v2.AddArg(v7)
 		v1.AddArg(v2)
 		v0.AddArg(v1)
-		v11 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
+		v11 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
 		v11.AddArg(x)
 		v0.AddArg(v11)
 		v.AddArg(v0)
-		v12 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
+		v12 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
 		v12.AddArg(x)
 		v.AddArg(v12)
 		return true
@@ -14811,17 +14811,17 @@ func rewriteValueARM_OpMod32u(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod32u x y)
 	// cond:
-	// result: (Select1 <fe.TypeUInt32()> (CALLudiv {config.ctxt.Lookup("udiv", 0)} x y))
+	// result: (Select1 <types.UInt32> (CALLudiv {config.ctxt.Lookup("udiv", 0)} x y))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v.Type = fe.TypeUInt32()
-		v0 := b.NewValue0(v.Pos, OpARMCALLudiv, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
+		v.Type = types.UInt32
+		v0 := b.NewValue0(v.Pos, OpARMCALLudiv, MakeTuple(types.UInt32, types.UInt32))
 		v0.Aux = config.ctxt.Lookup("udiv", 0)
 		v0.AddArg(x)
 		v0.AddArg(y)
@@ -14832,8 +14832,8 @@ func rewriteValueARM_OpMod32u(v *Value) bool {
 func rewriteValueARM_OpMod8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8 x y)
 	// cond:
 	// result: (Mod32 (SignExt8to32 x) (SignExt8to32 y))
@@ -14841,10 +14841,10 @@ func rewriteValueARM_OpMod8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMod32)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -14853,8 +14853,8 @@ func rewriteValueARM_OpMod8(v *Value) bool {
 func rewriteValueARM_OpMod8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8u x y)
 	// cond:
 	// result: (Mod32u (ZeroExt8to32 x) (ZeroExt8to32 y))
@@ -14862,10 +14862,10 @@ func rewriteValueARM_OpMod8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMod32u)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -14876,8 +14876,8 @@ func rewriteValueARM_OpMove(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Move [0] _ _ mem)
 	// cond:
 	// result: mem
@@ -14903,7 +14903,7 @@ func rewriteValueARM_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpARMMOVBstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARMMOVBUload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpARMMOVBUload, types.UInt8)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -14926,7 +14926,7 @@ func rewriteValueARM_OpMove(v *Value) bool {
 		}
 		v.reset(OpARMMOVHstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARMMOVHUload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpARMMOVHUload, types.UInt16)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -14946,14 +14946,14 @@ func rewriteValueARM_OpMove(v *Value) bool {
 		v.reset(OpARMMOVBstore)
 		v.AuxInt = 1
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARMMOVBUload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpARMMOVBUload, types.UInt8)
 		v0.AuxInt = 1
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARMMOVBstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpARMMOVBUload, fe.TypeUInt8())
+		v2 := b.NewValue0(v.Pos, OpARMMOVBUload, types.UInt8)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -14977,7 +14977,7 @@ func rewriteValueARM_OpMove(v *Value) bool {
 		}
 		v.reset(OpARMMOVWstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARMMOVWload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMMOVWload, types.UInt32)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -15001,14 +15001,14 @@ func rewriteValueARM_OpMove(v *Value) bool {
 		v.reset(OpARMMOVHstore)
 		v.AuxInt = 2
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARMMOVHUload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpARMMOVHUload, types.UInt16)
 		v0.AuxInt = 2
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARMMOVHstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpARMMOVHUload, fe.TypeUInt16())
+		v2 := b.NewValue0(v.Pos, OpARMMOVHUload, types.UInt16)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -15029,7 +15029,7 @@ func rewriteValueARM_OpMove(v *Value) bool {
 		v.reset(OpARMMOVBstore)
 		v.AuxInt = 3
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARMMOVBUload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpARMMOVBUload, types.UInt8)
 		v0.AuxInt = 3
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -15037,7 +15037,7 @@ func rewriteValueARM_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpARMMOVBstore, TypeMem)
 		v1.AuxInt = 2
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpARMMOVBUload, fe.TypeUInt8())
+		v2 := b.NewValue0(v.Pos, OpARMMOVBUload, types.UInt8)
 		v2.AuxInt = 2
 		v2.AddArg(src)
 		v2.AddArg(mem)
@@ -15045,14 +15045,14 @@ func rewriteValueARM_OpMove(v *Value) bool {
 		v3 := b.NewValue0(v.Pos, OpARMMOVBstore, TypeMem)
 		v3.AuxInt = 1
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpARMMOVBUload, fe.TypeUInt8())
+		v4 := b.NewValue0(v.Pos, OpARMMOVBUload, types.UInt8)
 		v4.AuxInt = 1
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpARMMOVBstore, TypeMem)
 		v5.AddArg(dst)
-		v6 := b.NewValue0(v.Pos, OpARMMOVBUload, fe.TypeUInt8())
+		v6 := b.NewValue0(v.Pos, OpARMMOVBUload, types.UInt8)
 		v6.AddArg(src)
 		v6.AddArg(mem)
 		v5.AddArg(v6)
@@ -15075,7 +15075,7 @@ func rewriteValueARM_OpMove(v *Value) bool {
 		v.reset(OpARMMOVBstore)
 		v.AuxInt = 2
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARMMOVBUload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpARMMOVBUload, types.UInt8)
 		v0.AuxInt = 2
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -15083,14 +15083,14 @@ func rewriteValueARM_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpARMMOVBstore, TypeMem)
 		v1.AuxInt = 1
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpARMMOVBUload, fe.TypeUInt8())
+		v2 := b.NewValue0(v.Pos, OpARMMOVBUload, types.UInt8)
 		v2.AuxInt = 1
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARMMOVBstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpARMMOVBUload, fe.TypeUInt8())
+		v4 := b.NewValue0(v.Pos, OpARMMOVBUload, types.UInt8)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -15282,8 +15282,8 @@ func rewriteValueARM_OpNeg8(v *Value) bool {
 func rewriteValueARM_OpNeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq16 x y)
 	// cond:
 	// result: (NotEqual (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -15292,10 +15292,10 @@ func rewriteValueARM_OpNeq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMNotEqual)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -15356,8 +15356,8 @@ func rewriteValueARM_OpNeq64F(v *Value) bool {
 func rewriteValueARM_OpNeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq8 x y)
 	// cond:
 	// result: (NotEqual (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -15366,10 +15366,10 @@ func rewriteValueARM_OpNeq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMNotEqual)
 		v0 := b.NewValue0(v.Pos, OpARMCMP, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -15537,8 +15537,8 @@ func rewriteValueARM_OpRound64F(v *Value) bool {
 func rewriteValueARM_OpRsh16Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux16 x y)
 	// cond:
 	// result: (CMOVWHSconst (SRL <x.Type> (ZeroExt16to32 x) (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
@@ -15548,16 +15548,16 @@ func rewriteValueARM_OpRsh16Ux16(v *Value) bool {
 		v.reset(OpARMCMOVWHSconst)
 		v.AuxInt = 0
 		v0 := b.NewValue0(v.Pos, OpARMSRL, x.Type)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
 		v3 := b.NewValue0(v.Pos, OpARMCMPconst, TypeFlags)
 		v3.AuxInt = 256
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -15567,8 +15567,8 @@ func rewriteValueARM_OpRsh16Ux16(v *Value) bool {
 func rewriteValueARM_OpRsh16Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux32 x y)
 	// cond:
 	// result: (CMOVWHSconst (SRL <x.Type> (ZeroExt16to32 x) y) (CMPconst [256] y) [0])
@@ -15578,7 +15578,7 @@ func rewriteValueARM_OpRsh16Ux32(v *Value) bool {
 		v.reset(OpARMCMOVWHSconst)
 		v.AuxInt = 0
 		v0 := b.NewValue0(v.Pos, OpARMSRL, x.Type)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -15593,11 +15593,11 @@ func rewriteValueARM_OpRsh16Ux32(v *Value) bool {
 func rewriteValueARM_OpRsh16Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux64 x (Const64 [c]))
 	// cond: uint64(c) < 16
-	// result: (SRLconst (SLLconst <fe.TypeUInt32()> x [16]) [c+16])
+	// result: (SRLconst (SLLconst <types.UInt32> x [16]) [c+16])
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -15610,7 +15610,7 @@ func rewriteValueARM_OpRsh16Ux64(v *Value) bool {
 		}
 		v.reset(OpARMSRLconst)
 		v.AuxInt = c + 16
-		v0 := b.NewValue0(v.Pos, OpARMSLLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMSLLconst, types.UInt32)
 		v0.AuxInt = 16
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -15637,8 +15637,8 @@ func rewriteValueARM_OpRsh16Ux64(v *Value) bool {
 func rewriteValueARM_OpRsh16Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux8  x y)
 	// cond:
 	// result: (SRL (ZeroExt16to32 x) (ZeroExt8to32 y))
@@ -15646,10 +15646,10 @@ func rewriteValueARM_OpRsh16Ux8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARMSRL)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -15658,8 +15658,8 @@ func rewriteValueARM_OpRsh16Ux8(v *Value) bool {
 func rewriteValueARM_OpRsh16x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x16 x y)
 	// cond:
 	// result: (SRAcond (SignExt16to32 x) (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
@@ -15667,15 +15667,15 @@ func rewriteValueARM_OpRsh16x16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARMSRAcond)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpARMCMPconst, TypeFlags)
 		v2.AuxInt = 256
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v.AddArg(v2)
@@ -15685,8 +15685,8 @@ func rewriteValueARM_OpRsh16x16(v *Value) bool {
 func rewriteValueARM_OpRsh16x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x32 x y)
 	// cond:
 	// result: (SRAcond (SignExt16to32 x) y (CMPconst [256] y))
@@ -15694,7 +15694,7 @@ func rewriteValueARM_OpRsh16x32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARMSRAcond)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v.AddArg(y)
@@ -15708,11 +15708,11 @@ func rewriteValueARM_OpRsh16x32(v *Value) bool {
 func rewriteValueARM_OpRsh16x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x64 x (Const64 [c]))
 	// cond: uint64(c) < 16
-	// result: (SRAconst (SLLconst <fe.TypeUInt32()> x [16]) [c+16])
+	// result: (SRAconst (SLLconst <types.UInt32> x [16]) [c+16])
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -15725,7 +15725,7 @@ func rewriteValueARM_OpRsh16x64(v *Value) bool {
 		}
 		v.reset(OpARMSRAconst)
 		v.AuxInt = c + 16
-		v0 := b.NewValue0(v.Pos, OpARMSLLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMSLLconst, types.UInt32)
 		v0.AuxInt = 16
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -15733,7 +15733,7 @@ func rewriteValueARM_OpRsh16x64(v *Value) bool {
 	}
 	// match: (Rsh16x64 x (Const64 [c]))
 	// cond: uint64(c) >= 16
-	// result: (SRAconst (SLLconst <fe.TypeUInt32()> x [16]) [31])
+	// result: (SRAconst (SLLconst <types.UInt32> x [16]) [31])
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -15746,7 +15746,7 @@ func rewriteValueARM_OpRsh16x64(v *Value) bool {
 		}
 		v.reset(OpARMSRAconst)
 		v.AuxInt = 31
-		v0 := b.NewValue0(v.Pos, OpARMSLLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMSLLconst, types.UInt32)
 		v0.AuxInt = 16
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -15757,8 +15757,8 @@ func rewriteValueARM_OpRsh16x64(v *Value) bool {
 func rewriteValueARM_OpRsh16x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x8  x y)
 	// cond:
 	// result: (SRA (SignExt16to32 x) (ZeroExt8to32 y))
@@ -15766,10 +15766,10 @@ func rewriteValueARM_OpRsh16x8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARMSRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -15778,8 +15778,8 @@ func rewriteValueARM_OpRsh16x8(v *Value) bool {
 func rewriteValueARM_OpRsh32Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux16 x y)
 	// cond:
 	// result: (CMOVWHSconst (SRL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
@@ -15790,13 +15790,13 @@ func rewriteValueARM_OpRsh32Ux16(v *Value) bool {
 		v.AuxInt = 0
 		v0 := b.NewValue0(v.Pos, OpARMSRL, x.Type)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v2 := b.NewValue0(v.Pos, OpARMCMPconst, TypeFlags)
 		v2.AuxInt = 256
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v.AddArg(v2)
@@ -15865,8 +15865,8 @@ func rewriteValueARM_OpRsh32Ux64(v *Value) bool {
 func rewriteValueARM_OpRsh32Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux8  x y)
 	// cond:
 	// result: (SRL x (ZeroExt8to32 y))
@@ -15875,7 +15875,7 @@ func rewriteValueARM_OpRsh32Ux8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMSRL)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(y)
 		v.AddArg(v0)
 		return true
@@ -15884,8 +15884,8 @@ func rewriteValueARM_OpRsh32Ux8(v *Value) bool {
 func rewriteValueARM_OpRsh32x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x16 x y)
 	// cond:
 	// result: (SRAcond x (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
@@ -15894,12 +15894,12 @@ func rewriteValueARM_OpRsh32x16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMSRAcond)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(y)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARMCMPconst, TypeFlags)
 		v1.AuxInt = 256
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v1.AddArg(v2)
 		v.AddArg(v1)
@@ -15967,8 +15967,8 @@ func rewriteValueARM_OpRsh32x64(v *Value) bool {
 func rewriteValueARM_OpRsh32x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x8  x y)
 	// cond:
 	// result: (SRA x (ZeroExt8to32 y))
@@ -15977,7 +15977,7 @@ func rewriteValueARM_OpRsh32x8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARMSRA)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(y)
 		v.AddArg(v0)
 		return true
@@ -15986,8 +15986,8 @@ func rewriteValueARM_OpRsh32x8(v *Value) bool {
 func rewriteValueARM_OpRsh8Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux16 x y)
 	// cond:
 	// result: (CMOVWHSconst (SRL <x.Type> (ZeroExt8to32 x) (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
@@ -15997,16 +15997,16 @@ func rewriteValueARM_OpRsh8Ux16(v *Value) bool {
 		v.reset(OpARMCMOVWHSconst)
 		v.AuxInt = 0
 		v0 := b.NewValue0(v.Pos, OpARMSRL, x.Type)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
 		v3 := b.NewValue0(v.Pos, OpARMCMPconst, TypeFlags)
 		v3.AuxInt = 256
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -16016,8 +16016,8 @@ func rewriteValueARM_OpRsh8Ux16(v *Value) bool {
 func rewriteValueARM_OpRsh8Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux32 x y)
 	// cond:
 	// result: (CMOVWHSconst (SRL <x.Type> (ZeroExt8to32 x) y) (CMPconst [256] y) [0])
@@ -16027,7 +16027,7 @@ func rewriteValueARM_OpRsh8Ux32(v *Value) bool {
 		v.reset(OpARMCMOVWHSconst)
 		v.AuxInt = 0
 		v0 := b.NewValue0(v.Pos, OpARMSRL, x.Type)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -16042,11 +16042,11 @@ func rewriteValueARM_OpRsh8Ux32(v *Value) bool {
 func rewriteValueARM_OpRsh8Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux64 x (Const64 [c]))
 	// cond: uint64(c) < 8
-	// result: (SRLconst (SLLconst <fe.TypeUInt32()> x [24]) [c+24])
+	// result: (SRLconst (SLLconst <types.UInt32> x [24]) [c+24])
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -16059,7 +16059,7 @@ func rewriteValueARM_OpRsh8Ux64(v *Value) bool {
 		}
 		v.reset(OpARMSRLconst)
 		v.AuxInt = c + 24
-		v0 := b.NewValue0(v.Pos, OpARMSLLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMSLLconst, types.UInt32)
 		v0.AuxInt = 24
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -16086,8 +16086,8 @@ func rewriteValueARM_OpRsh8Ux64(v *Value) bool {
 func rewriteValueARM_OpRsh8Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux8  x y)
 	// cond:
 	// result: (SRL (ZeroExt8to32 x) (ZeroExt8to32 y))
@@ -16095,10 +16095,10 @@ func rewriteValueARM_OpRsh8Ux8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARMSRL)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -16107,8 +16107,8 @@ func rewriteValueARM_OpRsh8Ux8(v *Value) bool {
 func rewriteValueARM_OpRsh8x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x16 x y)
 	// cond:
 	// result: (SRAcond (SignExt8to32 x) (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
@@ -16116,15 +16116,15 @@ func rewriteValueARM_OpRsh8x16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARMSRAcond)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpARMCMPconst, TypeFlags)
 		v2.AuxInt = 256
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v.AddArg(v2)
@@ -16134,8 +16134,8 @@ func rewriteValueARM_OpRsh8x16(v *Value) bool {
 func rewriteValueARM_OpRsh8x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x32 x y)
 	// cond:
 	// result: (SRAcond (SignExt8to32 x) y (CMPconst [256] y))
@@ -16143,7 +16143,7 @@ func rewriteValueARM_OpRsh8x32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARMSRAcond)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v.AddArg(y)
@@ -16157,11 +16157,11 @@ func rewriteValueARM_OpRsh8x32(v *Value) bool {
 func rewriteValueARM_OpRsh8x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x64 x (Const64 [c]))
 	// cond: uint64(c) < 8
-	// result: (SRAconst (SLLconst <fe.TypeUInt32()> x [24]) [c+24])
+	// result: (SRAconst (SLLconst <types.UInt32> x [24]) [c+24])
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -16174,7 +16174,7 @@ func rewriteValueARM_OpRsh8x64(v *Value) bool {
 		}
 		v.reset(OpARMSRAconst)
 		v.AuxInt = c + 24
-		v0 := b.NewValue0(v.Pos, OpARMSLLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMSLLconst, types.UInt32)
 		v0.AuxInt = 24
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -16182,7 +16182,7 @@ func rewriteValueARM_OpRsh8x64(v *Value) bool {
 	}
 	// match: (Rsh8x64 x (Const64 [c]))
 	// cond: uint64(c) >= 8
-	// result: (SRAconst (SLLconst <fe.TypeUInt32()> x [24]) [31])
+	// result: (SRAconst (SLLconst <types.UInt32> x [24]) [31])
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -16195,7 +16195,7 @@ func rewriteValueARM_OpRsh8x64(v *Value) bool {
 		}
 		v.reset(OpARMSRAconst)
 		v.AuxInt = 31
-		v0 := b.NewValue0(v.Pos, OpARMSLLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMSLLconst, types.UInt32)
 		v0.AuxInt = 24
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -16206,8 +16206,8 @@ func rewriteValueARM_OpRsh8x64(v *Value) bool {
 func rewriteValueARM_OpRsh8x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x8  x y)
 	// cond:
 	// result: (SRA (SignExt8to32 x) (ZeroExt8to32 y))
@@ -16215,10 +16215,10 @@ func rewriteValueARM_OpRsh8x8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARMSRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -16721,8 +16721,8 @@ func rewriteValueARM_OpZero(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Zero [0] _ mem)
 	// cond:
 	// result: mem
@@ -16747,7 +16747,7 @@ func rewriteValueARM_OpZero(v *Value) bool {
 		mem := v.Args[1]
 		v.reset(OpARMMOVBstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -16768,7 +16768,7 @@ func rewriteValueARM_OpZero(v *Value) bool {
 		}
 		v.reset(OpARMMOVHstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -16786,13 +16786,13 @@ func rewriteValueARM_OpZero(v *Value) bool {
 		v.reset(OpARMMOVBstore)
 		v.AuxInt = 1
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARMMOVBstore, TypeMem)
 		v1.AuxInt = 0
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -16814,7 +16814,7 @@ func rewriteValueARM_OpZero(v *Value) bool {
 		}
 		v.reset(OpARMMOVWstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -16836,13 +16836,13 @@ func rewriteValueARM_OpZero(v *Value) bool {
 		v.reset(OpARMMOVHstore)
 		v.AuxInt = 2
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARMMOVHstore, TypeMem)
 		v1.AuxInt = 0
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -16861,25 +16861,25 @@ func rewriteValueARM_OpZero(v *Value) bool {
 		v.reset(OpARMMOVBstore)
 		v.AuxInt = 3
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARMMOVBstore, TypeMem)
 		v1.AuxInt = 2
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARMMOVBstore, TypeMem)
 		v3.AuxInt = 1
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpARMMOVBstore, TypeMem)
 		v5.AuxInt = 0
 		v5.AddArg(ptr)
-		v6 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v6.AuxInt = 0
 		v5.AddArg(v6)
 		v5.AddArg(mem)
@@ -16900,19 +16900,19 @@ func rewriteValueARM_OpZero(v *Value) bool {
 		v.reset(OpARMMOVBstore)
 		v.AuxInt = 2
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARMMOVBstore, TypeMem)
 		v1.AuxInt = 1
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARMMOVBstore, TypeMem)
 		v3.AuxInt = 0
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v3.AddArg(mem)
@@ -16934,7 +16934,7 @@ func rewriteValueARM_OpZero(v *Value) bool {
 		v.reset(OpARMDUFFZERO)
 		v.AuxInt = 4 * (128 - int64(s/4))
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -16958,7 +16958,7 @@ func rewriteValueARM_OpZero(v *Value) bool {
 		v0.AuxInt = s - moveSize(t.(Type).Alignment(), config)
 		v0.AddArg(ptr)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpARMMOVWconst, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpARMMOVWconst, types.UInt32)
 		v1.AuxInt = 0
 		v.AddArg(v1)
 		v.AddArg(mem)
@@ -17002,16 +17002,16 @@ func rewriteValueARM_OpZeroExt8to32(v *Value) bool {
 func rewriteValueARM_OpZeromask(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Zeromask x)
 	// cond:
-	// result: (SRAconst (RSBshiftRL <fe.TypeInt32()> x x [1]) [31])
+	// result: (SRAconst (RSBshiftRL <types.Int32> x x [1]) [31])
 	for {
 		x := v.Args[0]
 		v.reset(OpARMSRAconst)
 		v.AuxInt = 31
-		v0 := b.NewValue0(v.Pos, OpARMRSBshiftRL, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpARMRSBshiftRL, types.Int32)
 		v0.AuxInt = 1
 		v0.AddArg(x)
 		v0.AddArg(x)
@@ -17024,6 +17024,8 @@ func rewriteBlockARM(b *Block) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &config.Types
+	_ = types
 	switch b.Kind {
 	case BlockARMEQ:
 		// match: (EQ (FlagEQ) yes no)
diff --git a/src/cmd/compile/internal/ssa/rewriteARM64.go b/src/cmd/compile/internal/ssa/rewriteARM64.go
index 5db99c2745..009e36b90f 100644
--- a/src/cmd/compile/internal/ssa/rewriteARM64.go
+++ b/src/cmd/compile/internal/ssa/rewriteARM64.go
@@ -9509,18 +9509,18 @@ func rewriteValueARM64_OpAvg64u(v *Value) bool {
 func rewriteValueARM64_OpBitLen64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (BitLen64 x)
 	// cond:
-	// result: (SUB (MOVDconst [64]) (CLZ <fe.TypeInt()> x))
+	// result: (SUB (MOVDconst [64]) (CLZ <types.Int> x))
 	for {
 		x := v.Args[0]
 		v.reset(OpARM64SUB)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 64
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpARM64CLZ, fe.TypeInt())
+		v1 := b.NewValue0(v.Pos, OpARM64CLZ, types.Int)
 		v1.AddArg(x)
 		v.AddArg(v1)
 		return true
@@ -9529,16 +9529,16 @@ func rewriteValueARM64_OpBitLen64(v *Value) bool {
 func rewriteValueARM64_OpBitRev16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (BitRev16 x)
 	// cond:
-	// result: (SRLconst [48] (RBIT <fe.TypeUInt64()> x))
+	// result: (SRLconst [48] (RBIT <types.UInt64> x))
 	for {
 		x := v.Args[0]
 		v.reset(OpARM64SRLconst)
 		v.AuxInt = 48
-		v0 := b.NewValue0(v.Pos, OpARM64RBIT, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64RBIT, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -9569,16 +9569,16 @@ func rewriteValueARM64_OpBitRev64(v *Value) bool {
 func rewriteValueARM64_OpBitRev8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (BitRev8 x)
 	// cond:
-	// result: (SRLconst [56] (RBIT <fe.TypeUInt64()> x))
+	// result: (SRLconst [56] (RBIT <types.UInt64> x))
 	for {
 		x := v.Args[0]
 		v.reset(OpARM64SRLconst)
 		v.AuxInt = 56
-		v0 := b.NewValue0(v.Pos, OpARM64RBIT, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64RBIT, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -10000,8 +10000,8 @@ func rewriteValueARM64_OpCvt64to64F(v *Value) bool {
 func rewriteValueARM64_OpDiv16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16 x y)
 	// cond:
 	// result: (DIVW (SignExt16to32 x) (SignExt16to32 y))
@@ -10009,10 +10009,10 @@ func rewriteValueARM64_OpDiv16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64DIVW)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -10021,8 +10021,8 @@ func rewriteValueARM64_OpDiv16(v *Value) bool {
 func rewriteValueARM64_OpDiv16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16u x y)
 	// cond:
 	// result: (UDIVW (ZeroExt16to32 x) (ZeroExt16to32 y))
@@ -10030,10 +10030,10 @@ func rewriteValueARM64_OpDiv16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64UDIVW)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -10120,8 +10120,8 @@ func rewriteValueARM64_OpDiv64u(v *Value) bool {
 func rewriteValueARM64_OpDiv8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8 x y)
 	// cond:
 	// result: (DIVW (SignExt8to32 x) (SignExt8to32 y))
@@ -10129,10 +10129,10 @@ func rewriteValueARM64_OpDiv8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64DIVW)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -10141,8 +10141,8 @@ func rewriteValueARM64_OpDiv8(v *Value) bool {
 func rewriteValueARM64_OpDiv8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8u x y)
 	// cond:
 	// result: (UDIVW (ZeroExt8to32 x) (ZeroExt8to32 y))
@@ -10150,10 +10150,10 @@ func rewriteValueARM64_OpDiv8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64UDIVW)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -10162,8 +10162,8 @@ func rewriteValueARM64_OpDiv8u(v *Value) bool {
 func rewriteValueARM64_OpEq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq16 x y)
 	// cond:
 	// result: (Equal (CMPW (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -10172,10 +10172,10 @@ func rewriteValueARM64_OpEq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64Equal)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -10253,8 +10253,8 @@ func rewriteValueARM64_OpEq64F(v *Value) bool {
 func rewriteValueARM64_OpEq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq8 x y)
 	// cond:
 	// result: (Equal (CMPW (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -10263,10 +10263,10 @@ func rewriteValueARM64_OpEq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64Equal)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -10276,19 +10276,19 @@ func rewriteValueARM64_OpEq8(v *Value) bool {
 func rewriteValueARM64_OpEqB(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (EqB x y)
 	// cond:
-	// result: (XOR (MOVDconst [1]) (XOR <fe.TypeBool()> x y))
+	// result: (XOR (MOVDconst [1]) (XOR <types.Bool> x y))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64XOR)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpARM64XOR, fe.TypeBool())
+		v1 := b.NewValue0(v.Pos, OpARM64XOR, types.Bool)
 		v1.AddArg(x)
 		v1.AddArg(y)
 		v.AddArg(v1)
@@ -10315,8 +10315,8 @@ func rewriteValueARM64_OpEqPtr(v *Value) bool {
 func rewriteValueARM64_OpGeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq16 x y)
 	// cond:
 	// result: (GreaterEqual (CMPW (SignExt16to32 x) (SignExt16to32 y)))
@@ -10325,10 +10325,10 @@ func rewriteValueARM64_OpGeq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64GreaterEqual)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -10338,8 +10338,8 @@ func rewriteValueARM64_OpGeq16(v *Value) bool {
 func rewriteValueARM64_OpGeq16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq16U x y)
 	// cond:
 	// result: (GreaterEqualU (CMPW (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -10348,10 +10348,10 @@ func rewriteValueARM64_OpGeq16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64GreaterEqualU)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -10463,8 +10463,8 @@ func rewriteValueARM64_OpGeq64U(v *Value) bool {
 func rewriteValueARM64_OpGeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq8 x y)
 	// cond:
 	// result: (GreaterEqual (CMPW (SignExt8to32 x) (SignExt8to32 y)))
@@ -10473,10 +10473,10 @@ func rewriteValueARM64_OpGeq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64GreaterEqual)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -10486,8 +10486,8 @@ func rewriteValueARM64_OpGeq8(v *Value) bool {
 func rewriteValueARM64_OpGeq8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq8U x y)
 	// cond:
 	// result: (GreaterEqualU (CMPW (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -10496,10 +10496,10 @@ func rewriteValueARM64_OpGeq8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64GreaterEqualU)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -10518,8 +10518,8 @@ func rewriteValueARM64_OpGetClosurePtr(v *Value) bool {
 func rewriteValueARM64_OpGreater16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater16 x y)
 	// cond:
 	// result: (GreaterThan (CMPW (SignExt16to32 x) (SignExt16to32 y)))
@@ -10528,10 +10528,10 @@ func rewriteValueARM64_OpGreater16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64GreaterThan)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -10541,8 +10541,8 @@ func rewriteValueARM64_OpGreater16(v *Value) bool {
 func rewriteValueARM64_OpGreater16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater16U x y)
 	// cond:
 	// result: (GreaterThanU (CMPW (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -10551,10 +10551,10 @@ func rewriteValueARM64_OpGreater16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64GreaterThanU)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -10666,8 +10666,8 @@ func rewriteValueARM64_OpGreater64U(v *Value) bool {
 func rewriteValueARM64_OpGreater8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater8 x y)
 	// cond:
 	// result: (GreaterThan (CMPW (SignExt8to32 x) (SignExt8to32 y)))
@@ -10676,10 +10676,10 @@ func rewriteValueARM64_OpGreater8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64GreaterThan)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -10689,8 +10689,8 @@ func rewriteValueARM64_OpGreater8(v *Value) bool {
 func rewriteValueARM64_OpGreater8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater8U x y)
 	// cond:
 	// result: (GreaterThanU (CMPW (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -10699,10 +10699,10 @@ func rewriteValueARM64_OpGreater8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64GreaterThanU)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -10712,17 +10712,17 @@ func rewriteValueARM64_OpGreater8U(v *Value) bool {
 func rewriteValueARM64_OpHmul32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Hmul32 x y)
 	// cond:
-	// result: (SRAconst (MULL <fe.TypeInt64()> x y) [32])
+	// result: (SRAconst (MULL <types.Int64> x y) [32])
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRAconst)
 		v.AuxInt = 32
-		v0 := b.NewValue0(v.Pos, OpARM64MULL, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MULL, types.Int64)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -10732,17 +10732,17 @@ func rewriteValueARM64_OpHmul32(v *Value) bool {
 func rewriteValueARM64_OpHmul32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Hmul32u x y)
 	// cond:
-	// result: (SRAconst (UMULL <fe.TypeUInt64()> x y) [32])
+	// result: (SRAconst (UMULL <types.UInt64> x y) [32])
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRAconst)
 		v.AuxInt = 32
-		v0 := b.NewValue0(v.Pos, OpARM64UMULL, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64UMULL, types.UInt64)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -10843,8 +10843,8 @@ func rewriteValueARM64_OpIsSliceInBounds(v *Value) bool {
 func rewriteValueARM64_OpLeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq16 x y)
 	// cond:
 	// result: (LessEqual (CMPW (SignExt16to32 x) (SignExt16to32 y)))
@@ -10853,10 +10853,10 @@ func rewriteValueARM64_OpLeq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64LessEqual)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -10866,8 +10866,8 @@ func rewriteValueARM64_OpLeq16(v *Value) bool {
 func rewriteValueARM64_OpLeq16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq16U x y)
 	// cond:
 	// result: (LessEqualU (CMPW (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -10876,10 +10876,10 @@ func rewriteValueARM64_OpLeq16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64LessEqualU)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -10991,8 +10991,8 @@ func rewriteValueARM64_OpLeq64U(v *Value) bool {
 func rewriteValueARM64_OpLeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq8 x y)
 	// cond:
 	// result: (LessEqual (CMPW (SignExt8to32 x) (SignExt8to32 y)))
@@ -11001,10 +11001,10 @@ func rewriteValueARM64_OpLeq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64LessEqual)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -11014,8 +11014,8 @@ func rewriteValueARM64_OpLeq8(v *Value) bool {
 func rewriteValueARM64_OpLeq8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq8U x y)
 	// cond:
 	// result: (LessEqualU (CMPW (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -11024,10 +11024,10 @@ func rewriteValueARM64_OpLeq8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64LessEqualU)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -11037,8 +11037,8 @@ func rewriteValueARM64_OpLeq8U(v *Value) bool {
 func rewriteValueARM64_OpLess16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less16 x y)
 	// cond:
 	// result: (LessThan (CMPW (SignExt16to32 x) (SignExt16to32 y)))
@@ -11047,10 +11047,10 @@ func rewriteValueARM64_OpLess16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64LessThan)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -11060,8 +11060,8 @@ func rewriteValueARM64_OpLess16(v *Value) bool {
 func rewriteValueARM64_OpLess16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less16U x y)
 	// cond:
 	// result: (LessThanU (CMPW (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -11070,10 +11070,10 @@ func rewriteValueARM64_OpLess16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64LessThanU)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -11185,8 +11185,8 @@ func rewriteValueARM64_OpLess64U(v *Value) bool {
 func rewriteValueARM64_OpLess8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less8 x y)
 	// cond:
 	// result: (LessThan (CMPW (SignExt8to32 x) (SignExt8to32 y)))
@@ -11195,10 +11195,10 @@ func rewriteValueARM64_OpLess8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64LessThan)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -11208,8 +11208,8 @@ func rewriteValueARM64_OpLess8(v *Value) bool {
 func rewriteValueARM64_OpLess8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less8U x y)
 	// cond:
 	// result: (LessThanU (CMPW (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -11218,10 +11218,10 @@ func rewriteValueARM64_OpLess8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64LessThanU)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -11384,8 +11384,8 @@ func rewriteValueARM64_OpLoad(v *Value) bool {
 func rewriteValueARM64_OpLsh16x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x16 <t> x y)
 	// cond:
 	// result: (CSELULT (SLL <t> x (ZeroExt16to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt16to64 y)))
@@ -11396,7 +11396,7 @@ func rewriteValueARM64_OpLsh16x16(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11405,7 +11405,7 @@ func rewriteValueARM64_OpLsh16x16(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -11415,8 +11415,8 @@ func rewriteValueARM64_OpLsh16x16(v *Value) bool {
 func rewriteValueARM64_OpLsh16x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x32 <t> x y)
 	// cond:
 	// result: (CSELULT (SLL <t> x (ZeroExt32to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt32to64 y)))
@@ -11427,7 +11427,7 @@ func rewriteValueARM64_OpLsh16x32(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11436,7 +11436,7 @@ func rewriteValueARM64_OpLsh16x32(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -11505,8 +11505,8 @@ func rewriteValueARM64_OpLsh16x64(v *Value) bool {
 func rewriteValueARM64_OpLsh16x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x8  <t> x y)
 	// cond:
 	// result: (CSELULT (SLL <t> x (ZeroExt8to64  y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt8to64  y)))
@@ -11517,7 +11517,7 @@ func rewriteValueARM64_OpLsh16x8(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11526,7 +11526,7 @@ func rewriteValueARM64_OpLsh16x8(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -11536,8 +11536,8 @@ func rewriteValueARM64_OpLsh16x8(v *Value) bool {
 func rewriteValueARM64_OpLsh32x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x16 <t> x y)
 	// cond:
 	// result: (CSELULT (SLL <t> x (ZeroExt16to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt16to64 y)))
@@ -11548,7 +11548,7 @@ func rewriteValueARM64_OpLsh32x16(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11557,7 +11557,7 @@ func rewriteValueARM64_OpLsh32x16(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -11567,8 +11567,8 @@ func rewriteValueARM64_OpLsh32x16(v *Value) bool {
 func rewriteValueARM64_OpLsh32x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x32 <t> x y)
 	// cond:
 	// result: (CSELULT (SLL <t> x (ZeroExt32to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt32to64 y)))
@@ -11579,7 +11579,7 @@ func rewriteValueARM64_OpLsh32x32(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11588,7 +11588,7 @@ func rewriteValueARM64_OpLsh32x32(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -11657,8 +11657,8 @@ func rewriteValueARM64_OpLsh32x64(v *Value) bool {
 func rewriteValueARM64_OpLsh32x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x8  <t> x y)
 	// cond:
 	// result: (CSELULT (SLL <t> x (ZeroExt8to64  y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt8to64  y)))
@@ -11669,7 +11669,7 @@ func rewriteValueARM64_OpLsh32x8(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11678,7 +11678,7 @@ func rewriteValueARM64_OpLsh32x8(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -11688,8 +11688,8 @@ func rewriteValueARM64_OpLsh32x8(v *Value) bool {
 func rewriteValueARM64_OpLsh64x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x16 <t> x y)
 	// cond:
 	// result: (CSELULT (SLL <t> x (ZeroExt16to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt16to64 y)))
@@ -11700,7 +11700,7 @@ func rewriteValueARM64_OpLsh64x16(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11709,7 +11709,7 @@ func rewriteValueARM64_OpLsh64x16(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -11719,8 +11719,8 @@ func rewriteValueARM64_OpLsh64x16(v *Value) bool {
 func rewriteValueARM64_OpLsh64x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x32 <t> x y)
 	// cond:
 	// result: (CSELULT (SLL <t> x (ZeroExt32to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt32to64 y)))
@@ -11731,7 +11731,7 @@ func rewriteValueARM64_OpLsh64x32(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11740,7 +11740,7 @@ func rewriteValueARM64_OpLsh64x32(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -11809,8 +11809,8 @@ func rewriteValueARM64_OpLsh64x64(v *Value) bool {
 func rewriteValueARM64_OpLsh64x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x8  <t> x y)
 	// cond:
 	// result: (CSELULT (SLL <t> x (ZeroExt8to64  y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt8to64  y)))
@@ -11821,7 +11821,7 @@ func rewriteValueARM64_OpLsh64x8(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11830,7 +11830,7 @@ func rewriteValueARM64_OpLsh64x8(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -11840,8 +11840,8 @@ func rewriteValueARM64_OpLsh64x8(v *Value) bool {
 func rewriteValueARM64_OpLsh8x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x16 <t> x y)
 	// cond:
 	// result: (CSELULT (SLL <t> x (ZeroExt16to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt16to64 y)))
@@ -11852,7 +11852,7 @@ func rewriteValueARM64_OpLsh8x16(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11861,7 +11861,7 @@ func rewriteValueARM64_OpLsh8x16(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -11871,8 +11871,8 @@ func rewriteValueARM64_OpLsh8x16(v *Value) bool {
 func rewriteValueARM64_OpLsh8x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x32 <t> x y)
 	// cond:
 	// result: (CSELULT (SLL <t> x (ZeroExt32to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt32to64 y)))
@@ -11883,7 +11883,7 @@ func rewriteValueARM64_OpLsh8x32(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11892,7 +11892,7 @@ func rewriteValueARM64_OpLsh8x32(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -11961,8 +11961,8 @@ func rewriteValueARM64_OpLsh8x64(v *Value) bool {
 func rewriteValueARM64_OpLsh8x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x8  <t> x y)
 	// cond:
 	// result: (CSELULT (SLL <t> x (ZeroExt8to64  y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt8to64  y)))
@@ -11973,7 +11973,7 @@ func rewriteValueARM64_OpLsh8x8(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11982,7 +11982,7 @@ func rewriteValueARM64_OpLsh8x8(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -11992,8 +11992,8 @@ func rewriteValueARM64_OpLsh8x8(v *Value) bool {
 func rewriteValueARM64_OpMod16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16 x y)
 	// cond:
 	// result: (MODW (SignExt16to32 x) (SignExt16to32 y))
@@ -12001,10 +12001,10 @@ func rewriteValueARM64_OpMod16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64MODW)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -12013,8 +12013,8 @@ func rewriteValueARM64_OpMod16(v *Value) bool {
 func rewriteValueARM64_OpMod16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16u x y)
 	// cond:
 	// result: (UMODW (ZeroExt16to32 x) (ZeroExt16to32 y))
@@ -12022,10 +12022,10 @@ func rewriteValueARM64_OpMod16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64UMODW)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -12086,8 +12086,8 @@ func rewriteValueARM64_OpMod64u(v *Value) bool {
 func rewriteValueARM64_OpMod8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8 x y)
 	// cond:
 	// result: (MODW (SignExt8to32 x) (SignExt8to32 y))
@@ -12095,10 +12095,10 @@ func rewriteValueARM64_OpMod8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64MODW)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -12107,8 +12107,8 @@ func rewriteValueARM64_OpMod8(v *Value) bool {
 func rewriteValueARM64_OpMod8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8u x y)
 	// cond:
 	// result: (UMODW (ZeroExt8to32 x) (ZeroExt8to32 y))
@@ -12116,10 +12116,10 @@ func rewriteValueARM64_OpMod8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64UMODW)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -12130,8 +12130,8 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Move [0] _ _ mem)
 	// cond:
 	// result: mem
@@ -12157,7 +12157,7 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpARM64MOVBstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVBUload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVBUload, types.UInt8)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -12176,7 +12176,7 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpARM64MOVHstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVHUload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVHUload, types.UInt16)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -12195,7 +12195,7 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpARM64MOVWstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVWUload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVWUload, types.UInt32)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -12214,7 +12214,7 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpARM64MOVDstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDload, types.UInt64)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -12234,14 +12234,14 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		v.reset(OpARM64MOVBstore)
 		v.AuxInt = 2
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVBUload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVBUload, types.UInt8)
 		v0.AuxInt = 2
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64MOVHstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVHUload, fe.TypeUInt16())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVHUload, types.UInt16)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -12262,14 +12262,14 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		v.reset(OpARM64MOVBstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVBUload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVBUload, types.UInt8)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64MOVWstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVWUload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVWUload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -12290,14 +12290,14 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		v.reset(OpARM64MOVHstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVHUload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVHUload, types.UInt16)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64MOVWstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVWUload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVWUload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -12318,7 +12318,7 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		v.reset(OpARM64MOVBstore)
 		v.AuxInt = 6
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVBUload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVBUload, types.UInt8)
 		v0.AuxInt = 6
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -12326,14 +12326,14 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpARM64MOVHstore, TypeMem)
 		v1.AuxInt = 4
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVHUload, fe.TypeUInt16())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVHUload, types.UInt16)
 		v2.AuxInt = 4
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64MOVWstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpARM64MOVWUload, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpARM64MOVWUload, types.UInt32)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -12355,14 +12355,14 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		v.reset(OpARM64MOVWstore)
 		v.AuxInt = 8
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVWUload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVWUload, types.UInt32)
 		v0.AuxInt = 8
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64MOVDstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVDload, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVDload, types.UInt64)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -12383,14 +12383,14 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		v.reset(OpARM64MOVDstore)
 		v.AuxInt = 8
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDload, types.UInt64)
 		v0.AuxInt = 8
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64MOVDstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVDload, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVDload, types.UInt64)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -12411,7 +12411,7 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		v.reset(OpARM64MOVDstore)
 		v.AuxInt = 16
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDload, types.UInt64)
 		v0.AuxInt = 16
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -12419,14 +12419,14 @@ func rewriteValueARM64_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpARM64MOVDstore, TypeMem)
 		v1.AuxInt = 8
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVDload, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVDload, types.UInt64)
 		v2.AuxInt = 8
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64MOVDstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpARM64MOVDload, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpARM64MOVDload, types.UInt64)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -12652,8 +12652,8 @@ func rewriteValueARM64_OpNeg8(v *Value) bool {
 func rewriteValueARM64_OpNeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq16 x y)
 	// cond:
 	// result: (NotEqual (CMPW (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -12662,10 +12662,10 @@ func rewriteValueARM64_OpNeq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64NotEqual)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -12743,8 +12743,8 @@ func rewriteValueARM64_OpNeq64F(v *Value) bool {
 func rewriteValueARM64_OpNeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq8 x y)
 	// cond:
 	// result: (NotEqual (CMPW (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -12753,10 +12753,10 @@ func rewriteValueARM64_OpNeq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64NotEqual)
 		v0 := b.NewValue0(v.Pos, OpARM64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -12809,15 +12809,15 @@ func rewriteValueARM64_OpNilCheck(v *Value) bool {
 func rewriteValueARM64_OpNot(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Not x)
 	// cond:
 	// result: (XOR (MOVDconst [1]) x)
 	for {
 		x := v.Args[0]
 		v.reset(OpARM64XOR)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
 		v.AddArg(x)
@@ -12943,8 +12943,8 @@ func rewriteValueARM64_OpRound64F(v *Value) bool {
 func rewriteValueARM64_OpRsh16Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux16 <t> x y)
 	// cond:
 	// result: (CSELULT (SRL <t> (ZeroExt16to64 x) (ZeroExt16to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt16to64 y)))
@@ -12954,10 +12954,10 @@ func rewriteValueARM64_OpRsh16Ux16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -12966,7 +12966,7 @@ func rewriteValueARM64_OpRsh16Ux16(v *Value) bool {
 		v.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -12976,8 +12976,8 @@ func rewriteValueARM64_OpRsh16Ux16(v *Value) bool {
 func rewriteValueARM64_OpRsh16Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux32 <t> x y)
 	// cond:
 	// result: (CSELULT (SRL <t> (ZeroExt16to64 x) (ZeroExt32to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt32to64 y)))
@@ -12987,10 +12987,10 @@ func rewriteValueARM64_OpRsh16Ux32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -12999,7 +12999,7 @@ func rewriteValueARM64_OpRsh16Ux32(v *Value) bool {
 		v.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -13009,8 +13009,8 @@ func rewriteValueARM64_OpRsh16Ux32(v *Value) bool {
 func rewriteValueARM64_OpRsh16Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux64 x (MOVDconst [c]))
 	// cond: uint64(c) < 16
 	// result: (SRLconst (ZeroExt16to64 x) [c])
@@ -13026,7 +13026,7 @@ func rewriteValueARM64_OpRsh16Ux64(v *Value) bool {
 		}
 		v.reset(OpARM64SRLconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -13056,7 +13056,7 @@ func rewriteValueARM64_OpRsh16Ux64(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -13074,8 +13074,8 @@ func rewriteValueARM64_OpRsh16Ux64(v *Value) bool {
 func rewriteValueARM64_OpRsh16Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux8  <t> x y)
 	// cond:
 	// result: (CSELULT (SRL <t> (ZeroExt16to64 x) (ZeroExt8to64  y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt8to64  y)))
@@ -13085,10 +13085,10 @@ func rewriteValueARM64_OpRsh16Ux8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13097,7 +13097,7 @@ func rewriteValueARM64_OpRsh16Ux8(v *Value) bool {
 		v.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -13107,8 +13107,8 @@ func rewriteValueARM64_OpRsh16Ux8(v *Value) bool {
 func rewriteValueARM64_OpRsh16x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x16 x y)
 	// cond:
 	// result: (SRA (SignExt16to64 x) (CSELULT <y.Type> (ZeroExt16to64 y) (Const64 <y.Type> [63]) (CMPconst [64] (ZeroExt16to64 y))))
@@ -13116,11 +13116,11 @@ func rewriteValueARM64_OpRsh16x16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpConst64, y.Type)
@@ -13128,7 +13128,7 @@ func rewriteValueARM64_OpRsh16x16(v *Value) bool {
 		v1.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -13139,8 +13139,8 @@ func rewriteValueARM64_OpRsh16x16(v *Value) bool {
 func rewriteValueARM64_OpRsh16x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x32 x y)
 	// cond:
 	// result: (SRA (SignExt16to64 x) (CSELULT <y.Type> (ZeroExt32to64 y) (Const64 <y.Type> [63]) (CMPconst [64] (ZeroExt32to64 y))))
@@ -13148,11 +13148,11 @@ func rewriteValueARM64_OpRsh16x32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpConst64, y.Type)
@@ -13160,7 +13160,7 @@ func rewriteValueARM64_OpRsh16x32(v *Value) bool {
 		v1.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -13171,8 +13171,8 @@ func rewriteValueARM64_OpRsh16x32(v *Value) bool {
 func rewriteValueARM64_OpRsh16x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x64  x (MOVDconst [c]))
 	// cond: uint64(c) < 16
 	// result: (SRAconst (SignExt16to64 x) [c])
@@ -13188,7 +13188,7 @@ func rewriteValueARM64_OpRsh16x64(v *Value) bool {
 		}
 		v.reset(OpARM64SRAconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -13208,7 +13208,7 @@ func rewriteValueARM64_OpRsh16x64(v *Value) bool {
 		}
 		v.reset(OpARM64SRAconst)
 		v.AuxInt = 63
-		v0 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -13220,7 +13220,7 @@ func rewriteValueARM64_OpRsh16x64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
@@ -13239,8 +13239,8 @@ func rewriteValueARM64_OpRsh16x64(v *Value) bool {
 func rewriteValueARM64_OpRsh16x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x8  x y)
 	// cond:
 	// result: (SRA (SignExt16to64 x) (CSELULT <y.Type> (ZeroExt8to64  y) (Const64 <y.Type> [63]) (CMPconst [64] (ZeroExt8to64  y))))
@@ -13248,11 +13248,11 @@ func rewriteValueARM64_OpRsh16x8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpConst64, y.Type)
@@ -13260,7 +13260,7 @@ func rewriteValueARM64_OpRsh16x8(v *Value) bool {
 		v1.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -13271,8 +13271,8 @@ func rewriteValueARM64_OpRsh16x8(v *Value) bool {
 func rewriteValueARM64_OpRsh32Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux16 <t> x y)
 	// cond:
 	// result: (CSELULT (SRL <t> (ZeroExt32to64 x) (ZeroExt16to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt16to64 y)))
@@ -13282,10 +13282,10 @@ func rewriteValueARM64_OpRsh32Ux16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13294,7 +13294,7 @@ func rewriteValueARM64_OpRsh32Ux16(v *Value) bool {
 		v.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -13304,8 +13304,8 @@ func rewriteValueARM64_OpRsh32Ux16(v *Value) bool {
 func rewriteValueARM64_OpRsh32Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux32 <t> x y)
 	// cond:
 	// result: (CSELULT (SRL <t> (ZeroExt32to64 x) (ZeroExt32to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt32to64 y)))
@@ -13315,10 +13315,10 @@ func rewriteValueARM64_OpRsh32Ux32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13327,7 +13327,7 @@ func rewriteValueARM64_OpRsh32Ux32(v *Value) bool {
 		v.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -13337,8 +13337,8 @@ func rewriteValueARM64_OpRsh32Ux32(v *Value) bool {
 func rewriteValueARM64_OpRsh32Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux64 x (MOVDconst [c]))
 	// cond: uint64(c) < 32
 	// result: (SRLconst (ZeroExt32to64 x) [c])
@@ -13354,7 +13354,7 @@ func rewriteValueARM64_OpRsh32Ux64(v *Value) bool {
 		}
 		v.reset(OpARM64SRLconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -13384,7 +13384,7 @@ func rewriteValueARM64_OpRsh32Ux64(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -13402,8 +13402,8 @@ func rewriteValueARM64_OpRsh32Ux64(v *Value) bool {
 func rewriteValueARM64_OpRsh32Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux8  <t> x y)
 	// cond:
 	// result: (CSELULT (SRL <t> (ZeroExt32to64 x) (ZeroExt8to64  y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt8to64  y)))
@@ -13413,10 +13413,10 @@ func rewriteValueARM64_OpRsh32Ux8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13425,7 +13425,7 @@ func rewriteValueARM64_OpRsh32Ux8(v *Value) bool {
 		v.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -13435,8 +13435,8 @@ func rewriteValueARM64_OpRsh32Ux8(v *Value) bool {
 func rewriteValueARM64_OpRsh32x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x16 x y)
 	// cond:
 	// result: (SRA (SignExt32to64 x) (CSELULT <y.Type> (ZeroExt16to64 y) (Const64 <y.Type> [63]) (CMPconst [64] (ZeroExt16to64 y))))
@@ -13444,11 +13444,11 @@ func rewriteValueARM64_OpRsh32x16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpConst64, y.Type)
@@ -13456,7 +13456,7 @@ func rewriteValueARM64_OpRsh32x16(v *Value) bool {
 		v1.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -13467,8 +13467,8 @@ func rewriteValueARM64_OpRsh32x16(v *Value) bool {
 func rewriteValueARM64_OpRsh32x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x32 x y)
 	// cond:
 	// result: (SRA (SignExt32to64 x) (CSELULT <y.Type> (ZeroExt32to64 y) (Const64 <y.Type> [63]) (CMPconst [64] (ZeroExt32to64 y))))
@@ -13476,11 +13476,11 @@ func rewriteValueARM64_OpRsh32x32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpConst64, y.Type)
@@ -13488,7 +13488,7 @@ func rewriteValueARM64_OpRsh32x32(v *Value) bool {
 		v1.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -13499,8 +13499,8 @@ func rewriteValueARM64_OpRsh32x32(v *Value) bool {
 func rewriteValueARM64_OpRsh32x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x64  x (MOVDconst [c]))
 	// cond: uint64(c) < 32
 	// result: (SRAconst (SignExt32to64 x) [c])
@@ -13516,7 +13516,7 @@ func rewriteValueARM64_OpRsh32x64(v *Value) bool {
 		}
 		v.reset(OpARM64SRAconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -13536,7 +13536,7 @@ func rewriteValueARM64_OpRsh32x64(v *Value) bool {
 		}
 		v.reset(OpARM64SRAconst)
 		v.AuxInt = 63
-		v0 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -13548,7 +13548,7 @@ func rewriteValueARM64_OpRsh32x64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
@@ -13567,8 +13567,8 @@ func rewriteValueARM64_OpRsh32x64(v *Value) bool {
 func rewriteValueARM64_OpRsh32x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x8  x y)
 	// cond:
 	// result: (SRA (SignExt32to64 x) (CSELULT <y.Type> (ZeroExt8to64  y) (Const64 <y.Type> [63]) (CMPconst [64] (ZeroExt8to64  y))))
@@ -13576,11 +13576,11 @@ func rewriteValueARM64_OpRsh32x8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpConst64, y.Type)
@@ -13588,7 +13588,7 @@ func rewriteValueARM64_OpRsh32x8(v *Value) bool {
 		v1.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -13599,8 +13599,8 @@ func rewriteValueARM64_OpRsh32x8(v *Value) bool {
 func rewriteValueARM64_OpRsh64Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux16 <t> x y)
 	// cond:
 	// result: (CSELULT (SRL <t> x (ZeroExt16to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt16to64 y)))
@@ -13611,7 +13611,7 @@ func rewriteValueARM64_OpRsh64Ux16(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -13620,7 +13620,7 @@ func rewriteValueARM64_OpRsh64Ux16(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -13630,8 +13630,8 @@ func rewriteValueARM64_OpRsh64Ux16(v *Value) bool {
 func rewriteValueARM64_OpRsh64Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux32 <t> x y)
 	// cond:
 	// result: (CSELULT (SRL <t> x (ZeroExt32to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt32to64 y)))
@@ -13642,7 +13642,7 @@ func rewriteValueARM64_OpRsh64Ux32(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -13651,7 +13651,7 @@ func rewriteValueARM64_OpRsh64Ux32(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -13720,8 +13720,8 @@ func rewriteValueARM64_OpRsh64Ux64(v *Value) bool {
 func rewriteValueARM64_OpRsh64Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux8  <t> x y)
 	// cond:
 	// result: (CSELULT (SRL <t> x (ZeroExt8to64  y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt8to64  y)))
@@ -13732,7 +13732,7 @@ func rewriteValueARM64_OpRsh64Ux8(v *Value) bool {
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -13741,7 +13741,7 @@ func rewriteValueARM64_OpRsh64Ux8(v *Value) bool {
 		v.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -13751,8 +13751,8 @@ func rewriteValueARM64_OpRsh64Ux8(v *Value) bool {
 func rewriteValueARM64_OpRsh64x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x16 x y)
 	// cond:
 	// result: (SRA x (CSELULT <y.Type> (ZeroExt16to64 y) (Const64 <y.Type> [63]) (CMPconst [64] (ZeroExt16to64 y))))
@@ -13762,7 +13762,7 @@ func rewriteValueARM64_OpRsh64x16(v *Value) bool {
 		v.reset(OpARM64SRA)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpConst64, y.Type)
@@ -13770,7 +13770,7 @@ func rewriteValueARM64_OpRsh64x16(v *Value) bool {
 		v0.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v0.AddArg(v3)
@@ -13781,8 +13781,8 @@ func rewriteValueARM64_OpRsh64x16(v *Value) bool {
 func rewriteValueARM64_OpRsh64x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x32 x y)
 	// cond:
 	// result: (SRA x (CSELULT <y.Type> (ZeroExt32to64 y) (Const64 <y.Type> [63]) (CMPconst [64] (ZeroExt32to64 y))))
@@ -13792,7 +13792,7 @@ func rewriteValueARM64_OpRsh64x32(v *Value) bool {
 		v.reset(OpARM64SRA)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpConst64, y.Type)
@@ -13800,7 +13800,7 @@ func rewriteValueARM64_OpRsh64x32(v *Value) bool {
 		v0.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v0.AddArg(v3)
@@ -13871,8 +13871,8 @@ func rewriteValueARM64_OpRsh64x64(v *Value) bool {
 func rewriteValueARM64_OpRsh64x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x8  x y)
 	// cond:
 	// result: (SRA x (CSELULT <y.Type> (ZeroExt8to64  y) (Const64 <y.Type> [63]) (CMPconst [64] (ZeroExt8to64  y))))
@@ -13882,7 +13882,7 @@ func rewriteValueARM64_OpRsh64x8(v *Value) bool {
 		v.reset(OpARM64SRA)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpConst64, y.Type)
@@ -13890,7 +13890,7 @@ func rewriteValueARM64_OpRsh64x8(v *Value) bool {
 		v0.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v3.AuxInt = 64
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v0.AddArg(v3)
@@ -13901,8 +13901,8 @@ func rewriteValueARM64_OpRsh64x8(v *Value) bool {
 func rewriteValueARM64_OpRsh8Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux16 <t> x y)
 	// cond:
 	// result: (CSELULT (SRL <t> (ZeroExt8to64 x) (ZeroExt16to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt16to64 y)))
@@ -13912,10 +13912,10 @@ func rewriteValueARM64_OpRsh8Ux16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13924,7 +13924,7 @@ func rewriteValueARM64_OpRsh8Ux16(v *Value) bool {
 		v.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -13934,8 +13934,8 @@ func rewriteValueARM64_OpRsh8Ux16(v *Value) bool {
 func rewriteValueARM64_OpRsh8Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux32 <t> x y)
 	// cond:
 	// result: (CSELULT (SRL <t> (ZeroExt8to64 x) (ZeroExt32to64 y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt32to64 y)))
@@ -13945,10 +13945,10 @@ func rewriteValueARM64_OpRsh8Ux32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -13957,7 +13957,7 @@ func rewriteValueARM64_OpRsh8Ux32(v *Value) bool {
 		v.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -13967,8 +13967,8 @@ func rewriteValueARM64_OpRsh8Ux32(v *Value) bool {
 func rewriteValueARM64_OpRsh8Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux64  x (MOVDconst [c]))
 	// cond: uint64(c) < 8
 	// result: (SRLconst (ZeroExt8to64  x) [c])
@@ -13984,7 +13984,7 @@ func rewriteValueARM64_OpRsh8Ux64(v *Value) bool {
 		}
 		v.reset(OpARM64SRLconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -14014,7 +14014,7 @@ func rewriteValueARM64_OpRsh8Ux64(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -14032,8 +14032,8 @@ func rewriteValueARM64_OpRsh8Ux64(v *Value) bool {
 func rewriteValueARM64_OpRsh8Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux8  <t> x y)
 	// cond:
 	// result: (CSELULT (SRL <t> (ZeroExt8to64 x) (ZeroExt8to64  y)) (Const64 <t> [0]) (CMPconst [64] (ZeroExt8to64  y)))
@@ -14043,10 +14043,10 @@ func rewriteValueARM64_OpRsh8Ux8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpARM64CSELULT)
 		v0 := b.NewValue0(v.Pos, OpARM64SRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -14055,7 +14055,7 @@ func rewriteValueARM64_OpRsh8Ux8(v *Value) bool {
 		v.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -14065,8 +14065,8 @@ func rewriteValueARM64_OpRsh8Ux8(v *Value) bool {
 func rewriteValueARM64_OpRsh8x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x16 x y)
 	// cond:
 	// result: (SRA (SignExt8to64 x) (CSELULT <y.Type> (ZeroExt16to64 y) (Const64 <y.Type> [63]) (CMPconst [64] (ZeroExt16to64 y))))
@@ -14074,11 +14074,11 @@ func rewriteValueARM64_OpRsh8x16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpConst64, y.Type)
@@ -14086,7 +14086,7 @@ func rewriteValueARM64_OpRsh8x16(v *Value) bool {
 		v1.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -14097,8 +14097,8 @@ func rewriteValueARM64_OpRsh8x16(v *Value) bool {
 func rewriteValueARM64_OpRsh8x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x32 x y)
 	// cond:
 	// result: (SRA (SignExt8to64 x) (CSELULT <y.Type> (ZeroExt32to64 y) (Const64 <y.Type> [63]) (CMPconst [64] (ZeroExt32to64 y))))
@@ -14106,11 +14106,11 @@ func rewriteValueARM64_OpRsh8x32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpConst64, y.Type)
@@ -14118,7 +14118,7 @@ func rewriteValueARM64_OpRsh8x32(v *Value) bool {
 		v1.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -14129,8 +14129,8 @@ func rewriteValueARM64_OpRsh8x32(v *Value) bool {
 func rewriteValueARM64_OpRsh8x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x64   x (MOVDconst [c]))
 	// cond: uint64(c) < 8
 	// result: (SRAconst (SignExt8to64  x) [c])
@@ -14146,7 +14146,7 @@ func rewriteValueARM64_OpRsh8x64(v *Value) bool {
 		}
 		v.reset(OpARM64SRAconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -14166,7 +14166,7 @@ func rewriteValueARM64_OpRsh8x64(v *Value) bool {
 		}
 		v.reset(OpARM64SRAconst)
 		v.AuxInt = 63
-		v0 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -14178,7 +14178,7 @@ func rewriteValueARM64_OpRsh8x64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
@@ -14197,8 +14197,8 @@ func rewriteValueARM64_OpRsh8x64(v *Value) bool {
 func rewriteValueARM64_OpRsh8x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x8  x y)
 	// cond:
 	// result: (SRA (SignExt8to64 x) (CSELULT <y.Type> (ZeroExt8to64  y) (Const64 <y.Type> [63]) (CMPconst [64] (ZeroExt8to64  y))))
@@ -14206,11 +14206,11 @@ func rewriteValueARM64_OpRsh8x8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpARM64SRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64CSELULT, y.Type)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpConst64, y.Type)
@@ -14218,7 +14218,7 @@ func rewriteValueARM64_OpRsh8x8(v *Value) bool {
 		v1.AddArg(v3)
 		v4 := b.NewValue0(v.Pos, OpARM64CMPconst, TypeFlags)
 		v4.AuxInt = 64
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -14660,8 +14660,8 @@ func rewriteValueARM64_OpZero(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Zero [0] _ mem)
 	// cond:
 	// result: mem
@@ -14686,7 +14686,7 @@ func rewriteValueARM64_OpZero(v *Value) bool {
 		mem := v.Args[1]
 		v.reset(OpARM64MOVBstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -14703,7 +14703,7 @@ func rewriteValueARM64_OpZero(v *Value) bool {
 		mem := v.Args[1]
 		v.reset(OpARM64MOVHstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -14720,7 +14720,7 @@ func rewriteValueARM64_OpZero(v *Value) bool {
 		mem := v.Args[1]
 		v.reset(OpARM64MOVWstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -14737,7 +14737,7 @@ func rewriteValueARM64_OpZero(v *Value) bool {
 		mem := v.Args[1]
 		v.reset(OpARM64MOVDstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -14755,12 +14755,12 @@ func rewriteValueARM64_OpZero(v *Value) bool {
 		v.reset(OpARM64MOVBstore)
 		v.AuxInt = 2
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64MOVHstore, TypeMem)
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -14779,12 +14779,12 @@ func rewriteValueARM64_OpZero(v *Value) bool {
 		v.reset(OpARM64MOVBstore)
 		v.AuxInt = 4
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64MOVWstore, TypeMem)
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -14803,12 +14803,12 @@ func rewriteValueARM64_OpZero(v *Value) bool {
 		v.reset(OpARM64MOVHstore)
 		v.AuxInt = 4
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64MOVWstore, TypeMem)
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -14827,18 +14827,18 @@ func rewriteValueARM64_OpZero(v *Value) bool {
 		v.reset(OpARM64MOVBstore)
 		v.AuxInt = 6
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64MOVHstore, TypeMem)
 		v1.AuxInt = 4
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64MOVWstore, TypeMem)
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v3.AddArg(mem)
@@ -14858,12 +14858,12 @@ func rewriteValueARM64_OpZero(v *Value) bool {
 		v.reset(OpARM64MOVWstore)
 		v.AuxInt = 8
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64MOVDstore, TypeMem)
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -14882,12 +14882,12 @@ func rewriteValueARM64_OpZero(v *Value) bool {
 		v.reset(OpARM64MOVDstore)
 		v.AuxInt = 8
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64MOVDstore, TypeMem)
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -14906,18 +14906,18 @@ func rewriteValueARM64_OpZero(v *Value) bool {
 		v.reset(OpARM64MOVDstore)
 		v.AuxInt = 16
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpARM64MOVDstore, TypeMem)
 		v1.AuxInt = 8
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpARM64MOVDstore, TypeMem)
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpARM64MOVDconst, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpARM64MOVDconst, types.UInt64)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v3.AddArg(mem)
@@ -15056,6 +15056,8 @@ func rewriteBlockARM64(b *Block) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &config.Types
+	_ = types
 	switch b.Kind {
 	case BlockARM64EQ:
 		// match: (EQ (CMPconst [0] x) yes no)
diff --git a/src/cmd/compile/internal/ssa/rewriteMIPS.go b/src/cmd/compile/internal/ssa/rewriteMIPS.go
index 5e95c57678..a555c58e58 100644
--- a/src/cmd/compile/internal/ssa/rewriteMIPS.go
+++ b/src/cmd/compile/internal/ssa/rewriteMIPS.go
@@ -684,11 +684,11 @@ func rewriteValueMIPS_OpAtomicAnd8(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (AtomicAnd8  ptr val mem)
 	// cond: !config.BigEndian
-	// result: (LoweredAtomicAnd (AND <fe.TypeUInt32().PtrTo()> (MOVWconst [^3]) ptr) 		(OR <fe.TypeUInt32()> (SLL <fe.TypeUInt32()> (ZeroExt8to32 val) 			(SLLconst <fe.TypeUInt32()> [3] 				(ANDconst  <fe.TypeUInt32()> [3] ptr))) 		(NORconst [0] <fe.TypeUInt32()> (SLL <fe.TypeUInt32()> 			(MOVWconst [0xff]) (SLLconst <fe.TypeUInt32()> [3] 				(ANDconst <fe.TypeUInt32()> [3] 					(XORconst <fe.TypeUInt32()> [3] ptr)))))) mem)
+	// result: (LoweredAtomicAnd (AND <types.UInt32.PtrTo()> (MOVWconst [^3]) ptr) 		(OR <types.UInt32> (SLL <types.UInt32> (ZeroExt8to32 val) 			(SLLconst <types.UInt32> [3] 				(ANDconst  <types.UInt32> [3] ptr))) 		(NORconst [0] <types.UInt32> (SLL <types.UInt32> 			(MOVWconst [0xff]) (SLLconst <types.UInt32> [3] 				(ANDconst <types.UInt32> [3] 					(XORconst <types.UInt32> [3] ptr)))))) mem)
 	for {
 		ptr := v.Args[0]
 		val := v.Args[1]
@@ -697,36 +697,36 @@ func rewriteValueMIPS_OpAtomicAnd8(v *Value) bool {
 			break
 		}
 		v.reset(OpMIPSLoweredAtomicAnd)
-		v0 := b.NewValue0(v.Pos, OpMIPSAND, fe.TypeUInt32().PtrTo())
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSAND, types.UInt32.PtrTo())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = ^3
 		v0.AddArg(v1)
 		v0.AddArg(ptr)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSOR, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpMIPSSLL, fe.TypeUInt32())
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSOR, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpMIPSSLL, types.UInt32)
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v4.AddArg(val)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpMIPSSLLconst, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpMIPSSLLconst, types.UInt32)
 		v5.AuxInt = 3
-		v6 := b.NewValue0(v.Pos, OpMIPSANDconst, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpMIPSANDconst, types.UInt32)
 		v6.AuxInt = 3
 		v6.AddArg(ptr)
 		v5.AddArg(v6)
 		v3.AddArg(v5)
 		v2.AddArg(v3)
-		v7 := b.NewValue0(v.Pos, OpMIPSNORconst, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpMIPSNORconst, types.UInt32)
 		v7.AuxInt = 0
-		v8 := b.NewValue0(v.Pos, OpMIPSSLL, fe.TypeUInt32())
-		v9 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v8 := b.NewValue0(v.Pos, OpMIPSSLL, types.UInt32)
+		v9 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v9.AuxInt = 0xff
 		v8.AddArg(v9)
-		v10 := b.NewValue0(v.Pos, OpMIPSSLLconst, fe.TypeUInt32())
+		v10 := b.NewValue0(v.Pos, OpMIPSSLLconst, types.UInt32)
 		v10.AuxInt = 3
-		v11 := b.NewValue0(v.Pos, OpMIPSANDconst, fe.TypeUInt32())
+		v11 := b.NewValue0(v.Pos, OpMIPSANDconst, types.UInt32)
 		v11.AuxInt = 3
-		v12 := b.NewValue0(v.Pos, OpMIPSXORconst, fe.TypeUInt32())
+		v12 := b.NewValue0(v.Pos, OpMIPSXORconst, types.UInt32)
 		v12.AuxInt = 3
 		v12.AddArg(ptr)
 		v11.AddArg(v12)
@@ -740,7 +740,7 @@ func rewriteValueMIPS_OpAtomicAnd8(v *Value) bool {
 	}
 	// match: (AtomicAnd8  ptr val mem)
 	// cond: config.BigEndian
-	// result: (LoweredAtomicAnd (AND <fe.TypeUInt32().PtrTo()> (MOVWconst [^3]) ptr) 		(OR <fe.TypeUInt32()> (SLL <fe.TypeUInt32()> (ZeroExt8to32 val) 			(SLLconst <fe.TypeUInt32()> [3] 				(ANDconst  <fe.TypeUInt32()> [3] 					(XORconst <fe.TypeUInt32()> [3] ptr)))) 		(NORconst [0] <fe.TypeUInt32()> (SLL <fe.TypeUInt32()> 			(MOVWconst [0xff]) (SLLconst <fe.TypeUInt32()> [3] 				(ANDconst <fe.TypeUInt32()> [3] 					(XORconst <fe.TypeUInt32()> [3] ptr)))))) mem)
+	// result: (LoweredAtomicAnd (AND <types.UInt32.PtrTo()> (MOVWconst [^3]) ptr) 		(OR <types.UInt32> (SLL <types.UInt32> (ZeroExt8to32 val) 			(SLLconst <types.UInt32> [3] 				(ANDconst  <types.UInt32> [3] 					(XORconst <types.UInt32> [3] ptr)))) 		(NORconst [0] <types.UInt32> (SLL <types.UInt32> 			(MOVWconst [0xff]) (SLLconst <types.UInt32> [3] 				(ANDconst <types.UInt32> [3] 					(XORconst <types.UInt32> [3] ptr)))))) mem)
 	for {
 		ptr := v.Args[0]
 		val := v.Args[1]
@@ -749,39 +749,39 @@ func rewriteValueMIPS_OpAtomicAnd8(v *Value) bool {
 			break
 		}
 		v.reset(OpMIPSLoweredAtomicAnd)
-		v0 := b.NewValue0(v.Pos, OpMIPSAND, fe.TypeUInt32().PtrTo())
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSAND, types.UInt32.PtrTo())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = ^3
 		v0.AddArg(v1)
 		v0.AddArg(ptr)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSOR, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpMIPSSLL, fe.TypeUInt32())
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSOR, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpMIPSSLL, types.UInt32)
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v4.AddArg(val)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpMIPSSLLconst, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpMIPSSLLconst, types.UInt32)
 		v5.AuxInt = 3
-		v6 := b.NewValue0(v.Pos, OpMIPSANDconst, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpMIPSANDconst, types.UInt32)
 		v6.AuxInt = 3
-		v7 := b.NewValue0(v.Pos, OpMIPSXORconst, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpMIPSXORconst, types.UInt32)
 		v7.AuxInt = 3
 		v7.AddArg(ptr)
 		v6.AddArg(v7)
 		v5.AddArg(v6)
 		v3.AddArg(v5)
 		v2.AddArg(v3)
-		v8 := b.NewValue0(v.Pos, OpMIPSNORconst, fe.TypeUInt32())
+		v8 := b.NewValue0(v.Pos, OpMIPSNORconst, types.UInt32)
 		v8.AuxInt = 0
-		v9 := b.NewValue0(v.Pos, OpMIPSSLL, fe.TypeUInt32())
-		v10 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpMIPSSLL, types.UInt32)
+		v10 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v10.AuxInt = 0xff
 		v9.AddArg(v10)
-		v11 := b.NewValue0(v.Pos, OpMIPSSLLconst, fe.TypeUInt32())
+		v11 := b.NewValue0(v.Pos, OpMIPSSLLconst, types.UInt32)
 		v11.AuxInt = 3
-		v12 := b.NewValue0(v.Pos, OpMIPSANDconst, fe.TypeUInt32())
+		v12 := b.NewValue0(v.Pos, OpMIPSANDconst, types.UInt32)
 		v12.AuxInt = 3
-		v13 := b.NewValue0(v.Pos, OpMIPSXORconst, fe.TypeUInt32())
+		v13 := b.NewValue0(v.Pos, OpMIPSXORconst, types.UInt32)
 		v13.AuxInt = 3
 		v13.AddArg(ptr)
 		v12.AddArg(v13)
@@ -858,11 +858,11 @@ func rewriteValueMIPS_OpAtomicOr8(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (AtomicOr8 ptr val mem)
 	// cond: !config.BigEndian
-	// result: (LoweredAtomicOr (AND <fe.TypeUInt32().PtrTo()> (MOVWconst [^3]) ptr) 		(SLL <fe.TypeUInt32()> (ZeroExt8to32 val) 			(SLLconst <fe.TypeUInt32()> [3] 				(ANDconst <fe.TypeUInt32()> [3] ptr))) mem)
+	// result: (LoweredAtomicOr (AND <types.UInt32.PtrTo()> (MOVWconst [^3]) ptr) 		(SLL <types.UInt32> (ZeroExt8to32 val) 			(SLLconst <types.UInt32> [3] 				(ANDconst <types.UInt32> [3] ptr))) mem)
 	for {
 		ptr := v.Args[0]
 		val := v.Args[1]
@@ -871,19 +871,19 @@ func rewriteValueMIPS_OpAtomicOr8(v *Value) bool {
 			break
 		}
 		v.reset(OpMIPSLoweredAtomicOr)
-		v0 := b.NewValue0(v.Pos, OpMIPSAND, fe.TypeUInt32().PtrTo())
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSAND, types.UInt32.PtrTo())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = ^3
 		v0.AddArg(v1)
 		v0.AddArg(ptr)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSSLL, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSSLL, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v3.AddArg(val)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpMIPSSLLconst, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpMIPSSLLconst, types.UInt32)
 		v4.AuxInt = 3
-		v5 := b.NewValue0(v.Pos, OpMIPSANDconst, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpMIPSANDconst, types.UInt32)
 		v5.AuxInt = 3
 		v5.AddArg(ptr)
 		v4.AddArg(v5)
@@ -894,7 +894,7 @@ func rewriteValueMIPS_OpAtomicOr8(v *Value) bool {
 	}
 	// match: (AtomicOr8 ptr val mem)
 	// cond: config.BigEndian
-	// result: (LoweredAtomicOr (AND <fe.TypeUInt32().PtrTo()> (MOVWconst [^3]) ptr) 		(SLL <fe.TypeUInt32()> (ZeroExt8to32 val) 			(SLLconst <fe.TypeUInt32()> [3] 				(ANDconst <fe.TypeUInt32()> [3] 					(XORconst <fe.TypeUInt32()> [3] ptr)))) mem)
+	// result: (LoweredAtomicOr (AND <types.UInt32.PtrTo()> (MOVWconst [^3]) ptr) 		(SLL <types.UInt32> (ZeroExt8to32 val) 			(SLLconst <types.UInt32> [3] 				(ANDconst <types.UInt32> [3] 					(XORconst <types.UInt32> [3] ptr)))) mem)
 	for {
 		ptr := v.Args[0]
 		val := v.Args[1]
@@ -903,21 +903,21 @@ func rewriteValueMIPS_OpAtomicOr8(v *Value) bool {
 			break
 		}
 		v.reset(OpMIPSLoweredAtomicOr)
-		v0 := b.NewValue0(v.Pos, OpMIPSAND, fe.TypeUInt32().PtrTo())
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSAND, types.UInt32.PtrTo())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = ^3
 		v0.AddArg(v1)
 		v0.AddArg(ptr)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSSLL, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSSLL, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v3.AddArg(val)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpMIPSSLLconst, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpMIPSSLLconst, types.UInt32)
 		v4.AuxInt = 3
-		v5 := b.NewValue0(v.Pos, OpMIPSANDconst, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpMIPSANDconst, types.UInt32)
 		v5.AuxInt = 3
-		v6 := b.NewValue0(v.Pos, OpMIPSXORconst, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpMIPSXORconst, types.UInt32)
 		v6.AuxInt = 3
 		v6.AddArg(ptr)
 		v5.AddArg(v6)
@@ -984,8 +984,8 @@ func rewriteValueMIPS_OpAvg32u(v *Value) bool {
 func rewriteValueMIPS_OpBitLen32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (BitLen32 <t> x)
 	// cond:
 	// result: (SUB (MOVWconst [32]) (CLZ <t> x))
@@ -993,7 +993,7 @@ func rewriteValueMIPS_OpBitLen32(v *Value) bool {
 		t := v.Type
 		x := v.Args[0]
 		v.reset(OpMIPSSUB)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 32
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSCLZ, t)
@@ -1147,8 +1147,8 @@ func rewriteValueMIPS_OpConvert(v *Value) bool {
 func rewriteValueMIPS_OpCtz32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Ctz32 <t> x)
 	// cond:
 	// result: (SUB (MOVWconst [32]) (CLZ <t> (SUBconst <t> [1] (AND <t> x (NEG <t> x)))))
@@ -1156,7 +1156,7 @@ func rewriteValueMIPS_OpCtz32(v *Value) bool {
 		t := v.Type
 		x := v.Args[0]
 		v.reset(OpMIPSSUB)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 32
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSCLZ, t)
@@ -1242,8 +1242,8 @@ func rewriteValueMIPS_OpCvt64Fto32F(v *Value) bool {
 func rewriteValueMIPS_OpDiv16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16 x y)
 	// cond:
 	// result: (Select1 (DIV (SignExt16to32 x) (SignExt16to32 y)))
@@ -1251,11 +1251,11 @@ func rewriteValueMIPS_OpDiv16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPSDIV, MakeTuple(fe.TypeInt32(), fe.TypeInt32()))
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSDIV, MakeTuple(types.Int32, types.Int32))
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1265,8 +1265,8 @@ func rewriteValueMIPS_OpDiv16(v *Value) bool {
 func rewriteValueMIPS_OpDiv16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16u x y)
 	// cond:
 	// result: (Select1 (DIVU (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -1274,11 +1274,11 @@ func rewriteValueMIPS_OpDiv16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPSDIVU, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSDIVU, MakeTuple(types.UInt32, types.UInt32))
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1288,8 +1288,8 @@ func rewriteValueMIPS_OpDiv16u(v *Value) bool {
 func rewriteValueMIPS_OpDiv32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div32 x y)
 	// cond:
 	// result: (Select1 (DIV x y))
@@ -1297,7 +1297,7 @@ func rewriteValueMIPS_OpDiv32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPSDIV, MakeTuple(fe.TypeInt32(), fe.TypeInt32()))
+		v0 := b.NewValue0(v.Pos, OpMIPSDIV, MakeTuple(types.Int32, types.Int32))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -1320,8 +1320,8 @@ func rewriteValueMIPS_OpDiv32F(v *Value) bool {
 func rewriteValueMIPS_OpDiv32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div32u x y)
 	// cond:
 	// result: (Select1 (DIVU x y))
@@ -1329,7 +1329,7 @@ func rewriteValueMIPS_OpDiv32u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPSDIVU, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
+		v0 := b.NewValue0(v.Pos, OpMIPSDIVU, MakeTuple(types.UInt32, types.UInt32))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -1352,8 +1352,8 @@ func rewriteValueMIPS_OpDiv64F(v *Value) bool {
 func rewriteValueMIPS_OpDiv8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8 x y)
 	// cond:
 	// result: (Select1 (DIV (SignExt8to32 x) (SignExt8to32 y)))
@@ -1361,11 +1361,11 @@ func rewriteValueMIPS_OpDiv8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPSDIV, MakeTuple(fe.TypeInt32(), fe.TypeInt32()))
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSDIV, MakeTuple(types.Int32, types.Int32))
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1375,8 +1375,8 @@ func rewriteValueMIPS_OpDiv8(v *Value) bool {
 func rewriteValueMIPS_OpDiv8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8u x y)
 	// cond:
 	// result: (Select1 (DIVU (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -1384,11 +1384,11 @@ func rewriteValueMIPS_OpDiv8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPSDIVU, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSDIVU, MakeTuple(types.UInt32, types.UInt32))
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1398,8 +1398,8 @@ func rewriteValueMIPS_OpDiv8u(v *Value) bool {
 func rewriteValueMIPS_OpEq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq16 x y)
 	// cond:
 	// result: (SGTUconst [1] (XOR (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -1408,11 +1408,11 @@ func rewriteValueMIPS_OpEq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSSGTUconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSXOR, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSXOR, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1422,8 +1422,8 @@ func rewriteValueMIPS_OpEq16(v *Value) bool {
 func rewriteValueMIPS_OpEq32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq32 x y)
 	// cond:
 	// result: (SGTUconst [1] (XOR x y))
@@ -1432,7 +1432,7 @@ func rewriteValueMIPS_OpEq32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSSGTUconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSXOR, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSXOR, types.UInt32)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -1476,8 +1476,8 @@ func rewriteValueMIPS_OpEq64F(v *Value) bool {
 func rewriteValueMIPS_OpEq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq8 x y)
 	// cond:
 	// result: (SGTUconst [1] (XOR (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -1486,11 +1486,11 @@ func rewriteValueMIPS_OpEq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSSGTUconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSXOR, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSXOR, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1500,17 +1500,17 @@ func rewriteValueMIPS_OpEq8(v *Value) bool {
 func rewriteValueMIPS_OpEqB(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (EqB x y)
 	// cond:
-	// result: (XORconst [1] (XOR <fe.TypeBool()> x y))
+	// result: (XORconst [1] (XOR <types.Bool> x y))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSXOR, fe.TypeBool())
+		v0 := b.NewValue0(v.Pos, OpMIPSXOR, types.Bool)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -1520,8 +1520,8 @@ func rewriteValueMIPS_OpEqB(v *Value) bool {
 func rewriteValueMIPS_OpEqPtr(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (EqPtr x y)
 	// cond:
 	// result: (SGTUconst [1] (XOR x y))
@@ -1530,7 +1530,7 @@ func rewriteValueMIPS_OpEqPtr(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSSGTUconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSXOR, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSXOR, types.UInt32)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -1540,8 +1540,8 @@ func rewriteValueMIPS_OpEqPtr(v *Value) bool {
 func rewriteValueMIPS_OpGeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq16 x y)
 	// cond:
 	// result: (XORconst [1] (SGT (SignExt16to32 y) (SignExt16to32 x)))
@@ -1550,11 +1550,11 @@ func rewriteValueMIPS_OpGeq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGT, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGT, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(x)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1564,8 +1564,8 @@ func rewriteValueMIPS_OpGeq16(v *Value) bool {
 func rewriteValueMIPS_OpGeq16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq16U x y)
 	// cond:
 	// result: (XORconst [1] (SGTU (ZeroExt16to32 y) (ZeroExt16to32 x)))
@@ -1574,11 +1574,11 @@ func rewriteValueMIPS_OpGeq16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(x)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1588,8 +1588,8 @@ func rewriteValueMIPS_OpGeq16U(v *Value) bool {
 func rewriteValueMIPS_OpGeq32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq32 x y)
 	// cond:
 	// result: (XORconst [1] (SGT y x))
@@ -1598,7 +1598,7 @@ func rewriteValueMIPS_OpGeq32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGT, fe.TypeBool())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGT, types.Bool)
 		v0.AddArg(y)
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -1625,8 +1625,8 @@ func rewriteValueMIPS_OpGeq32F(v *Value) bool {
 func rewriteValueMIPS_OpGeq32U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq32U x y)
 	// cond:
 	// result: (XORconst [1] (SGTU y x))
@@ -1635,7 +1635,7 @@ func rewriteValueMIPS_OpGeq32U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, fe.TypeBool())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, types.Bool)
 		v0.AddArg(y)
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -1662,8 +1662,8 @@ func rewriteValueMIPS_OpGeq64F(v *Value) bool {
 func rewriteValueMIPS_OpGeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq8 x y)
 	// cond:
 	// result: (XORconst [1] (SGT (SignExt8to32 y) (SignExt8to32 x)))
@@ -1672,11 +1672,11 @@ func rewriteValueMIPS_OpGeq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGT, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGT, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(x)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1686,8 +1686,8 @@ func rewriteValueMIPS_OpGeq8(v *Value) bool {
 func rewriteValueMIPS_OpGeq8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq8U x y)
 	// cond:
 	// result: (XORconst [1] (SGTU (ZeroExt8to32 y) (ZeroExt8to32 x)))
@@ -1696,11 +1696,11 @@ func rewriteValueMIPS_OpGeq8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(x)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1719,8 +1719,8 @@ func rewriteValueMIPS_OpGetClosurePtr(v *Value) bool {
 func rewriteValueMIPS_OpGreater16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater16 x y)
 	// cond:
 	// result: (SGT (SignExt16to32 x) (SignExt16to32 y))
@@ -1728,10 +1728,10 @@ func rewriteValueMIPS_OpGreater16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSGT)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1740,8 +1740,8 @@ func rewriteValueMIPS_OpGreater16(v *Value) bool {
 func rewriteValueMIPS_OpGreater16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater16U x y)
 	// cond:
 	// result: (SGTU (ZeroExt16to32 x) (ZeroExt16to32 y))
@@ -1749,10 +1749,10 @@ func rewriteValueMIPS_OpGreater16U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSGTU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1821,8 +1821,8 @@ func rewriteValueMIPS_OpGreater64F(v *Value) bool {
 func rewriteValueMIPS_OpGreater8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater8 x y)
 	// cond:
 	// result: (SGT (SignExt8to32 x) (SignExt8to32 y))
@@ -1830,10 +1830,10 @@ func rewriteValueMIPS_OpGreater8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSGT)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1842,8 +1842,8 @@ func rewriteValueMIPS_OpGreater8(v *Value) bool {
 func rewriteValueMIPS_OpGreater8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater8U x y)
 	// cond:
 	// result: (SGTU (ZeroExt8to32 x) (ZeroExt8to32 y))
@@ -1851,10 +1851,10 @@ func rewriteValueMIPS_OpGreater8U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSGTU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1863,8 +1863,8 @@ func rewriteValueMIPS_OpGreater8U(v *Value) bool {
 func rewriteValueMIPS_OpHmul32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Hmul32 x y)
 	// cond:
 	// result: (Select0 (MULT x y))
@@ -1872,7 +1872,7 @@ func rewriteValueMIPS_OpHmul32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPSMULT, MakeTuple(fe.TypeInt32(), fe.TypeInt32()))
+		v0 := b.NewValue0(v.Pos, OpMIPSMULT, MakeTuple(types.Int32, types.Int32))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -1882,8 +1882,8 @@ func rewriteValueMIPS_OpHmul32(v *Value) bool {
 func rewriteValueMIPS_OpHmul32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Hmul32u x y)
 	// cond:
 	// result: (Select0 (MULTU x y))
@@ -1891,7 +1891,7 @@ func rewriteValueMIPS_OpHmul32u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPSMULTU, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
+		v0 := b.NewValue0(v.Pos, OpMIPSMULTU, MakeTuple(types.UInt32, types.UInt32))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -1929,8 +1929,8 @@ func rewriteValueMIPS_OpIsInBounds(v *Value) bool {
 func rewriteValueMIPS_OpIsNonNil(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (IsNonNil ptr)
 	// cond:
 	// result: (SGTU ptr (MOVWconst [0]))
@@ -1938,7 +1938,7 @@ func rewriteValueMIPS_OpIsNonNil(v *Value) bool {
 		ptr := v.Args[0]
 		v.reset(OpMIPSSGTU)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		return true
@@ -1947,8 +1947,8 @@ func rewriteValueMIPS_OpIsNonNil(v *Value) bool {
 func rewriteValueMIPS_OpIsSliceInBounds(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (IsSliceInBounds idx len)
 	// cond:
 	// result: (XORconst [1] (SGTU idx len))
@@ -1957,7 +1957,7 @@ func rewriteValueMIPS_OpIsSliceInBounds(v *Value) bool {
 		len := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, fe.TypeBool())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, types.Bool)
 		v0.AddArg(idx)
 		v0.AddArg(len)
 		v.AddArg(v0)
@@ -1967,8 +1967,8 @@ func rewriteValueMIPS_OpIsSliceInBounds(v *Value) bool {
 func rewriteValueMIPS_OpLeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq16 x y)
 	// cond:
 	// result: (XORconst [1] (SGT (SignExt16to32 x) (SignExt16to32 y)))
@@ -1977,11 +1977,11 @@ func rewriteValueMIPS_OpLeq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGT, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGT, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1991,8 +1991,8 @@ func rewriteValueMIPS_OpLeq16(v *Value) bool {
 func rewriteValueMIPS_OpLeq16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq16U x y)
 	// cond:
 	// result: (XORconst [1] (SGTU (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -2001,11 +2001,11 @@ func rewriteValueMIPS_OpLeq16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2015,8 +2015,8 @@ func rewriteValueMIPS_OpLeq16U(v *Value) bool {
 func rewriteValueMIPS_OpLeq32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq32 x y)
 	// cond:
 	// result: (XORconst [1] (SGT x y))
@@ -2025,7 +2025,7 @@ func rewriteValueMIPS_OpLeq32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGT, fe.TypeBool())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGT, types.Bool)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -2052,8 +2052,8 @@ func rewriteValueMIPS_OpLeq32F(v *Value) bool {
 func rewriteValueMIPS_OpLeq32U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq32U x y)
 	// cond:
 	// result: (XORconst [1] (SGTU x y))
@@ -2062,7 +2062,7 @@ func rewriteValueMIPS_OpLeq32U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, fe.TypeBool())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, types.Bool)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -2089,8 +2089,8 @@ func rewriteValueMIPS_OpLeq64F(v *Value) bool {
 func rewriteValueMIPS_OpLeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq8 x y)
 	// cond:
 	// result: (XORconst [1] (SGT (SignExt8to32 x) (SignExt8to32 y)))
@@ -2099,11 +2099,11 @@ func rewriteValueMIPS_OpLeq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGT, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGT, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2113,8 +2113,8 @@ func rewriteValueMIPS_OpLeq8(v *Value) bool {
 func rewriteValueMIPS_OpLeq8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq8U x y)
 	// cond:
 	// result: (XORconst [1] (SGTU (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -2123,11 +2123,11 @@ func rewriteValueMIPS_OpLeq8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSXORconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2137,8 +2137,8 @@ func rewriteValueMIPS_OpLeq8U(v *Value) bool {
 func rewriteValueMIPS_OpLess16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less16 x y)
 	// cond:
 	// result: (SGT (SignExt16to32 y) (SignExt16to32 x))
@@ -2146,10 +2146,10 @@ func rewriteValueMIPS_OpLess16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSGT)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v.AddArg(v1)
 		return true
@@ -2158,8 +2158,8 @@ func rewriteValueMIPS_OpLess16(v *Value) bool {
 func rewriteValueMIPS_OpLess16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less16U x y)
 	// cond:
 	// result: (SGTU (ZeroExt16to32 y) (ZeroExt16to32 x))
@@ -2167,10 +2167,10 @@ func rewriteValueMIPS_OpLess16U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSGTU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v.AddArg(v1)
 		return true
@@ -2239,8 +2239,8 @@ func rewriteValueMIPS_OpLess64F(v *Value) bool {
 func rewriteValueMIPS_OpLess8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less8 x y)
 	// cond:
 	// result: (SGT (SignExt8to32 y) (SignExt8to32 x))
@@ -2248,10 +2248,10 @@ func rewriteValueMIPS_OpLess8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSGT)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v.AddArg(v1)
 		return true
@@ -2260,8 +2260,8 @@ func rewriteValueMIPS_OpLess8(v *Value) bool {
 func rewriteValueMIPS_OpLess8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less8U x y)
 	// cond:
 	// result: (SGTU (ZeroExt8to32 y) (ZeroExt8to32 x))
@@ -2269,10 +2269,10 @@ func rewriteValueMIPS_OpLess8U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSGTU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v.AddArg(v1)
 		return true
@@ -2404,8 +2404,8 @@ func rewriteValueMIPS_OpLoad(v *Value) bool {
 func rewriteValueMIPS_OpLsh16x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x16 <t> x y)
 	// cond:
 	// result: (CMOVZ (SLL <t> x (ZeroExt16to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt16to32 y)))
@@ -2416,16 +2416,16 @@ func rewriteValueMIPS_OpLsh16x16(v *Value) bool {
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -2435,8 +2435,8 @@ func rewriteValueMIPS_OpLsh16x16(v *Value) bool {
 func rewriteValueMIPS_OpLsh16x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x32 <t> x y)
 	// cond:
 	// result: (CMOVZ (SLL <t> x y) (MOVWconst [0]) (SGTUconst [32] y))
@@ -2449,10 +2449,10 @@ func rewriteValueMIPS_OpLsh16x32(v *Value) bool {
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = 0
 		v.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v2 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v2.AuxInt = 32
 		v2.AddArg(y)
 		v.AddArg(v2)
@@ -2499,8 +2499,8 @@ func rewriteValueMIPS_OpLsh16x64(v *Value) bool {
 func rewriteValueMIPS_OpLsh16x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x8 <t> x y)
 	// cond:
 	// result: (CMOVZ (SLL <t> x (ZeroExt8to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt8to32 y)))
@@ -2511,16 +2511,16 @@ func rewriteValueMIPS_OpLsh16x8(v *Value) bool {
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -2530,8 +2530,8 @@ func rewriteValueMIPS_OpLsh16x8(v *Value) bool {
 func rewriteValueMIPS_OpLsh32x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x16 <t> x y)
 	// cond:
 	// result: (CMOVZ (SLL <t> x (ZeroExt16to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt16to32 y)))
@@ -2542,16 +2542,16 @@ func rewriteValueMIPS_OpLsh32x16(v *Value) bool {
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -2561,8 +2561,8 @@ func rewriteValueMIPS_OpLsh32x16(v *Value) bool {
 func rewriteValueMIPS_OpLsh32x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x32 <t> x y)
 	// cond:
 	// result: (CMOVZ (SLL <t> x y) (MOVWconst [0]) (SGTUconst [32] y))
@@ -2575,10 +2575,10 @@ func rewriteValueMIPS_OpLsh32x32(v *Value) bool {
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = 0
 		v.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v2 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v2.AuxInt = 32
 		v2.AddArg(y)
 		v.AddArg(v2)
@@ -2625,8 +2625,8 @@ func rewriteValueMIPS_OpLsh32x64(v *Value) bool {
 func rewriteValueMIPS_OpLsh32x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x8 <t> x y)
 	// cond:
 	// result: (CMOVZ (SLL <t> x (ZeroExt8to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt8to32 y)))
@@ -2637,16 +2637,16 @@ func rewriteValueMIPS_OpLsh32x8(v *Value) bool {
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -2656,8 +2656,8 @@ func rewriteValueMIPS_OpLsh32x8(v *Value) bool {
 func rewriteValueMIPS_OpLsh8x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x16 <t> x y)
 	// cond:
 	// result: (CMOVZ (SLL <t> x (ZeroExt16to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt16to32 y)))
@@ -2668,16 +2668,16 @@ func rewriteValueMIPS_OpLsh8x16(v *Value) bool {
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -2687,8 +2687,8 @@ func rewriteValueMIPS_OpLsh8x16(v *Value) bool {
 func rewriteValueMIPS_OpLsh8x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x32 <t> x y)
 	// cond:
 	// result: (CMOVZ (SLL <t> x y) (MOVWconst [0]) (SGTUconst [32] y))
@@ -2701,10 +2701,10 @@ func rewriteValueMIPS_OpLsh8x32(v *Value) bool {
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = 0
 		v.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v2 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v2.AuxInt = 32
 		v2.AddArg(y)
 		v.AddArg(v2)
@@ -2751,8 +2751,8 @@ func rewriteValueMIPS_OpLsh8x64(v *Value) bool {
 func rewriteValueMIPS_OpLsh8x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x8 <t> x y)
 	// cond:
 	// result: (CMOVZ (SLL <t> x (ZeroExt8to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt8to32 y)))
@@ -2763,16 +2763,16 @@ func rewriteValueMIPS_OpLsh8x8(v *Value) bool {
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSLL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -5906,8 +5906,8 @@ func rewriteValueMIPS_OpMIPSXORconst(v *Value) bool {
 func rewriteValueMIPS_OpMod16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16 x y)
 	// cond:
 	// result: (Select0 (DIV (SignExt16to32 x) (SignExt16to32 y)))
@@ -5915,11 +5915,11 @@ func rewriteValueMIPS_OpMod16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPSDIV, MakeTuple(fe.TypeInt32(), fe.TypeInt32()))
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSDIV, MakeTuple(types.Int32, types.Int32))
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -5929,8 +5929,8 @@ func rewriteValueMIPS_OpMod16(v *Value) bool {
 func rewriteValueMIPS_OpMod16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16u x y)
 	// cond:
 	// result: (Select0 (DIVU (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -5938,11 +5938,11 @@ func rewriteValueMIPS_OpMod16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPSDIVU, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSDIVU, MakeTuple(types.UInt32, types.UInt32))
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -5952,8 +5952,8 @@ func rewriteValueMIPS_OpMod16u(v *Value) bool {
 func rewriteValueMIPS_OpMod32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod32 x y)
 	// cond:
 	// result: (Select0 (DIV x y))
@@ -5961,7 +5961,7 @@ func rewriteValueMIPS_OpMod32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPSDIV, MakeTuple(fe.TypeInt32(), fe.TypeInt32()))
+		v0 := b.NewValue0(v.Pos, OpMIPSDIV, MakeTuple(types.Int32, types.Int32))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -5971,8 +5971,8 @@ func rewriteValueMIPS_OpMod32(v *Value) bool {
 func rewriteValueMIPS_OpMod32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod32u x y)
 	// cond:
 	// result: (Select0 (DIVU x y))
@@ -5980,7 +5980,7 @@ func rewriteValueMIPS_OpMod32u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPSDIVU, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
+		v0 := b.NewValue0(v.Pos, OpMIPSDIVU, MakeTuple(types.UInt32, types.UInt32))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -5990,8 +5990,8 @@ func rewriteValueMIPS_OpMod32u(v *Value) bool {
 func rewriteValueMIPS_OpMod8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8 x y)
 	// cond:
 	// result: (Select0 (DIV (SignExt8to32 x) (SignExt8to32 y)))
@@ -5999,11 +5999,11 @@ func rewriteValueMIPS_OpMod8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPSDIV, MakeTuple(fe.TypeInt32(), fe.TypeInt32()))
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSDIV, MakeTuple(types.Int32, types.Int32))
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -6013,8 +6013,8 @@ func rewriteValueMIPS_OpMod8(v *Value) bool {
 func rewriteValueMIPS_OpMod8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8u x y)
 	// cond:
 	// result: (Select0 (DIVU (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -6022,11 +6022,11 @@ func rewriteValueMIPS_OpMod8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPSDIVU, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSDIVU, MakeTuple(types.UInt32, types.UInt32))
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -6038,8 +6038,8 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Move [0] _ _ mem)
 	// cond:
 	// result: mem
@@ -6065,7 +6065,7 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpMIPSMOVBstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVBUload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVBUload, types.UInt8)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -6088,7 +6088,7 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		}
 		v.reset(OpMIPSMOVHstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVHUload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVHUload, types.UInt16)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -6108,14 +6108,14 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v.reset(OpMIPSMOVBstore)
 		v.AuxInt = 1
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVBUload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVBUload, types.UInt8)
 		v0.AuxInt = 1
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVBstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVBUload, fe.TypeUInt8())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVBUload, types.UInt8)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -6139,7 +6139,7 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		}
 		v.reset(OpMIPSMOVWstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWload, types.UInt32)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -6163,14 +6163,14 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v.reset(OpMIPSMOVHstore)
 		v.AuxInt = 2
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVHUload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVHUload, types.UInt16)
 		v0.AuxInt = 2
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVHstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVHUload, fe.TypeUInt16())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVHUload, types.UInt16)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -6191,7 +6191,7 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v.reset(OpMIPSMOVBstore)
 		v.AuxInt = 3
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVBUload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVBUload, types.UInt8)
 		v0.AuxInt = 3
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -6199,7 +6199,7 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVBstore, TypeMem)
 		v1.AuxInt = 2
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVBUload, fe.TypeUInt8())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVBUload, types.UInt8)
 		v2.AuxInt = 2
 		v2.AddArg(src)
 		v2.AddArg(mem)
@@ -6207,14 +6207,14 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v3 := b.NewValue0(v.Pos, OpMIPSMOVBstore, TypeMem)
 		v3.AuxInt = 1
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpMIPSMOVBUload, fe.TypeUInt8())
+		v4 := b.NewValue0(v.Pos, OpMIPSMOVBUload, types.UInt8)
 		v4.AuxInt = 1
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpMIPSMOVBstore, TypeMem)
 		v5.AddArg(dst)
-		v6 := b.NewValue0(v.Pos, OpMIPSMOVBUload, fe.TypeUInt8())
+		v6 := b.NewValue0(v.Pos, OpMIPSMOVBUload, types.UInt8)
 		v6.AddArg(src)
 		v6.AddArg(mem)
 		v5.AddArg(v6)
@@ -6237,7 +6237,7 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v.reset(OpMIPSMOVBstore)
 		v.AuxInt = 2
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVBUload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVBUload, types.UInt8)
 		v0.AuxInt = 2
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -6245,14 +6245,14 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVBstore, TypeMem)
 		v1.AuxInt = 1
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVBUload, fe.TypeUInt8())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVBUload, types.UInt8)
 		v2.AuxInt = 1
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPSMOVBstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpMIPSMOVBUload, fe.TypeUInt8())
+		v4 := b.NewValue0(v.Pos, OpMIPSMOVBUload, types.UInt8)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -6278,14 +6278,14 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v.reset(OpMIPSMOVWstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWload, types.UInt32)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVWstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -6310,7 +6310,7 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v.reset(OpMIPSMOVHstore)
 		v.AuxInt = 6
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVHload, fe.TypeInt16())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVHload, types.Int16)
 		v0.AuxInt = 6
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -6318,7 +6318,7 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVHstore, TypeMem)
 		v1.AuxInt = 4
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVHload, fe.TypeInt16())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVHload, types.Int16)
 		v2.AuxInt = 4
 		v2.AddArg(src)
 		v2.AddArg(mem)
@@ -6326,14 +6326,14 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v3 := b.NewValue0(v.Pos, OpMIPSMOVHstore, TypeMem)
 		v3.AuxInt = 2
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpMIPSMOVHload, fe.TypeInt16())
+		v4 := b.NewValue0(v.Pos, OpMIPSMOVHload, types.Int16)
 		v4.AuxInt = 2
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpMIPSMOVHstore, TypeMem)
 		v5.AddArg(dst)
-		v6 := b.NewValue0(v.Pos, OpMIPSMOVHload, fe.TypeInt16())
+		v6 := b.NewValue0(v.Pos, OpMIPSMOVHload, types.Int16)
 		v6.AddArg(src)
 		v6.AddArg(mem)
 		v5.AddArg(v6)
@@ -6360,7 +6360,7 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v.reset(OpMIPSMOVHstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVHload, fe.TypeInt16())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVHload, types.Int16)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -6368,14 +6368,14 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVHstore, TypeMem)
 		v1.AuxInt = 2
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVHload, fe.TypeInt16())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVHload, types.Int16)
 		v2.AuxInt = 2
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPSMOVHstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpMIPSMOVHload, fe.TypeInt16())
+		v4 := b.NewValue0(v.Pos, OpMIPSMOVHload, types.Int16)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -6401,7 +6401,7 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v.reset(OpMIPSMOVWstore)
 		v.AuxInt = 8
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWload, types.UInt32)
 		v0.AuxInt = 8
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -6409,14 +6409,14 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVWstore, TypeMem)
 		v1.AuxInt = 4
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWload, types.UInt32)
 		v2.AuxInt = 4
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPSMOVWstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpMIPSMOVWload, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpMIPSMOVWload, types.UInt32)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -6442,7 +6442,7 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v.reset(OpMIPSMOVWstore)
 		v.AuxInt = 12
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWload, types.UInt32)
 		v0.AuxInt = 12
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -6450,7 +6450,7 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVWstore, TypeMem)
 		v1.AuxInt = 8
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWload, types.UInt32)
 		v2.AuxInt = 8
 		v2.AddArg(src)
 		v2.AddArg(mem)
@@ -6458,14 +6458,14 @@ func rewriteValueMIPS_OpMove(v *Value) bool {
 		v3 := b.NewValue0(v.Pos, OpMIPSMOVWstore, TypeMem)
 		v3.AuxInt = 4
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpMIPSMOVWload, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpMIPSMOVWload, types.UInt32)
 		v4.AuxInt = 4
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpMIPSMOVWstore, TypeMem)
 		v5.AddArg(dst)
-		v6 := b.NewValue0(v.Pos, OpMIPSMOVWload, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpMIPSMOVWload, types.UInt32)
 		v6.AddArg(src)
 		v6.AddArg(mem)
 		v5.AddArg(v6)
@@ -6636,8 +6636,8 @@ func rewriteValueMIPS_OpNeg8(v *Value) bool {
 func rewriteValueMIPS_OpNeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq16 x y)
 	// cond:
 	// result: (SGTU (XOR (ZeroExt16to32 x) (ZeroExt16to32 y)) (MOVWconst [0]))
@@ -6645,15 +6645,15 @@ func rewriteValueMIPS_OpNeq16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPSXOR, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSXOR, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v3.AuxInt = 0
 		v.AddArg(v3)
 		return true
@@ -6662,8 +6662,8 @@ func rewriteValueMIPS_OpNeq16(v *Value) bool {
 func rewriteValueMIPS_OpNeq32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq32 x y)
 	// cond:
 	// result: (SGTU (XOR x y) (MOVWconst [0]))
@@ -6671,11 +6671,11 @@ func rewriteValueMIPS_OpNeq32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPSXOR, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSXOR, types.UInt32)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = 0
 		v.AddArg(v1)
 		return true
@@ -6718,8 +6718,8 @@ func rewriteValueMIPS_OpNeq64F(v *Value) bool {
 func rewriteValueMIPS_OpNeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq8 x y)
 	// cond:
 	// result: (SGTU (XOR (ZeroExt8to32 x) (ZeroExt8to32 y)) (MOVWconst [0]))
@@ -6727,15 +6727,15 @@ func rewriteValueMIPS_OpNeq8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPSXOR, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSXOR, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v3.AuxInt = 0
 		v.AddArg(v3)
 		return true
@@ -6757,8 +6757,8 @@ func rewriteValueMIPS_OpNeqB(v *Value) bool {
 func rewriteValueMIPS_OpNeqPtr(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (NeqPtr x y)
 	// cond:
 	// result: (SGTU (XOR x y) (MOVWconst [0]))
@@ -6766,11 +6766,11 @@ func rewriteValueMIPS_OpNeqPtr(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPSXOR, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSXOR, types.UInt32)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = 0
 		v.AddArg(v1)
 		return true
@@ -6907,8 +6907,8 @@ func rewriteValueMIPS_OpRound64F(v *Value) bool {
 func rewriteValueMIPS_OpRsh16Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux16 <t> x y)
 	// cond:
 	// result: (CMOVZ (SRL <t> (ZeroExt16to32 x) (ZeroExt16to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt16to32 y)))
@@ -6918,19 +6918,19 @@ func rewriteValueMIPS_OpRsh16Ux16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v3.AuxInt = 0
 		v.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v4.AuxInt = 32
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -6940,8 +6940,8 @@ func rewriteValueMIPS_OpRsh16Ux16(v *Value) bool {
 func rewriteValueMIPS_OpRsh16Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux32 <t> x y)
 	// cond:
 	// result: (CMOVZ (SRL <t> (ZeroExt16to32 x) y) (MOVWconst [0]) (SGTUconst [32] y))
@@ -6951,15 +6951,15 @@ func rewriteValueMIPS_OpRsh16Ux32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
 		v3.AddArg(y)
 		v.AddArg(v3)
@@ -6969,11 +6969,11 @@ func rewriteValueMIPS_OpRsh16Ux32(v *Value) bool {
 func rewriteValueMIPS_OpRsh16Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux64 x (Const64 [c]))
 	// cond: uint32(c) < 16
-	// result: (SRLconst (SLLconst <fe.TypeUInt32()> x [16]) [c+16])
+	// result: (SRLconst (SLLconst <types.UInt32> x [16]) [c+16])
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -6986,7 +6986,7 @@ func rewriteValueMIPS_OpRsh16Ux64(v *Value) bool {
 		}
 		v.reset(OpMIPSSRLconst)
 		v.AuxInt = c + 16
-		v0 := b.NewValue0(v.Pos, OpMIPSSLLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSLLconst, types.UInt32)
 		v0.AuxInt = 16
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -7013,8 +7013,8 @@ func rewriteValueMIPS_OpRsh16Ux64(v *Value) bool {
 func rewriteValueMIPS_OpRsh16Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux8 <t> x y)
 	// cond:
 	// result: (CMOVZ (SRL <t> (ZeroExt16to32 x) (ZeroExt8to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt8to32 y)))
@@ -7024,19 +7024,19 @@ func rewriteValueMIPS_OpRsh16Ux8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v3.AuxInt = 0
 		v.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v4.AuxInt = 32
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -7046,28 +7046,28 @@ func rewriteValueMIPS_OpRsh16Ux8(v *Value) bool {
 func rewriteValueMIPS_OpRsh16x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x16 x y)
 	// cond:
-	// result: (SRA (SignExt16to32 x) ( CMOVZ <fe.TypeUInt32()> (ZeroExt16to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt16to32 y))))
+	// result: (SRA (SignExt16to32 x) ( CMOVZ <types.UInt32> (ZeroExt16to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt16to32 y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSCMOVZ, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSCMOVZ, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v3.AuxInt = -1
 		v1.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v4.AuxInt = 32
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -7078,24 +7078,24 @@ func rewriteValueMIPS_OpRsh16x16(v *Value) bool {
 func rewriteValueMIPS_OpRsh16x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x32 x y)
 	// cond:
-	// result: (SRA (SignExt16to32 x) ( CMOVZ <fe.TypeUInt32()> y (MOVWconst [-1]) (SGTUconst [32] y)))
+	// result: (SRA (SignExt16to32 x) ( CMOVZ <types.UInt32> y (MOVWconst [-1]) (SGTUconst [32] y)))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSCMOVZ, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSCMOVZ, types.UInt32)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = -1
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
 		v3.AddArg(y)
 		v1.AddArg(v3)
@@ -7106,11 +7106,11 @@ func rewriteValueMIPS_OpRsh16x32(v *Value) bool {
 func rewriteValueMIPS_OpRsh16x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x64 x (Const64 [c]))
 	// cond: uint32(c) < 16
-	// result: (SRAconst (SLLconst <fe.TypeUInt32()> x [16]) [c+16])
+	// result: (SRAconst (SLLconst <types.UInt32> x [16]) [c+16])
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -7123,7 +7123,7 @@ func rewriteValueMIPS_OpRsh16x64(v *Value) bool {
 		}
 		v.reset(OpMIPSSRAconst)
 		v.AuxInt = c + 16
-		v0 := b.NewValue0(v.Pos, OpMIPSSLLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSLLconst, types.UInt32)
 		v0.AuxInt = 16
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -7131,7 +7131,7 @@ func rewriteValueMIPS_OpRsh16x64(v *Value) bool {
 	}
 	// match: (Rsh16x64 x (Const64 [c]))
 	// cond: uint32(c) >= 16
-	// result: (SRAconst (SLLconst <fe.TypeUInt32()> x [16]) [31])
+	// result: (SRAconst (SLLconst <types.UInt32> x [16]) [31])
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -7144,7 +7144,7 @@ func rewriteValueMIPS_OpRsh16x64(v *Value) bool {
 		}
 		v.reset(OpMIPSSRAconst)
 		v.AuxInt = 31
-		v0 := b.NewValue0(v.Pos, OpMIPSSLLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSLLconst, types.UInt32)
 		v0.AuxInt = 16
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -7155,28 +7155,28 @@ func rewriteValueMIPS_OpRsh16x64(v *Value) bool {
 func rewriteValueMIPS_OpRsh16x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x8 x y)
 	// cond:
-	// result: (SRA (SignExt16to32 x) ( CMOVZ <fe.TypeUInt32()> (ZeroExt8to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt8to32 y))))
+	// result: (SRA (SignExt16to32 x) ( CMOVZ <types.UInt32> (ZeroExt8to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt8to32 y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSCMOVZ, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSCMOVZ, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v3.AuxInt = -1
 		v1.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v4.AuxInt = 32
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -7187,8 +7187,8 @@ func rewriteValueMIPS_OpRsh16x8(v *Value) bool {
 func rewriteValueMIPS_OpRsh32Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux16 <t> x y)
 	// cond:
 	// result: (CMOVZ (SRL <t> x (ZeroExt16to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt16to32 y)))
@@ -7199,16 +7199,16 @@ func rewriteValueMIPS_OpRsh32Ux16(v *Value) bool {
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSRL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -7218,8 +7218,8 @@ func rewriteValueMIPS_OpRsh32Ux16(v *Value) bool {
 func rewriteValueMIPS_OpRsh32Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux32 <t> x y)
 	// cond:
 	// result: (CMOVZ (SRL <t> x y) (MOVWconst [0]) (SGTUconst [32] y))
@@ -7232,10 +7232,10 @@ func rewriteValueMIPS_OpRsh32Ux32(v *Value) bool {
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = 0
 		v.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v2 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v2.AuxInt = 32
 		v2.AddArg(y)
 		v.AddArg(v2)
@@ -7282,8 +7282,8 @@ func rewriteValueMIPS_OpRsh32Ux64(v *Value) bool {
 func rewriteValueMIPS_OpRsh32Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux8 <t> x y)
 	// cond:
 	// result: (CMOVZ (SRL <t> x (ZeroExt8to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt8to32 y)))
@@ -7294,16 +7294,16 @@ func rewriteValueMIPS_OpRsh32Ux8(v *Value) bool {
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSRL, t)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v.AddArg(v3)
@@ -7313,26 +7313,26 @@ func rewriteValueMIPS_OpRsh32Ux8(v *Value) bool {
 func rewriteValueMIPS_OpRsh32x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x16 x y)
 	// cond:
-	// result: (SRA x ( CMOVZ <fe.TypeUInt32()> (ZeroExt16to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt16to32 y))))
+	// result: (SRA x ( CMOVZ <types.UInt32> (ZeroExt16to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt16to32 y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSRA)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpMIPSCMOVZ, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSCMOVZ, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = -1
 		v0.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v0.AddArg(v3)
@@ -7343,22 +7343,22 @@ func rewriteValueMIPS_OpRsh32x16(v *Value) bool {
 func rewriteValueMIPS_OpRsh32x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x32 x y)
 	// cond:
-	// result: (SRA x ( CMOVZ <fe.TypeUInt32()> y (MOVWconst [-1]) (SGTUconst [32] y)))
+	// result: (SRA x ( CMOVZ <types.UInt32> y (MOVWconst [-1]) (SGTUconst [32] y)))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSRA)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpMIPSCMOVZ, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSCMOVZ, types.UInt32)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = -1
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v2 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v2.AuxInt = 32
 		v2.AddArg(y)
 		v0.AddArg(v2)
@@ -7408,26 +7408,26 @@ func rewriteValueMIPS_OpRsh32x64(v *Value) bool {
 func rewriteValueMIPS_OpRsh32x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x8 x y)
 	// cond:
-	// result: (SRA x ( CMOVZ <fe.TypeUInt32()> (ZeroExt8to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt8to32 y))))
+	// result: (SRA x ( CMOVZ <types.UInt32> (ZeroExt8to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt8to32 y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSRA)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpMIPSCMOVZ, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSCMOVZ, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = -1
 		v0.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v0.AddArg(v3)
@@ -7438,8 +7438,8 @@ func rewriteValueMIPS_OpRsh32x8(v *Value) bool {
 func rewriteValueMIPS_OpRsh8Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux16 <t> x y)
 	// cond:
 	// result: (CMOVZ (SRL <t> (ZeroExt8to32 x) (ZeroExt16to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt16to32 y)))
@@ -7449,19 +7449,19 @@ func rewriteValueMIPS_OpRsh8Ux16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v3.AuxInt = 0
 		v.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v4.AuxInt = 32
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -7471,8 +7471,8 @@ func rewriteValueMIPS_OpRsh8Ux16(v *Value) bool {
 func rewriteValueMIPS_OpRsh8Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux32 <t> x y)
 	// cond:
 	// result: (CMOVZ (SRL <t> (ZeroExt8to32 x) y) (MOVWconst [0]) (SGTUconst [32] y))
@@ -7482,15 +7482,15 @@ func rewriteValueMIPS_OpRsh8Ux32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
 		v3.AddArg(y)
 		v.AddArg(v3)
@@ -7500,11 +7500,11 @@ func rewriteValueMIPS_OpRsh8Ux32(v *Value) bool {
 func rewriteValueMIPS_OpRsh8Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux64 x (Const64 [c]))
 	// cond: uint32(c) < 8
-	// result: (SRLconst (SLLconst <fe.TypeUInt32()> x [24]) [c+24])
+	// result: (SRLconst (SLLconst <types.UInt32> x [24]) [c+24])
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -7517,7 +7517,7 @@ func rewriteValueMIPS_OpRsh8Ux64(v *Value) bool {
 		}
 		v.reset(OpMIPSSRLconst)
 		v.AuxInt = c + 24
-		v0 := b.NewValue0(v.Pos, OpMIPSSLLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSLLconst, types.UInt32)
 		v0.AuxInt = 24
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -7544,8 +7544,8 @@ func rewriteValueMIPS_OpRsh8Ux64(v *Value) bool {
 func rewriteValueMIPS_OpRsh8Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux8 <t> x y)
 	// cond:
 	// result: (CMOVZ (SRL <t> (ZeroExt8to32 x) (ZeroExt8to32 y) ) (MOVWconst [0]) (SGTUconst [32] (ZeroExt8to32 y)))
@@ -7555,19 +7555,19 @@ func rewriteValueMIPS_OpRsh8Ux8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpMIPSCMOVZ)
 		v0 := b.NewValue0(v.Pos, OpMIPSSRL, t)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v3.AuxInt = 0
 		v.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v4.AuxInt = 32
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -7577,28 +7577,28 @@ func rewriteValueMIPS_OpRsh8Ux8(v *Value) bool {
 func rewriteValueMIPS_OpRsh8x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x16 x y)
 	// cond:
-	// result: (SRA (SignExt16to32 x) ( CMOVZ <fe.TypeUInt32()> (ZeroExt16to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt16to32 y))))
+	// result: (SRA (SignExt16to32 x) ( CMOVZ <types.UInt32> (ZeroExt16to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt16to32 y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSCMOVZ, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSCMOVZ, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v3.AuxInt = -1
 		v1.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v4.AuxInt = 32
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -7609,24 +7609,24 @@ func rewriteValueMIPS_OpRsh8x16(v *Value) bool {
 func rewriteValueMIPS_OpRsh8x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x32 x y)
 	// cond:
-	// result: (SRA (SignExt16to32 x) ( CMOVZ <fe.TypeUInt32()> y (MOVWconst [-1]) (SGTUconst [32] y)))
+	// result: (SRA (SignExt16to32 x) ( CMOVZ <types.UInt32> y (MOVWconst [-1]) (SGTUconst [32] y)))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSCMOVZ, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSCMOVZ, types.UInt32)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = -1
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v3.AuxInt = 32
 		v3.AddArg(y)
 		v1.AddArg(v3)
@@ -7637,11 +7637,11 @@ func rewriteValueMIPS_OpRsh8x32(v *Value) bool {
 func rewriteValueMIPS_OpRsh8x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x64 x (Const64 [c]))
 	// cond: uint32(c) < 8
-	// result: (SRAconst (SLLconst <fe.TypeUInt32()> x [24]) [c+24])
+	// result: (SRAconst (SLLconst <types.UInt32> x [24]) [c+24])
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -7654,7 +7654,7 @@ func rewriteValueMIPS_OpRsh8x64(v *Value) bool {
 		}
 		v.reset(OpMIPSSRAconst)
 		v.AuxInt = c + 24
-		v0 := b.NewValue0(v.Pos, OpMIPSSLLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSLLconst, types.UInt32)
 		v0.AuxInt = 24
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -7662,7 +7662,7 @@ func rewriteValueMIPS_OpRsh8x64(v *Value) bool {
 	}
 	// match: (Rsh8x64 x (Const64 [c]))
 	// cond: uint32(c) >= 8
-	// result: (SRAconst (SLLconst <fe.TypeUInt32()> x [24]) [31])
+	// result: (SRAconst (SLLconst <types.UInt32> x [24]) [31])
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -7675,7 +7675,7 @@ func rewriteValueMIPS_OpRsh8x64(v *Value) bool {
 		}
 		v.reset(OpMIPSSRAconst)
 		v.AuxInt = 31
-		v0 := b.NewValue0(v.Pos, OpMIPSSLLconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSSLLconst, types.UInt32)
 		v0.AuxInt = 24
 		v0.AddArg(x)
 		v.AddArg(v0)
@@ -7686,28 +7686,28 @@ func rewriteValueMIPS_OpRsh8x64(v *Value) bool {
 func rewriteValueMIPS_OpRsh8x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x8 x y)
 	// cond:
-	// result: (SRA (SignExt16to32 x) ( CMOVZ <fe.TypeUInt32()> (ZeroExt8to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt8to32 y))))
+	// result: (SRA (SignExt16to32 x) ( CMOVZ <types.UInt32> (ZeroExt8to32 y) (MOVWconst [-1]) (SGTUconst [32] (ZeroExt8to32 y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPSSRA)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSCMOVZ, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSCMOVZ, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v3.AuxInt = -1
 		v1.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, fe.TypeBool())
+		v4 := b.NewValue0(v.Pos, OpMIPSSGTUconst, types.Bool)
 		v4.AuxInt = 32
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v1.AddArg(v4)
@@ -7718,8 +7718,8 @@ func rewriteValueMIPS_OpRsh8x8(v *Value) bool {
 func rewriteValueMIPS_OpSelect0(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Select0 (Add32carry <t> x y))
 	// cond:
 	// result: (ADD <t.FieldType(0)> x y)
@@ -7772,8 +7772,8 @@ func rewriteValueMIPS_OpSelect0(v *Value) bool {
 			break
 		}
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPSMULTU, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMULTU, MakeTuple(types.UInt32, types.UInt32))
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = c
 		v0.AddArg(v1)
 		v0.AddArg(x)
@@ -7839,7 +7839,7 @@ func rewriteValueMIPS_OpSelect0(v *Value) bool {
 		v0.AuxInt = -1
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = 0
 		v.AddArg(v1)
 		v.AddArg(x)
@@ -7938,11 +7938,11 @@ func rewriteValueMIPS_OpSelect0(v *Value) bool {
 func rewriteValueMIPS_OpSelect1(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Select1 (Add32carry <t> x y))
 	// cond:
-	// result: (SGTU <fe.TypeBool()> x (ADD <t.FieldType(0)> x y))
+	// result: (SGTU <types.Bool> x (ADD <t.FieldType(0)> x y))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpAdd32carry {
@@ -7952,7 +7952,7 @@ func rewriteValueMIPS_OpSelect1(v *Value) bool {
 		x := v_0.Args[0]
 		y := v_0.Args[1]
 		v.reset(OpMIPSSGTU)
-		v.Type = fe.TypeBool()
+		v.Type = types.Bool
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Pos, OpMIPSADD, t.FieldType(0))
 		v0.AddArg(x)
@@ -7962,7 +7962,7 @@ func rewriteValueMIPS_OpSelect1(v *Value) bool {
 	}
 	// match: (Select1 (Sub32carry <t> x y))
 	// cond:
-	// result: (SGTU <fe.TypeBool()> (SUB <t.FieldType(0)> x y) x)
+	// result: (SGTU <types.Bool> (SUB <t.FieldType(0)> x y) x)
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpSub32carry {
@@ -7972,7 +7972,7 @@ func rewriteValueMIPS_OpSelect1(v *Value) bool {
 		x := v_0.Args[0]
 		y := v_0.Args[1]
 		v.reset(OpMIPSSGTU)
-		v.Type = fe.TypeBool()
+		v.Type = types.Bool
 		v0 := b.NewValue0(v.Pos, OpMIPSSUB, t.FieldType(0))
 		v0.AddArg(x)
 		v0.AddArg(y)
@@ -7998,8 +7998,8 @@ func rewriteValueMIPS_OpSelect1(v *Value) bool {
 			break
 		}
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPSMULTU, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMULTU, MakeTuple(types.UInt32, types.UInt32))
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = c
 		v0.AddArg(v1)
 		v0.AddArg(x)
@@ -8511,8 +8511,8 @@ func rewriteValueMIPS_OpZero(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Zero [0] _ mem)
 	// cond:
 	// result: mem
@@ -8537,7 +8537,7 @@ func rewriteValueMIPS_OpZero(v *Value) bool {
 		mem := v.Args[1]
 		v.reset(OpMIPSMOVBstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -8558,7 +8558,7 @@ func rewriteValueMIPS_OpZero(v *Value) bool {
 		}
 		v.reset(OpMIPSMOVHstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -8576,13 +8576,13 @@ func rewriteValueMIPS_OpZero(v *Value) bool {
 		v.reset(OpMIPSMOVBstore)
 		v.AuxInt = 1
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVBstore, TypeMem)
 		v1.AuxInt = 0
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -8604,7 +8604,7 @@ func rewriteValueMIPS_OpZero(v *Value) bool {
 		}
 		v.reset(OpMIPSMOVWstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -8626,13 +8626,13 @@ func rewriteValueMIPS_OpZero(v *Value) bool {
 		v.reset(OpMIPSMOVHstore)
 		v.AuxInt = 2
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVHstore, TypeMem)
 		v1.AuxInt = 0
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -8651,25 +8651,25 @@ func rewriteValueMIPS_OpZero(v *Value) bool {
 		v.reset(OpMIPSMOVBstore)
 		v.AuxInt = 3
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVBstore, TypeMem)
 		v1.AuxInt = 2
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPSMOVBstore, TypeMem)
 		v3.AuxInt = 1
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpMIPSMOVBstore, TypeMem)
 		v5.AuxInt = 0
 		v5.AddArg(ptr)
-		v6 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v6.AuxInt = 0
 		v5.AddArg(v6)
 		v5.AddArg(mem)
@@ -8690,19 +8690,19 @@ func rewriteValueMIPS_OpZero(v *Value) bool {
 		v.reset(OpMIPSMOVBstore)
 		v.AuxInt = 2
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVBstore, TypeMem)
 		v1.AuxInt = 1
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPSMOVBstore, TypeMem)
 		v3.AuxInt = 0
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v3.AddArg(mem)
@@ -8726,19 +8726,19 @@ func rewriteValueMIPS_OpZero(v *Value) bool {
 		v.reset(OpMIPSMOVHstore)
 		v.AuxInt = 4
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVHstore, TypeMem)
 		v1.AuxInt = 2
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPSMOVHstore, TypeMem)
 		v3.AuxInt = 0
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v3.AddArg(mem)
@@ -8762,13 +8762,13 @@ func rewriteValueMIPS_OpZero(v *Value) bool {
 		v.reset(OpMIPSMOVWstore)
 		v.AuxInt = 4
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVWstore, TypeMem)
 		v1.AuxInt = 0
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -8791,19 +8791,19 @@ func rewriteValueMIPS_OpZero(v *Value) bool {
 		v.reset(OpMIPSMOVWstore)
 		v.AuxInt = 8
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVWstore, TypeMem)
 		v1.AuxInt = 4
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPSMOVWstore, TypeMem)
 		v3.AuxInt = 0
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v3.AddArg(mem)
@@ -8827,25 +8827,25 @@ func rewriteValueMIPS_OpZero(v *Value) bool {
 		v.reset(OpMIPSMOVWstore)
 		v.AuxInt = 12
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPSMOVWstore, TypeMem)
 		v1.AuxInt = 8
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPSMOVWstore, TypeMem)
 		v3.AuxInt = 4
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpMIPSMOVWstore, TypeMem)
 		v5.AuxInt = 0
 		v5.AddArg(ptr)
-		v6 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v6.AuxInt = 0
 		v5.AddArg(v6)
 		v5.AddArg(mem)
@@ -8913,17 +8913,17 @@ func rewriteValueMIPS_OpZeroExt8to32(v *Value) bool {
 func rewriteValueMIPS_OpZeromask(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Zeromask x)
 	// cond:
 	// result: (NEG (SGTU x (MOVWconst [0])))
 	for {
 		x := v.Args[0]
 		v.reset(OpMIPSNEG)
-		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, fe.TypeBool())
+		v0 := b.NewValue0(v.Pos, OpMIPSSGTU, types.Bool)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMIPSMOVWconst, types.UInt32)
 		v1.AuxInt = 0
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -8935,6 +8935,8 @@ func rewriteBlockMIPS(b *Block) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &config.Types
+	_ = types
 	switch b.Kind {
 	case BlockMIPSEQ:
 		// match: (EQ (FPFlagTrue cmp) yes no)
diff --git a/src/cmd/compile/internal/ssa/rewriteMIPS64.go b/src/cmd/compile/internal/ssa/rewriteMIPS64.go
index 862b1b5d26..e0f16a9f87 100644
--- a/src/cmd/compile/internal/ssa/rewriteMIPS64.go
+++ b/src/cmd/compile/internal/ssa/rewriteMIPS64.go
@@ -776,15 +776,15 @@ func rewriteValueMIPS64_OpClosureCall(v *Value) bool {
 func rewriteValueMIPS64_OpCom16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Com16 x)
 	// cond:
 	// result: (NOR (MOVVconst [0]) x)
 	for {
 		x := v.Args[0]
 		v.reset(OpMIPS64NOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(x)
@@ -794,15 +794,15 @@ func rewriteValueMIPS64_OpCom16(v *Value) bool {
 func rewriteValueMIPS64_OpCom32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Com32 x)
 	// cond:
 	// result: (NOR (MOVVconst [0]) x)
 	for {
 		x := v.Args[0]
 		v.reset(OpMIPS64NOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(x)
@@ -812,15 +812,15 @@ func rewriteValueMIPS64_OpCom32(v *Value) bool {
 func rewriteValueMIPS64_OpCom64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Com64 x)
 	// cond:
 	// result: (NOR (MOVVconst [0]) x)
 	for {
 		x := v.Args[0]
 		v.reset(OpMIPS64NOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(x)
@@ -830,15 +830,15 @@ func rewriteValueMIPS64_OpCom64(v *Value) bool {
 func rewriteValueMIPS64_OpCom8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Com8 x)
 	// cond:
 	// result: (NOR (MOVVconst [0]) x)
 	for {
 		x := v.Args[0]
 		v.reset(OpMIPS64NOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(x)
@@ -1058,8 +1058,8 @@ func rewriteValueMIPS64_OpCvt64to64F(v *Value) bool {
 func rewriteValueMIPS64_OpDiv16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16 x y)
 	// cond:
 	// result: (Select1 (DIVV (SignExt16to64 x) (SignExt16to64 y)))
@@ -1067,11 +1067,11 @@ func rewriteValueMIPS64_OpDiv16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(fe.TypeInt64(), fe.TypeInt64()))
-		v1 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(types.Int64, types.Int64))
+		v1 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1081,8 +1081,8 @@ func rewriteValueMIPS64_OpDiv16(v *Value) bool {
 func rewriteValueMIPS64_OpDiv16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16u x y)
 	// cond:
 	// result: (Select1 (DIVVU (ZeroExt16to64 x) (ZeroExt16to64 y)))
@@ -1090,11 +1090,11 @@ func rewriteValueMIPS64_OpDiv16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(types.UInt64, types.UInt64))
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1104,8 +1104,8 @@ func rewriteValueMIPS64_OpDiv16u(v *Value) bool {
 func rewriteValueMIPS64_OpDiv32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div32 x y)
 	// cond:
 	// result: (Select1 (DIVV (SignExt32to64 x) (SignExt32to64 y)))
@@ -1113,11 +1113,11 @@ func rewriteValueMIPS64_OpDiv32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(fe.TypeInt64(), fe.TypeInt64()))
-		v1 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(types.Int64, types.Int64))
+		v1 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1140,8 +1140,8 @@ func rewriteValueMIPS64_OpDiv32F(v *Value) bool {
 func rewriteValueMIPS64_OpDiv32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div32u x y)
 	// cond:
 	// result: (Select1 (DIVVU (ZeroExt32to64 x) (ZeroExt32to64 y)))
@@ -1149,11 +1149,11 @@ func rewriteValueMIPS64_OpDiv32u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(types.UInt64, types.UInt64))
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1163,8 +1163,8 @@ func rewriteValueMIPS64_OpDiv32u(v *Value) bool {
 func rewriteValueMIPS64_OpDiv64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div64 x y)
 	// cond:
 	// result: (Select1 (DIVV x y))
@@ -1172,7 +1172,7 @@ func rewriteValueMIPS64_OpDiv64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(fe.TypeInt64(), fe.TypeInt64()))
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(types.Int64, types.Int64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -1195,8 +1195,8 @@ func rewriteValueMIPS64_OpDiv64F(v *Value) bool {
 func rewriteValueMIPS64_OpDiv64u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div64u x y)
 	// cond:
 	// result: (Select1 (DIVVU x y))
@@ -1204,7 +1204,7 @@ func rewriteValueMIPS64_OpDiv64u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(types.UInt64, types.UInt64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -1214,8 +1214,8 @@ func rewriteValueMIPS64_OpDiv64u(v *Value) bool {
 func rewriteValueMIPS64_OpDiv8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8 x y)
 	// cond:
 	// result: (Select1 (DIVV (SignExt8to64 x) (SignExt8to64 y)))
@@ -1223,11 +1223,11 @@ func rewriteValueMIPS64_OpDiv8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(fe.TypeInt64(), fe.TypeInt64()))
-		v1 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(types.Int64, types.Int64))
+		v1 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1237,8 +1237,8 @@ func rewriteValueMIPS64_OpDiv8(v *Value) bool {
 func rewriteValueMIPS64_OpDiv8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8u x y)
 	// cond:
 	// result: (Select1 (DIVVU (ZeroExt8to64 x) (ZeroExt8to64 y)))
@@ -1246,11 +1246,11 @@ func rewriteValueMIPS64_OpDiv8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(types.UInt64, types.UInt64))
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1260,8 +1260,8 @@ func rewriteValueMIPS64_OpDiv8u(v *Value) bool {
 func rewriteValueMIPS64_OpEq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq16 x y)
 	// cond:
 	// result: (SGTU (MOVVconst [1]) (XOR (ZeroExt16to64 x) (ZeroExt16to64 y)))
@@ -1269,14 +1269,14 @@ func rewriteValueMIPS64_OpEq16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64XOR, fe.TypeUInt64())
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64XOR, types.UInt64)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v2.AddArg(x)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -1286,8 +1286,8 @@ func rewriteValueMIPS64_OpEq16(v *Value) bool {
 func rewriteValueMIPS64_OpEq32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq32 x y)
 	// cond:
 	// result: (SGTU (MOVVconst [1]) (XOR (ZeroExt32to64 x) (ZeroExt32to64 y)))
@@ -1295,14 +1295,14 @@ func rewriteValueMIPS64_OpEq32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64XOR, fe.TypeUInt64())
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64XOR, types.UInt64)
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(x)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -1329,8 +1329,8 @@ func rewriteValueMIPS64_OpEq32F(v *Value) bool {
 func rewriteValueMIPS64_OpEq64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq64 x y)
 	// cond:
 	// result: (SGTU (MOVVconst [1]) (XOR x y))
@@ -1338,10 +1338,10 @@ func rewriteValueMIPS64_OpEq64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64XOR, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64XOR, types.UInt64)
 		v1.AddArg(x)
 		v1.AddArg(y)
 		v.AddArg(v1)
@@ -1368,8 +1368,8 @@ func rewriteValueMIPS64_OpEq64F(v *Value) bool {
 func rewriteValueMIPS64_OpEq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq8 x y)
 	// cond:
 	// result: (SGTU (MOVVconst [1]) (XOR (ZeroExt8to64 x) (ZeroExt8to64 y)))
@@ -1377,14 +1377,14 @@ func rewriteValueMIPS64_OpEq8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64XOR, fe.TypeUInt64())
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64XOR, types.UInt64)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v2.AddArg(x)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -1394,19 +1394,19 @@ func rewriteValueMIPS64_OpEq8(v *Value) bool {
 func rewriteValueMIPS64_OpEqB(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (EqB x y)
 	// cond:
-	// result: (XOR (MOVVconst [1]) (XOR <fe.TypeBool()> x y))
+	// result: (XOR (MOVVconst [1]) (XOR <types.Bool> x y))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64XOR, fe.TypeBool())
+		v1 := b.NewValue0(v.Pos, OpMIPS64XOR, types.Bool)
 		v1.AddArg(x)
 		v1.AddArg(y)
 		v.AddArg(v1)
@@ -1416,8 +1416,8 @@ func rewriteValueMIPS64_OpEqB(v *Value) bool {
 func rewriteValueMIPS64_OpEqPtr(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (EqPtr x y)
 	// cond:
 	// result: (SGTU (MOVVconst [1]) (XOR x y))
@@ -1425,10 +1425,10 @@ func rewriteValueMIPS64_OpEqPtr(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64XOR, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64XOR, types.UInt64)
 		v1.AddArg(x)
 		v1.AddArg(y)
 		v.AddArg(v1)
@@ -1438,8 +1438,8 @@ func rewriteValueMIPS64_OpEqPtr(v *Value) bool {
 func rewriteValueMIPS64_OpGeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq16 x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGT (SignExt16to64 y) (SignExt16to64 x)))
@@ -1447,14 +1447,14 @@ func rewriteValueMIPS64_OpGeq16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -1464,8 +1464,8 @@ func rewriteValueMIPS64_OpGeq16(v *Value) bool {
 func rewriteValueMIPS64_OpGeq16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq16U x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGTU (ZeroExt16to64 y) (ZeroExt16to64 x)))
@@ -1473,14 +1473,14 @@ func rewriteValueMIPS64_OpGeq16U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -1490,8 +1490,8 @@ func rewriteValueMIPS64_OpGeq16U(v *Value) bool {
 func rewriteValueMIPS64_OpGeq32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq32 x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGT (SignExt32to64 y) (SignExt32to64 x)))
@@ -1499,14 +1499,14 @@ func rewriteValueMIPS64_OpGeq32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -1533,8 +1533,8 @@ func rewriteValueMIPS64_OpGeq32F(v *Value) bool {
 func rewriteValueMIPS64_OpGeq32U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq32U x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGTU (ZeroExt32to64 y) (ZeroExt32to64 x)))
@@ -1542,14 +1542,14 @@ func rewriteValueMIPS64_OpGeq32U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -1559,8 +1559,8 @@ func rewriteValueMIPS64_OpGeq32U(v *Value) bool {
 func rewriteValueMIPS64_OpGeq64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq64 x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGT y x))
@@ -1568,10 +1568,10 @@ func rewriteValueMIPS64_OpGeq64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, fe.TypeBool())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, types.Bool)
 		v1.AddArg(y)
 		v1.AddArg(x)
 		v.AddArg(v1)
@@ -1598,8 +1598,8 @@ func rewriteValueMIPS64_OpGeq64F(v *Value) bool {
 func rewriteValueMIPS64_OpGeq64U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq64U x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGTU y x))
@@ -1607,10 +1607,10 @@ func rewriteValueMIPS64_OpGeq64U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
 		v1.AddArg(y)
 		v1.AddArg(x)
 		v.AddArg(v1)
@@ -1620,8 +1620,8 @@ func rewriteValueMIPS64_OpGeq64U(v *Value) bool {
 func rewriteValueMIPS64_OpGeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq8 x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGT (SignExt8to64 y) (SignExt8to64 x)))
@@ -1629,14 +1629,14 @@ func rewriteValueMIPS64_OpGeq8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -1646,8 +1646,8 @@ func rewriteValueMIPS64_OpGeq8(v *Value) bool {
 func rewriteValueMIPS64_OpGeq8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq8U x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGTU (ZeroExt8to64 y) (ZeroExt8to64 x)))
@@ -1655,14 +1655,14 @@ func rewriteValueMIPS64_OpGeq8U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v2.AddArg(y)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -1681,8 +1681,8 @@ func rewriteValueMIPS64_OpGetClosurePtr(v *Value) bool {
 func rewriteValueMIPS64_OpGreater16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater16 x y)
 	// cond:
 	// result: (SGT (SignExt16to64 x) (SignExt16to64 y))
@@ -1690,10 +1690,10 @@ func rewriteValueMIPS64_OpGreater16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGT)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1702,8 +1702,8 @@ func rewriteValueMIPS64_OpGreater16(v *Value) bool {
 func rewriteValueMIPS64_OpGreater16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater16U x y)
 	// cond:
 	// result: (SGTU (ZeroExt16to64 x) (ZeroExt16to64 y))
@@ -1711,10 +1711,10 @@ func rewriteValueMIPS64_OpGreater16U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1723,8 +1723,8 @@ func rewriteValueMIPS64_OpGreater16U(v *Value) bool {
 func rewriteValueMIPS64_OpGreater32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater32 x y)
 	// cond:
 	// result: (SGT (SignExt32to64 x) (SignExt32to64 y))
@@ -1732,10 +1732,10 @@ func rewriteValueMIPS64_OpGreater32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGT)
-		v0 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1761,8 +1761,8 @@ func rewriteValueMIPS64_OpGreater32F(v *Value) bool {
 func rewriteValueMIPS64_OpGreater32U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater32U x y)
 	// cond:
 	// result: (SGTU (ZeroExt32to64 x) (ZeroExt32to64 y))
@@ -1770,10 +1770,10 @@ func rewriteValueMIPS64_OpGreater32U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1825,8 +1825,8 @@ func rewriteValueMIPS64_OpGreater64U(v *Value) bool {
 func rewriteValueMIPS64_OpGreater8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater8 x y)
 	// cond:
 	// result: (SGT (SignExt8to64 x) (SignExt8to64 y))
@@ -1834,10 +1834,10 @@ func rewriteValueMIPS64_OpGreater8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGT)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1846,8 +1846,8 @@ func rewriteValueMIPS64_OpGreater8(v *Value) bool {
 func rewriteValueMIPS64_OpGreater8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater8U x y)
 	// cond:
 	// result: (SGTU (ZeroExt8to64 x) (ZeroExt8to64 y))
@@ -1855,10 +1855,10 @@ func rewriteValueMIPS64_OpGreater8U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1867,22 +1867,22 @@ func rewriteValueMIPS64_OpGreater8U(v *Value) bool {
 func rewriteValueMIPS64_OpHmul32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Hmul32 x y)
 	// cond:
-	// result: (SRAVconst (Select1 <fe.TypeInt64()> (MULV (SignExt32to64 x) (SignExt32to64 y))) [32])
+	// result: (SRAVconst (Select1 <types.Int64> (MULV (SignExt32to64 x) (SignExt32to64 y))) [32])
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAVconst)
 		v.AuxInt = 32
-		v0 := b.NewValue0(v.Pos, OpSelect1, fe.TypeInt64())
-		v1 := b.NewValue0(v.Pos, OpMIPS64MULV, MakeTuple(fe.TypeInt64(), fe.TypeInt64()))
-		v2 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSelect1, types.Int64)
+		v1 := b.NewValue0(v.Pos, OpMIPS64MULV, MakeTuple(types.Int64, types.Int64))
+		v2 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v2.AddArg(x)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
@@ -1893,22 +1893,22 @@ func rewriteValueMIPS64_OpHmul32(v *Value) bool {
 func rewriteValueMIPS64_OpHmul32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Hmul32u x y)
 	// cond:
-	// result: (SRLVconst (Select1 <fe.TypeUInt64()> (MULVU (ZeroExt32to64 x) (ZeroExt32to64 y))) [32])
+	// result: (SRLVconst (Select1 <types.UInt64> (MULVU (ZeroExt32to64 x) (ZeroExt32to64 y))) [32])
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRLVconst)
 		v.AuxInt = 32
-		v0 := b.NewValue0(v.Pos, OpSelect1, fe.TypeUInt64())
-		v1 := b.NewValue0(v.Pos, OpMIPS64MULVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpSelect1, types.UInt64)
+		v1 := b.NewValue0(v.Pos, OpMIPS64MULVU, MakeTuple(types.UInt64, types.UInt64))
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(x)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
@@ -1919,8 +1919,8 @@ func rewriteValueMIPS64_OpHmul32u(v *Value) bool {
 func rewriteValueMIPS64_OpHmul64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Hmul64 x y)
 	// cond:
 	// result: (Select0 (MULV x y))
@@ -1928,7 +1928,7 @@ func rewriteValueMIPS64_OpHmul64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MULV, MakeTuple(fe.TypeInt64(), fe.TypeInt64()))
+		v0 := b.NewValue0(v.Pos, OpMIPS64MULV, MakeTuple(types.Int64, types.Int64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -1938,8 +1938,8 @@ func rewriteValueMIPS64_OpHmul64(v *Value) bool {
 func rewriteValueMIPS64_OpHmul64u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Hmul64u x y)
 	// cond:
 	// result: (Select0 (MULVU x y))
@@ -1947,7 +1947,7 @@ func rewriteValueMIPS64_OpHmul64u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MULVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
+		v0 := b.NewValue0(v.Pos, OpMIPS64MULVU, MakeTuple(types.UInt64, types.UInt64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -1985,8 +1985,8 @@ func rewriteValueMIPS64_OpIsInBounds(v *Value) bool {
 func rewriteValueMIPS64_OpIsNonNil(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (IsNonNil ptr)
 	// cond:
 	// result: (SGTU ptr (MOVVconst [0]))
@@ -1994,7 +1994,7 @@ func rewriteValueMIPS64_OpIsNonNil(v *Value) bool {
 		ptr := v.Args[0]
 		v.reset(OpMIPS64SGTU)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		return true
@@ -2003,8 +2003,8 @@ func rewriteValueMIPS64_OpIsNonNil(v *Value) bool {
 func rewriteValueMIPS64_OpIsSliceInBounds(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (IsSliceInBounds idx len)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGTU idx len))
@@ -2012,10 +2012,10 @@ func rewriteValueMIPS64_OpIsSliceInBounds(v *Value) bool {
 		idx := v.Args[0]
 		len := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
 		v1.AddArg(idx)
 		v1.AddArg(len)
 		v.AddArg(v1)
@@ -2025,8 +2025,8 @@ func rewriteValueMIPS64_OpIsSliceInBounds(v *Value) bool {
 func rewriteValueMIPS64_OpLeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq16 x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGT (SignExt16to64 x) (SignExt16to64 y)))
@@ -2034,14 +2034,14 @@ func rewriteValueMIPS64_OpLeq16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v2.AddArg(x)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -2051,8 +2051,8 @@ func rewriteValueMIPS64_OpLeq16(v *Value) bool {
 func rewriteValueMIPS64_OpLeq16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq16U x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGTU (ZeroExt16to64 x) (ZeroExt16to64 y)))
@@ -2060,14 +2060,14 @@ func rewriteValueMIPS64_OpLeq16U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v2.AddArg(x)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -2077,8 +2077,8 @@ func rewriteValueMIPS64_OpLeq16U(v *Value) bool {
 func rewriteValueMIPS64_OpLeq32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq32 x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGT (SignExt32to64 x) (SignExt32to64 y)))
@@ -2086,14 +2086,14 @@ func rewriteValueMIPS64_OpLeq32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v2.AddArg(x)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -2120,8 +2120,8 @@ func rewriteValueMIPS64_OpLeq32F(v *Value) bool {
 func rewriteValueMIPS64_OpLeq32U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq32U x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGTU (ZeroExt32to64 x) (ZeroExt32to64 y)))
@@ -2129,14 +2129,14 @@ func rewriteValueMIPS64_OpLeq32U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(x)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -2146,8 +2146,8 @@ func rewriteValueMIPS64_OpLeq32U(v *Value) bool {
 func rewriteValueMIPS64_OpLeq64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq64 x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGT x y))
@@ -2155,10 +2155,10 @@ func rewriteValueMIPS64_OpLeq64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, fe.TypeBool())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, types.Bool)
 		v1.AddArg(x)
 		v1.AddArg(y)
 		v.AddArg(v1)
@@ -2185,8 +2185,8 @@ func rewriteValueMIPS64_OpLeq64F(v *Value) bool {
 func rewriteValueMIPS64_OpLeq64U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq64U x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGTU x y))
@@ -2194,10 +2194,10 @@ func rewriteValueMIPS64_OpLeq64U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
 		v1.AddArg(x)
 		v1.AddArg(y)
 		v.AddArg(v1)
@@ -2207,8 +2207,8 @@ func rewriteValueMIPS64_OpLeq64U(v *Value) bool {
 func rewriteValueMIPS64_OpLeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq8 x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGT (SignExt8to64 x) (SignExt8to64 y)))
@@ -2216,14 +2216,14 @@ func rewriteValueMIPS64_OpLeq8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGT, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v2.AddArg(x)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -2233,8 +2233,8 @@ func rewriteValueMIPS64_OpLeq8(v *Value) bool {
 func rewriteValueMIPS64_OpLeq8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq8U x y)
 	// cond:
 	// result: (XOR (MOVVconst [1]) (SGTU (ZeroExt8to64 x) (ZeroExt8to64 y)))
@@ -2242,14 +2242,14 @@ func rewriteValueMIPS64_OpLeq8U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64XOR)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 1
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v2.AddArg(x)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v.AddArg(v1)
@@ -2259,8 +2259,8 @@ func rewriteValueMIPS64_OpLeq8U(v *Value) bool {
 func rewriteValueMIPS64_OpLess16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less16 x y)
 	// cond:
 	// result: (SGT (SignExt16to64 y) (SignExt16to64 x))
@@ -2268,10 +2268,10 @@ func rewriteValueMIPS64_OpLess16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGT)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v1.AddArg(x)
 		v.AddArg(v1)
 		return true
@@ -2280,8 +2280,8 @@ func rewriteValueMIPS64_OpLess16(v *Value) bool {
 func rewriteValueMIPS64_OpLess16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less16U x y)
 	// cond:
 	// result: (SGTU (ZeroExt16to64 y) (ZeroExt16to64 x))
@@ -2289,10 +2289,10 @@ func rewriteValueMIPS64_OpLess16U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(x)
 		v.AddArg(v1)
 		return true
@@ -2301,8 +2301,8 @@ func rewriteValueMIPS64_OpLess16U(v *Value) bool {
 func rewriteValueMIPS64_OpLess32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less32 x y)
 	// cond:
 	// result: (SGT (SignExt32to64 y) (SignExt32to64 x))
@@ -2310,10 +2310,10 @@ func rewriteValueMIPS64_OpLess32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGT)
-		v0 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v1.AddArg(x)
 		v.AddArg(v1)
 		return true
@@ -2339,8 +2339,8 @@ func rewriteValueMIPS64_OpLess32F(v *Value) bool {
 func rewriteValueMIPS64_OpLess32U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less32U x y)
 	// cond:
 	// result: (SGTU (ZeroExt32to64 y) (ZeroExt32to64 x))
@@ -2348,10 +2348,10 @@ func rewriteValueMIPS64_OpLess32U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(x)
 		v.AddArg(v1)
 		return true
@@ -2403,8 +2403,8 @@ func rewriteValueMIPS64_OpLess64U(v *Value) bool {
 func rewriteValueMIPS64_OpLess8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less8 x y)
 	// cond:
 	// result: (SGT (SignExt8to64 y) (SignExt8to64 x))
@@ -2412,10 +2412,10 @@ func rewriteValueMIPS64_OpLess8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGT)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v1.AddArg(x)
 		v.AddArg(v1)
 		return true
@@ -2424,8 +2424,8 @@ func rewriteValueMIPS64_OpLess8(v *Value) bool {
 func rewriteValueMIPS64_OpLess8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less8U x y)
 	// cond:
 	// result: (SGTU (ZeroExt8to64 y) (ZeroExt8to64 x))
@@ -2433,10 +2433,10 @@ func rewriteValueMIPS64_OpLess8U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(x)
 		v.AddArg(v1)
 		return true
@@ -2598,29 +2598,29 @@ func rewriteValueMIPS64_OpLoad(v *Value) bool {
 func rewriteValueMIPS64_OpLsh16x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x16 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SLLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -2630,29 +2630,29 @@ func rewriteValueMIPS64_OpLsh16x16(v *Value) bool {
 func rewriteValueMIPS64_OpLsh16x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x32 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SLLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -2662,19 +2662,19 @@ func rewriteValueMIPS64_OpLsh16x32(v *Value) bool {
 func rewriteValueMIPS64_OpLsh16x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x64 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SLLV <t> x y))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SLLV <t> x y))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
 		v1.AddArg(y)
@@ -2690,29 +2690,29 @@ func rewriteValueMIPS64_OpLsh16x64(v *Value) bool {
 func rewriteValueMIPS64_OpLsh16x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x8  <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SLLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -2722,29 +2722,29 @@ func rewriteValueMIPS64_OpLsh16x8(v *Value) bool {
 func rewriteValueMIPS64_OpLsh32x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x16 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SLLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -2754,29 +2754,29 @@ func rewriteValueMIPS64_OpLsh32x16(v *Value) bool {
 func rewriteValueMIPS64_OpLsh32x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x32 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SLLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -2786,19 +2786,19 @@ func rewriteValueMIPS64_OpLsh32x32(v *Value) bool {
 func rewriteValueMIPS64_OpLsh32x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x64 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SLLV <t> x y))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SLLV <t> x y))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
 		v1.AddArg(y)
@@ -2814,29 +2814,29 @@ func rewriteValueMIPS64_OpLsh32x64(v *Value) bool {
 func rewriteValueMIPS64_OpLsh32x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x8  <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SLLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -2846,29 +2846,29 @@ func rewriteValueMIPS64_OpLsh32x8(v *Value) bool {
 func rewriteValueMIPS64_OpLsh64x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x16 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SLLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -2878,29 +2878,29 @@ func rewriteValueMIPS64_OpLsh64x16(v *Value) bool {
 func rewriteValueMIPS64_OpLsh64x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x32 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SLLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -2910,19 +2910,19 @@ func rewriteValueMIPS64_OpLsh64x32(v *Value) bool {
 func rewriteValueMIPS64_OpLsh64x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x64 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SLLV <t> x y))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SLLV <t> x y))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
 		v1.AddArg(y)
@@ -2938,29 +2938,29 @@ func rewriteValueMIPS64_OpLsh64x64(v *Value) bool {
 func rewriteValueMIPS64_OpLsh64x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x8  <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SLLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -2970,29 +2970,29 @@ func rewriteValueMIPS64_OpLsh64x8(v *Value) bool {
 func rewriteValueMIPS64_OpLsh8x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x16 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SLLV <t> x (ZeroExt16to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SLLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -3002,29 +3002,29 @@ func rewriteValueMIPS64_OpLsh8x16(v *Value) bool {
 func rewriteValueMIPS64_OpLsh8x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x32 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SLLV <t> x (ZeroExt32to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SLLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -3034,19 +3034,19 @@ func rewriteValueMIPS64_OpLsh8x32(v *Value) bool {
 func rewriteValueMIPS64_OpLsh8x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x64 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SLLV <t> x y))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SLLV <t> x y))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
 		v1.AddArg(y)
@@ -3062,29 +3062,29 @@ func rewriteValueMIPS64_OpLsh8x64(v *Value) bool {
 func rewriteValueMIPS64_OpLsh8x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x8  <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SLLV <t> x (ZeroExt8to64  y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SLLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -6085,8 +6085,8 @@ func rewriteValueMIPS64_OpMIPS64XORconst(v *Value) bool {
 func rewriteValueMIPS64_OpMod16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16 x y)
 	// cond:
 	// result: (Select0 (DIVV (SignExt16to64 x) (SignExt16to64 y)))
@@ -6094,11 +6094,11 @@ func rewriteValueMIPS64_OpMod16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(fe.TypeInt64(), fe.TypeInt64()))
-		v1 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(types.Int64, types.Int64))
+		v1 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -6108,8 +6108,8 @@ func rewriteValueMIPS64_OpMod16(v *Value) bool {
 func rewriteValueMIPS64_OpMod16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16u x y)
 	// cond:
 	// result: (Select0 (DIVVU (ZeroExt16to64 x) (ZeroExt16to64 y)))
@@ -6117,11 +6117,11 @@ func rewriteValueMIPS64_OpMod16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(types.UInt64, types.UInt64))
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -6131,8 +6131,8 @@ func rewriteValueMIPS64_OpMod16u(v *Value) bool {
 func rewriteValueMIPS64_OpMod32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod32 x y)
 	// cond:
 	// result: (Select0 (DIVV (SignExt32to64 x) (SignExt32to64 y)))
@@ -6140,11 +6140,11 @@ func rewriteValueMIPS64_OpMod32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(fe.TypeInt64(), fe.TypeInt64()))
-		v1 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(types.Int64, types.Int64))
+		v1 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -6154,8 +6154,8 @@ func rewriteValueMIPS64_OpMod32(v *Value) bool {
 func rewriteValueMIPS64_OpMod32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod32u x y)
 	// cond:
 	// result: (Select0 (DIVVU (ZeroExt32to64 x) (ZeroExt32to64 y)))
@@ -6163,11 +6163,11 @@ func rewriteValueMIPS64_OpMod32u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(types.UInt64, types.UInt64))
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -6177,8 +6177,8 @@ func rewriteValueMIPS64_OpMod32u(v *Value) bool {
 func rewriteValueMIPS64_OpMod64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod64 x y)
 	// cond:
 	// result: (Select0 (DIVV x y))
@@ -6186,7 +6186,7 @@ func rewriteValueMIPS64_OpMod64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(fe.TypeInt64(), fe.TypeInt64()))
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(types.Int64, types.Int64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -6196,8 +6196,8 @@ func rewriteValueMIPS64_OpMod64(v *Value) bool {
 func rewriteValueMIPS64_OpMod64u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod64u x y)
 	// cond:
 	// result: (Select0 (DIVVU x y))
@@ -6205,7 +6205,7 @@ func rewriteValueMIPS64_OpMod64u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(types.UInt64, types.UInt64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -6215,8 +6215,8 @@ func rewriteValueMIPS64_OpMod64u(v *Value) bool {
 func rewriteValueMIPS64_OpMod8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8 x y)
 	// cond:
 	// result: (Select0 (DIVV (SignExt8to64 x) (SignExt8to64 y)))
@@ -6224,11 +6224,11 @@ func rewriteValueMIPS64_OpMod8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(fe.TypeInt64(), fe.TypeInt64()))
-		v1 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVV, MakeTuple(types.Int64, types.Int64))
+		v1 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -6238,8 +6238,8 @@ func rewriteValueMIPS64_OpMod8(v *Value) bool {
 func rewriteValueMIPS64_OpMod8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8u x y)
 	// cond:
 	// result: (Select0 (DIVVU (ZeroExt8to64 x) (ZeroExt8to64 y)))
@@ -6247,11 +6247,11 @@ func rewriteValueMIPS64_OpMod8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect0)
-		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64DIVVU, MakeTuple(types.UInt64, types.UInt64))
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -6263,8 +6263,8 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Move [0] _ _ mem)
 	// cond:
 	// result: mem
@@ -6290,7 +6290,7 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpMIPS64MOVBstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVBload, fe.TypeInt8())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVBload, types.Int8)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -6313,7 +6313,7 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		}
 		v.reset(OpMIPS64MOVHstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVHload, fe.TypeInt16())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVHload, types.Int16)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -6333,14 +6333,14 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v.reset(OpMIPS64MOVBstore)
 		v.AuxInt = 1
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVBload, fe.TypeInt8())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVBload, types.Int8)
 		v0.AuxInt = 1
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVBstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVBload, fe.TypeInt8())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVBload, types.Int8)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -6364,7 +6364,7 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		}
 		v.reset(OpMIPS64MOVWstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVWload, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVWload, types.Int32)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -6388,14 +6388,14 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v.reset(OpMIPS64MOVHstore)
 		v.AuxInt = 2
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVHload, fe.TypeInt16())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVHload, types.Int16)
 		v0.AuxInt = 2
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVHstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVHload, fe.TypeInt16())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVHload, types.Int16)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -6416,7 +6416,7 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v.reset(OpMIPS64MOVBstore)
 		v.AuxInt = 3
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVBload, fe.TypeInt8())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVBload, types.Int8)
 		v0.AuxInt = 3
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -6424,7 +6424,7 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVBstore, TypeMem)
 		v1.AuxInt = 2
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVBload, fe.TypeInt8())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVBload, types.Int8)
 		v2.AuxInt = 2
 		v2.AddArg(src)
 		v2.AddArg(mem)
@@ -6432,14 +6432,14 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v3 := b.NewValue0(v.Pos, OpMIPS64MOVBstore, TypeMem)
 		v3.AuxInt = 1
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpMIPS64MOVBload, fe.TypeInt8())
+		v4 := b.NewValue0(v.Pos, OpMIPS64MOVBload, types.Int8)
 		v4.AuxInt = 1
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpMIPS64MOVBstore, TypeMem)
 		v5.AddArg(dst)
-		v6 := b.NewValue0(v.Pos, OpMIPS64MOVBload, fe.TypeInt8())
+		v6 := b.NewValue0(v.Pos, OpMIPS64MOVBload, types.Int8)
 		v6.AddArg(src)
 		v6.AddArg(mem)
 		v5.AddArg(v6)
@@ -6465,7 +6465,7 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		}
 		v.reset(OpMIPS64MOVVstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVload, types.UInt64)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -6489,14 +6489,14 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v.reset(OpMIPS64MOVWstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVWload, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVWload, types.Int32)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVWstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVWload, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVWload, types.Int32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -6521,7 +6521,7 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v.reset(OpMIPS64MOVHstore)
 		v.AuxInt = 6
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVHload, fe.TypeInt16())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVHload, types.Int16)
 		v0.AuxInt = 6
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -6529,7 +6529,7 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVHstore, TypeMem)
 		v1.AuxInt = 4
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVHload, fe.TypeInt16())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVHload, types.Int16)
 		v2.AuxInt = 4
 		v2.AddArg(src)
 		v2.AddArg(mem)
@@ -6537,14 +6537,14 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v3 := b.NewValue0(v.Pos, OpMIPS64MOVHstore, TypeMem)
 		v3.AuxInt = 2
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpMIPS64MOVHload, fe.TypeInt16())
+		v4 := b.NewValue0(v.Pos, OpMIPS64MOVHload, types.Int16)
 		v4.AuxInt = 2
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpMIPS64MOVHstore, TypeMem)
 		v5.AddArg(dst)
-		v6 := b.NewValue0(v.Pos, OpMIPS64MOVHload, fe.TypeInt16())
+		v6 := b.NewValue0(v.Pos, OpMIPS64MOVHload, types.Int16)
 		v6.AddArg(src)
 		v6.AddArg(mem)
 		v5.AddArg(v6)
@@ -6567,7 +6567,7 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v.reset(OpMIPS64MOVBstore)
 		v.AuxInt = 2
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVBload, fe.TypeInt8())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVBload, types.Int8)
 		v0.AuxInt = 2
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -6575,14 +6575,14 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVBstore, TypeMem)
 		v1.AuxInt = 1
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVBload, fe.TypeInt8())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVBload, types.Int8)
 		v2.AuxInt = 1
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPS64MOVBstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpMIPS64MOVBload, fe.TypeInt8())
+		v4 := b.NewValue0(v.Pos, OpMIPS64MOVBload, types.Int8)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -6608,7 +6608,7 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v.reset(OpMIPS64MOVHstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVHload, fe.TypeInt16())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVHload, types.Int16)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -6616,14 +6616,14 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVHstore, TypeMem)
 		v1.AuxInt = 2
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVHload, fe.TypeInt16())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVHload, types.Int16)
 		v2.AuxInt = 2
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPS64MOVHstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpMIPS64MOVHload, fe.TypeInt16())
+		v4 := b.NewValue0(v.Pos, OpMIPS64MOVHload, types.Int16)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -6649,7 +6649,7 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v.reset(OpMIPS64MOVWstore)
 		v.AuxInt = 8
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVWload, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVWload, types.Int32)
 		v0.AuxInt = 8
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -6657,14 +6657,14 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVWstore, TypeMem)
 		v1.AuxInt = 4
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVWload, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVWload, types.Int32)
 		v2.AuxInt = 4
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPS64MOVWstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpMIPS64MOVWload, fe.TypeInt32())
+		v4 := b.NewValue0(v.Pos, OpMIPS64MOVWload, types.Int32)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -6690,14 +6690,14 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v.reset(OpMIPS64MOVVstore)
 		v.AuxInt = 8
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVload, types.UInt64)
 		v0.AuxInt = 8
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVVstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVload, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVload, types.UInt64)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -6722,7 +6722,7 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v.reset(OpMIPS64MOVVstore)
 		v.AuxInt = 16
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVload, types.UInt64)
 		v0.AuxInt = 16
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -6730,14 +6730,14 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVVstore, TypeMem)
 		v1.AuxInt = 8
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVload, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVload, types.UInt64)
 		v2.AuxInt = 8
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPS64MOVVstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVload, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVload, types.UInt64)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -6774,8 +6774,8 @@ func rewriteValueMIPS64_OpMove(v *Value) bool {
 func rewriteValueMIPS64_OpMul16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mul16 x y)
 	// cond:
 	// result: (Select1 (MULVU x y))
@@ -6783,7 +6783,7 @@ func rewriteValueMIPS64_OpMul16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MULVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
+		v0 := b.NewValue0(v.Pos, OpMIPS64MULVU, MakeTuple(types.UInt64, types.UInt64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -6793,8 +6793,8 @@ func rewriteValueMIPS64_OpMul16(v *Value) bool {
 func rewriteValueMIPS64_OpMul32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mul32 x y)
 	// cond:
 	// result: (Select1 (MULVU x y))
@@ -6802,7 +6802,7 @@ func rewriteValueMIPS64_OpMul32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MULVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
+		v0 := b.NewValue0(v.Pos, OpMIPS64MULVU, MakeTuple(types.UInt64, types.UInt64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -6825,8 +6825,8 @@ func rewriteValueMIPS64_OpMul32F(v *Value) bool {
 func rewriteValueMIPS64_OpMul64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mul64 x y)
 	// cond:
 	// result: (Select1 (MULVU x y))
@@ -6834,7 +6834,7 @@ func rewriteValueMIPS64_OpMul64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MULVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
+		v0 := b.NewValue0(v.Pos, OpMIPS64MULVU, MakeTuple(types.UInt64, types.UInt64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -6857,8 +6857,8 @@ func rewriteValueMIPS64_OpMul64F(v *Value) bool {
 func rewriteValueMIPS64_OpMul8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mul8 x y)
 	// cond:
 	// result: (Select1 (MULVU x y))
@@ -6866,7 +6866,7 @@ func rewriteValueMIPS64_OpMul8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpSelect1)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MULVU, MakeTuple(fe.TypeUInt64(), fe.TypeUInt64()))
+		v0 := b.NewValue0(v.Pos, OpMIPS64MULVU, MakeTuple(types.UInt64, types.UInt64))
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -6942,8 +6942,8 @@ func rewriteValueMIPS64_OpNeg8(v *Value) bool {
 func rewriteValueMIPS64_OpNeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq16 x y)
 	// cond:
 	// result: (SGTU (XOR (ZeroExt16to32 x) (ZeroExt16to64 y)) (MOVVconst [0]))
@@ -6951,15 +6951,15 @@ func rewriteValueMIPS64_OpNeq16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPS64XOR, fe.TypeUInt64())
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpMIPS64XOR, types.UInt64)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v3.AuxInt = 0
 		v.AddArg(v3)
 		return true
@@ -6968,8 +6968,8 @@ func rewriteValueMIPS64_OpNeq16(v *Value) bool {
 func rewriteValueMIPS64_OpNeq32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq32 x y)
 	// cond:
 	// result: (SGTU (XOR (ZeroExt32to64 x) (ZeroExt32to64 y)) (MOVVconst [0]))
@@ -6977,15 +6977,15 @@ func rewriteValueMIPS64_OpNeq32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPS64XOR, fe.TypeUInt64())
-		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64XOR, types.UInt64)
+		v1 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v3.AuxInt = 0
 		v.AddArg(v3)
 		return true
@@ -7011,8 +7011,8 @@ func rewriteValueMIPS64_OpNeq32F(v *Value) bool {
 func rewriteValueMIPS64_OpNeq64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq64 x y)
 	// cond:
 	// result: (SGTU (XOR x y) (MOVVconst [0]))
@@ -7020,11 +7020,11 @@ func rewriteValueMIPS64_OpNeq64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPS64XOR, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64XOR, types.UInt64)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v1.AuxInt = 0
 		v.AddArg(v1)
 		return true
@@ -7050,8 +7050,8 @@ func rewriteValueMIPS64_OpNeq64F(v *Value) bool {
 func rewriteValueMIPS64_OpNeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq8 x y)
 	// cond:
 	// result: (SGTU (XOR (ZeroExt8to64 x) (ZeroExt8to64 y)) (MOVVconst [0]))
@@ -7059,15 +7059,15 @@ func rewriteValueMIPS64_OpNeq8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPS64XOR, fe.TypeUInt64())
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64XOR, types.UInt64)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v3.AuxInt = 0
 		v.AddArg(v3)
 		return true
@@ -7089,8 +7089,8 @@ func rewriteValueMIPS64_OpNeqB(v *Value) bool {
 func rewriteValueMIPS64_OpNeqPtr(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (NeqPtr x y)
 	// cond:
 	// result: (SGTU (XOR x y) (MOVVconst [0]))
@@ -7098,11 +7098,11 @@ func rewriteValueMIPS64_OpNeqPtr(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SGTU)
-		v0 := b.NewValue0(v.Pos, OpMIPS64XOR, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64XOR, types.UInt64)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v1.AuxInt = 0
 		v.AddArg(v1)
 		return true
@@ -7252,31 +7252,31 @@ func rewriteValueMIPS64_OpRound64F(v *Value) bool {
 func rewriteValueMIPS64_OpRsh16Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux16 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SRLV <t> (ZeroExt16to64 x) (ZeroExt16to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SRLV <t> (ZeroExt16to64 x) (ZeroExt16to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v.AddArg(v4)
@@ -7286,31 +7286,31 @@ func rewriteValueMIPS64_OpRsh16Ux16(v *Value) bool {
 func rewriteValueMIPS64_OpRsh16Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux32 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SRLV <t> (ZeroExt16to64 x) (ZeroExt32to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SRLV <t> (ZeroExt16to64 x) (ZeroExt32to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v.AddArg(v4)
@@ -7320,26 +7320,26 @@ func rewriteValueMIPS64_OpRsh16Ux32(v *Value) bool {
 func rewriteValueMIPS64_OpRsh16Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux64 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SRLV <t> (ZeroExt16to64 x) y))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SRLV <t> (ZeroExt16to64 x) y))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v3 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(x)
 		v3.AddArg(v4)
 		v3.AddArg(y)
@@ -7350,31 +7350,31 @@ func rewriteValueMIPS64_OpRsh16Ux64(v *Value) bool {
 func rewriteValueMIPS64_OpRsh16Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux8  <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SRLV <t> (ZeroExt16to64 x) (ZeroExt8to64  y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SRLV <t> (ZeroExt16to64 x) (ZeroExt8to64  y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v.AddArg(v4)
@@ -7384,31 +7384,31 @@ func rewriteValueMIPS64_OpRsh16Ux8(v *Value) bool {
 func rewriteValueMIPS64_OpRsh16x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x16 <t> x y)
 	// cond:
-	// result: (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt16to64 y)))
+	// result: (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt16to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAV)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v2 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 63
 		v3.AddArg(v5)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v6 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v6.AddArg(y)
 		v1.AddArg(v6)
 		v.AddArg(v1)
@@ -7418,31 +7418,31 @@ func rewriteValueMIPS64_OpRsh16x16(v *Value) bool {
 func rewriteValueMIPS64_OpRsh16x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x32 <t> x y)
 	// cond:
-	// result: (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt32to64 y)))
+	// result: (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt32to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAV)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v2 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 63
 		v3.AddArg(v5)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v6 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v6.AddArg(y)
 		v1.AddArg(v6)
 		v.AddArg(v1)
@@ -7452,24 +7452,24 @@ func rewriteValueMIPS64_OpRsh16x32(v *Value) bool {
 func rewriteValueMIPS64_OpRsh16x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x64 <t> x y)
 	// cond:
-	// result: (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU y (Const64 <fe.TypeUInt64()> [63]))) y))
+	// result: (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU y (Const64 <types.UInt64> [63]))) y))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAV)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v2 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
 		v3.AddArg(y)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 63
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -7482,31 +7482,31 @@ func rewriteValueMIPS64_OpRsh16x64(v *Value) bool {
 func rewriteValueMIPS64_OpRsh16x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x8  <t> x y)
 	// cond:
-	// result: (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt8to64  y)))
+	// result: (SRAV (SignExt16to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <types.UInt64> [63]))) (ZeroExt8to64  y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAV)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v2 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 63
 		v3.AddArg(v5)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v6 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v6.AddArg(y)
 		v1.AddArg(v6)
 		v.AddArg(v1)
@@ -7516,31 +7516,31 @@ func rewriteValueMIPS64_OpRsh16x8(v *Value) bool {
 func rewriteValueMIPS64_OpRsh32Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux16 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SRLV <t> (ZeroExt32to64 x) (ZeroExt16to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SRLV <t> (ZeroExt32to64 x) (ZeroExt16to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v.AddArg(v4)
@@ -7550,31 +7550,31 @@ func rewriteValueMIPS64_OpRsh32Ux16(v *Value) bool {
 func rewriteValueMIPS64_OpRsh32Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux32 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SRLV <t> (ZeroExt32to64 x) (ZeroExt32to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SRLV <t> (ZeroExt32to64 x) (ZeroExt32to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v.AddArg(v4)
@@ -7584,26 +7584,26 @@ func rewriteValueMIPS64_OpRsh32Ux32(v *Value) bool {
 func rewriteValueMIPS64_OpRsh32Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux64 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SRLV <t> (ZeroExt32to64 x) y))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SRLV <t> (ZeroExt32to64 x) y))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v3 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(x)
 		v3.AddArg(v4)
 		v3.AddArg(y)
@@ -7614,31 +7614,31 @@ func rewriteValueMIPS64_OpRsh32Ux64(v *Value) bool {
 func rewriteValueMIPS64_OpRsh32Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux8  <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SRLV <t> (ZeroExt32to64 x) (ZeroExt8to64  y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SRLV <t> (ZeroExt32to64 x) (ZeroExt8to64  y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v.AddArg(v4)
@@ -7648,31 +7648,31 @@ func rewriteValueMIPS64_OpRsh32Ux8(v *Value) bool {
 func rewriteValueMIPS64_OpRsh32x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x16 <t> x y)
 	// cond:
-	// result: (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt16to64 y)))
+	// result: (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt16to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAV)
-		v0 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v2 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 63
 		v3.AddArg(v5)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v6 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v6.AddArg(y)
 		v1.AddArg(v6)
 		v.AddArg(v1)
@@ -7682,31 +7682,31 @@ func rewriteValueMIPS64_OpRsh32x16(v *Value) bool {
 func rewriteValueMIPS64_OpRsh32x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x32 <t> x y)
 	// cond:
-	// result: (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt32to64 y)))
+	// result: (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt32to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAV)
-		v0 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v2 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 63
 		v3.AddArg(v5)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v6 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v6.AddArg(y)
 		v1.AddArg(v6)
 		v.AddArg(v1)
@@ -7716,24 +7716,24 @@ func rewriteValueMIPS64_OpRsh32x32(v *Value) bool {
 func rewriteValueMIPS64_OpRsh32x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x64 <t> x y)
 	// cond:
-	// result: (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU y (Const64 <fe.TypeUInt64()> [63]))) y))
+	// result: (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU y (Const64 <types.UInt64> [63]))) y))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAV)
-		v0 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v2 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
 		v3.AddArg(y)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 63
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -7746,31 +7746,31 @@ func rewriteValueMIPS64_OpRsh32x64(v *Value) bool {
 func rewriteValueMIPS64_OpRsh32x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x8  <t> x y)
 	// cond:
-	// result: (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt8to64  y)))
+	// result: (SRAV (SignExt32to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <types.UInt64> [63]))) (ZeroExt8to64  y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAV)
-		v0 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v2 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 63
 		v3.AddArg(v5)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v6 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v6.AddArg(y)
 		v1.AddArg(v6)
 		v.AddArg(v1)
@@ -7780,29 +7780,29 @@ func rewriteValueMIPS64_OpRsh32x8(v *Value) bool {
 func rewriteValueMIPS64_OpRsh64Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux16 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SRLV <t> x (ZeroExt16to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SRLV <t> x (ZeroExt16to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -7812,29 +7812,29 @@ func rewriteValueMIPS64_OpRsh64Ux16(v *Value) bool {
 func rewriteValueMIPS64_OpRsh64Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux32 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SRLV <t> x (ZeroExt32to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SRLV <t> x (ZeroExt32to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -7844,19 +7844,19 @@ func rewriteValueMIPS64_OpRsh64Ux32(v *Value) bool {
 func rewriteValueMIPS64_OpRsh64Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux64 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SRLV <t> x y))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SRLV <t> x y))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
 		v1.AddArg(y)
@@ -7872,29 +7872,29 @@ func rewriteValueMIPS64_OpRsh64Ux64(v *Value) bool {
 func rewriteValueMIPS64_OpRsh64Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux8  <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SRLV <t> x (ZeroExt8to64  y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SRLV <t> x (ZeroExt8to64  y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -7904,11 +7904,11 @@ func rewriteValueMIPS64_OpRsh64Ux8(v *Value) bool {
 func rewriteValueMIPS64_OpRsh64x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x16 <t> x y)
 	// cond:
-	// result: (SRAV x (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt16to64 y)))
+	// result: (SRAV x (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt16to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -7917,16 +7917,16 @@ func rewriteValueMIPS64_OpRsh64x16(v *Value) bool {
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v1 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v2 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 63
 		v2.AddArg(v4)
 		v1.AddArg(v2)
 		v0.AddArg(v1)
-		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v5.AddArg(y)
 		v0.AddArg(v5)
 		v.AddArg(v0)
@@ -7936,11 +7936,11 @@ func rewriteValueMIPS64_OpRsh64x16(v *Value) bool {
 func rewriteValueMIPS64_OpRsh64x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x32 <t> x y)
 	// cond:
-	// result: (SRAV x (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt32to64 y)))
+	// result: (SRAV x (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt32to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -7949,16 +7949,16 @@ func rewriteValueMIPS64_OpRsh64x32(v *Value) bool {
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v1 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v2 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 63
 		v2.AddArg(v4)
 		v1.AddArg(v2)
 		v0.AddArg(v1)
-		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v5.AddArg(y)
 		v0.AddArg(v5)
 		v.AddArg(v0)
@@ -7968,11 +7968,11 @@ func rewriteValueMIPS64_OpRsh64x32(v *Value) bool {
 func rewriteValueMIPS64_OpRsh64x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x64 <t> x y)
 	// cond:
-	// result: (SRAV x (OR <t> (NEGV <t> (SGTU y (Const64 <fe.TypeUInt64()> [63]))) y))
+	// result: (SRAV x (OR <t> (NEGV <t> (SGTU y (Const64 <types.UInt64> [63]))) y))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -7981,9 +7981,9 @@ func rewriteValueMIPS64_OpRsh64x64(v *Value) bool {
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v1 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v2 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
+		v2 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
 		v2.AddArg(y)
-		v3 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v3.AuxInt = 63
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -7996,11 +7996,11 @@ func rewriteValueMIPS64_OpRsh64x64(v *Value) bool {
 func rewriteValueMIPS64_OpRsh64x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x8  <t> x y)
 	// cond:
-	// result: (SRAV x (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt8to64  y)))
+	// result: (SRAV x (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <types.UInt64> [63]))) (ZeroExt8to64  y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -8009,16 +8009,16 @@ func rewriteValueMIPS64_OpRsh64x8(v *Value) bool {
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v1 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v2 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 63
 		v2.AddArg(v4)
 		v1.AddArg(v2)
 		v0.AddArg(v1)
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(y)
 		v0.AddArg(v5)
 		v.AddArg(v0)
@@ -8028,31 +8028,31 @@ func rewriteValueMIPS64_OpRsh64x8(v *Value) bool {
 func rewriteValueMIPS64_OpRsh8Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux16 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt16to64 y))) (SRLV <t> (ZeroExt8to64 x) (ZeroExt16to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt16to64 y))) (SRLV <t> (ZeroExt8to64 x) (ZeroExt16to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v.AddArg(v4)
@@ -8062,31 +8062,31 @@ func rewriteValueMIPS64_OpRsh8Ux16(v *Value) bool {
 func rewriteValueMIPS64_OpRsh8Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux32 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt32to64 y))) (SRLV <t> (ZeroExt8to64 x) (ZeroExt32to64 y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt32to64 y))) (SRLV <t> (ZeroExt8to64 x) (ZeroExt32to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v.AddArg(v4)
@@ -8096,26 +8096,26 @@ func rewriteValueMIPS64_OpRsh8Ux32(v *Value) bool {
 func rewriteValueMIPS64_OpRsh8Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux64 <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) y)) (SRLV <t> (ZeroExt8to64 x) y))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) y)) (SRLV <t> (ZeroExt8to64 x) y))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
 		v1.AddArg(y)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v3 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(x)
 		v3.AddArg(v4)
 		v3.AddArg(y)
@@ -8126,31 +8126,31 @@ func rewriteValueMIPS64_OpRsh8Ux64(v *Value) bool {
 func rewriteValueMIPS64_OpRsh8Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux8  <t> x y)
 	// cond:
-	// result: (AND (NEGV <t> (SGTU (Const64 <fe.TypeUInt64()> [64]) (ZeroExt8to64  y))) (SRLV <t> (ZeroExt8to64 x) (ZeroExt8to64  y)))
+	// result: (AND (NEGV <t> (SGTU (Const64 <types.UInt64> [64]) (ZeroExt8to64  y))) (SRLV <t> (ZeroExt8to64 x) (ZeroExt8to64  y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64AND)
 		v0 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 64
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpMIPS64SRLV, t)
-		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v.AddArg(v4)
@@ -8160,31 +8160,31 @@ func rewriteValueMIPS64_OpRsh8Ux8(v *Value) bool {
 func rewriteValueMIPS64_OpRsh8x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x16 <t> x y)
 	// cond:
-	// result: (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt16to64 y)))
+	// result: (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt16to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt16to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAV)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v2 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 63
 		v3.AddArg(v5)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v6 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v6.AddArg(y)
 		v1.AddArg(v6)
 		v.AddArg(v1)
@@ -8194,31 +8194,31 @@ func rewriteValueMIPS64_OpRsh8x16(v *Value) bool {
 func rewriteValueMIPS64_OpRsh8x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x32 <t> x y)
 	// cond:
-	// result: (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt32to64 y)))
+	// result: (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt32to64 y) (Const64 <types.UInt64> [63]))) (ZeroExt32to64 y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAV)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v2 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 63
 		v3.AddArg(v5)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v6 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v6.AddArg(y)
 		v1.AddArg(v6)
 		v.AddArg(v1)
@@ -8228,24 +8228,24 @@ func rewriteValueMIPS64_OpRsh8x32(v *Value) bool {
 func rewriteValueMIPS64_OpRsh8x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x64 <t> x y)
 	// cond:
-	// result: (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU y (Const64 <fe.TypeUInt64()> [63]))) y))
+	// result: (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU y (Const64 <types.UInt64> [63]))) y))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAV)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v2 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
+		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
 		v3.AddArg(y)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 63
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -8258,31 +8258,31 @@ func rewriteValueMIPS64_OpRsh8x64(v *Value) bool {
 func rewriteValueMIPS64_OpRsh8x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x8  <t> x y)
 	// cond:
-	// result: (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <fe.TypeUInt64()> [63]))) (ZeroExt8to64  y)))
+	// result: (SRAV (SignExt8to64 x) (OR <t> (NEGV <t> (SGTU (ZeroExt8to64  y) (Const64 <types.UInt64> [63]))) (ZeroExt8to64  y)))
 	for {
 		t := v.Type
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMIPS64SRAV)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to64, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64OR, t)
 		v2 := b.NewValue0(v.Pos, OpMIPS64NEGV, t)
-		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpMIPS64SGTU, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 63
 		v3.AddArg(v5)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v6 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v6.AddArg(y)
 		v1.AddArg(v6)
 		v.AddArg(v1)
@@ -9076,8 +9076,8 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Zero [0] _ mem)
 	// cond:
 	// result: mem
@@ -9102,7 +9102,7 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		mem := v.Args[1]
 		v.reset(OpMIPS64MOVBstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -9123,7 +9123,7 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		}
 		v.reset(OpMIPS64MOVHstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -9141,13 +9141,13 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		v.reset(OpMIPS64MOVBstore)
 		v.AuxInt = 1
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVBstore, TypeMem)
 		v1.AuxInt = 0
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -9169,7 +9169,7 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		}
 		v.reset(OpMIPS64MOVWstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -9191,13 +9191,13 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		v.reset(OpMIPS64MOVHstore)
 		v.AuxInt = 2
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVHstore, TypeMem)
 		v1.AuxInt = 0
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -9216,25 +9216,25 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		v.reset(OpMIPS64MOVBstore)
 		v.AuxInt = 3
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVBstore, TypeMem)
 		v1.AuxInt = 2
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPS64MOVBstore, TypeMem)
 		v3.AuxInt = 1
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpMIPS64MOVBstore, TypeMem)
 		v5.AuxInt = 0
 		v5.AddArg(ptr)
-		v6 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v6.AuxInt = 0
 		v5.AddArg(v6)
 		v5.AddArg(mem)
@@ -9258,7 +9258,7 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		}
 		v.reset(OpMIPS64MOVVstore)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -9280,13 +9280,13 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		v.reset(OpMIPS64MOVWstore)
 		v.AuxInt = 4
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVWstore, TypeMem)
 		v1.AuxInt = 0
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -9309,25 +9309,25 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		v.reset(OpMIPS64MOVHstore)
 		v.AuxInt = 6
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVHstore, TypeMem)
 		v1.AuxInt = 4
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPS64MOVHstore, TypeMem)
 		v3.AuxInt = 2
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpMIPS64MOVHstore, TypeMem)
 		v5.AuxInt = 0
 		v5.AddArg(ptr)
-		v6 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v6.AuxInt = 0
 		v5.AddArg(v6)
 		v5.AddArg(mem)
@@ -9348,19 +9348,19 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		v.reset(OpMIPS64MOVBstore)
 		v.AuxInt = 2
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVBstore, TypeMem)
 		v1.AuxInt = 1
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPS64MOVBstore, TypeMem)
 		v3.AuxInt = 0
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v3.AddArg(mem)
@@ -9384,19 +9384,19 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		v.reset(OpMIPS64MOVHstore)
 		v.AuxInt = 4
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVHstore, TypeMem)
 		v1.AuxInt = 2
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPS64MOVHstore, TypeMem)
 		v3.AuxInt = 0
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v3.AddArg(mem)
@@ -9420,19 +9420,19 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		v.reset(OpMIPS64MOVWstore)
 		v.AuxInt = 8
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVWstore, TypeMem)
 		v1.AuxInt = 4
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPS64MOVWstore, TypeMem)
 		v3.AuxInt = 0
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v3.AddArg(mem)
@@ -9456,13 +9456,13 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		v.reset(OpMIPS64MOVVstore)
 		v.AuxInt = 8
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVVstore, TypeMem)
 		v1.AuxInt = 0
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v1.AddArg(mem)
@@ -9485,19 +9485,19 @@ func rewriteValueMIPS64_OpZero(v *Value) bool {
 		v.reset(OpMIPS64MOVVstore)
 		v.AuxInt = 16
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpMIPS64MOVVstore, TypeMem)
 		v1.AuxInt = 8
 		v1.AddArg(ptr)
-		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v2.AuxInt = 0
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpMIPS64MOVVstore, TypeMem)
 		v3.AuxInt = 0
 		v3.AddArg(ptr)
-		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpMIPS64MOVVconst, types.UInt64)
 		v4.AuxInt = 0
 		v3.AddArg(v4)
 		v3.AddArg(mem)
@@ -9616,6 +9616,8 @@ func rewriteBlockMIPS64(b *Block) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &config.Types
+	_ = types
 	switch b.Kind {
 	case BlockMIPS64EQ:
 		// match: (EQ (FPFlagTrue cmp) yes no)
diff --git a/src/cmd/compile/internal/ssa/rewritePPC64.go b/src/cmd/compile/internal/ssa/rewritePPC64.go
index d1123e9a94..4f330c0b24 100644
--- a/src/cmd/compile/internal/ssa/rewritePPC64.go
+++ b/src/cmd/compile/internal/ssa/rewritePPC64.go
@@ -1143,15 +1143,15 @@ func rewriteValuePPC64_OpConvert(v *Value) bool {
 func rewriteValuePPC64_OpCvt32Fto32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Cvt32Fto32 x)
 	// cond:
 	// result: (Xf2i64 (FCTIWZ x))
 	for {
 		x := v.Args[0]
 		v.reset(OpPPC64Xf2i64)
-		v0 := b.NewValue0(v.Pos, OpPPC64FCTIWZ, fe.TypeFloat64())
+		v0 := b.NewValue0(v.Pos, OpPPC64FCTIWZ, types.Float64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -1160,15 +1160,15 @@ func rewriteValuePPC64_OpCvt32Fto32(v *Value) bool {
 func rewriteValuePPC64_OpCvt32Fto64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Cvt32Fto64 x)
 	// cond:
 	// result: (Xf2i64 (FCTIDZ x))
 	for {
 		x := v.Args[0]
 		v.reset(OpPPC64Xf2i64)
-		v0 := b.NewValue0(v.Pos, OpPPC64FCTIDZ, fe.TypeFloat64())
+		v0 := b.NewValue0(v.Pos, OpPPC64FCTIDZ, types.Float64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -1189,17 +1189,17 @@ func rewriteValuePPC64_OpCvt32Fto64F(v *Value) bool {
 func rewriteValuePPC64_OpCvt32to32F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Cvt32to32F x)
 	// cond:
 	// result: (FRSP (FCFID (Xi2f64 (SignExt32to64 x))))
 	for {
 		x := v.Args[0]
 		v.reset(OpPPC64FRSP)
-		v0 := b.NewValue0(v.Pos, OpPPC64FCFID, fe.TypeFloat64())
-		v1 := b.NewValue0(v.Pos, OpPPC64Xi2f64, fe.TypeFloat64())
-		v2 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64FCFID, types.Float64)
+		v1 := b.NewValue0(v.Pos, OpPPC64Xi2f64, types.Float64)
+		v2 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v2.AddArg(x)
 		v1.AddArg(v2)
 		v0.AddArg(v1)
@@ -1210,16 +1210,16 @@ func rewriteValuePPC64_OpCvt32to32F(v *Value) bool {
 func rewriteValuePPC64_OpCvt32to64F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Cvt32to64F x)
 	// cond:
 	// result: (FCFID (Xi2f64 (SignExt32to64 x)))
 	for {
 		x := v.Args[0]
 		v.reset(OpPPC64FCFID)
-		v0 := b.NewValue0(v.Pos, OpPPC64Xi2f64, fe.TypeFloat64())
-		v1 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64Xi2f64, types.Float64)
+		v1 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -1229,15 +1229,15 @@ func rewriteValuePPC64_OpCvt32to64F(v *Value) bool {
 func rewriteValuePPC64_OpCvt64Fto32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Cvt64Fto32 x)
 	// cond:
 	// result: (Xf2i64 (FCTIWZ x))
 	for {
 		x := v.Args[0]
 		v.reset(OpPPC64Xf2i64)
-		v0 := b.NewValue0(v.Pos, OpPPC64FCTIWZ, fe.TypeFloat64())
+		v0 := b.NewValue0(v.Pos, OpPPC64FCTIWZ, types.Float64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -1257,15 +1257,15 @@ func rewriteValuePPC64_OpCvt64Fto32F(v *Value) bool {
 func rewriteValuePPC64_OpCvt64Fto64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Cvt64Fto64 x)
 	// cond:
 	// result: (Xf2i64 (FCTIDZ x))
 	for {
 		x := v.Args[0]
 		v.reset(OpPPC64Xf2i64)
-		v0 := b.NewValue0(v.Pos, OpPPC64FCTIDZ, fe.TypeFloat64())
+		v0 := b.NewValue0(v.Pos, OpPPC64FCTIDZ, types.Float64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -1274,16 +1274,16 @@ func rewriteValuePPC64_OpCvt64Fto64(v *Value) bool {
 func rewriteValuePPC64_OpCvt64to32F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Cvt64to32F x)
 	// cond:
 	// result: (FRSP (FCFID (Xi2f64 x)))
 	for {
 		x := v.Args[0]
 		v.reset(OpPPC64FRSP)
-		v0 := b.NewValue0(v.Pos, OpPPC64FCFID, fe.TypeFloat64())
-		v1 := b.NewValue0(v.Pos, OpPPC64Xi2f64, fe.TypeFloat64())
+		v0 := b.NewValue0(v.Pos, OpPPC64FCFID, types.Float64)
+		v1 := b.NewValue0(v.Pos, OpPPC64Xi2f64, types.Float64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -1293,15 +1293,15 @@ func rewriteValuePPC64_OpCvt64to32F(v *Value) bool {
 func rewriteValuePPC64_OpCvt64to64F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Cvt64to64F x)
 	// cond:
 	// result: (FCFID (Xi2f64 x))
 	for {
 		x := v.Args[0]
 		v.reset(OpPPC64FCFID)
-		v0 := b.NewValue0(v.Pos, OpPPC64Xi2f64, fe.TypeFloat64())
+		v0 := b.NewValue0(v.Pos, OpPPC64Xi2f64, types.Float64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -1310,8 +1310,8 @@ func rewriteValuePPC64_OpCvt64to64F(v *Value) bool {
 func rewriteValuePPC64_OpDiv16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16  x y)
 	// cond:
 	// result: (DIVW  (SignExt16to32 x) (SignExt16to32 y))
@@ -1319,10 +1319,10 @@ func rewriteValuePPC64_OpDiv16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64DIVW)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1331,8 +1331,8 @@ func rewriteValuePPC64_OpDiv16(v *Value) bool {
 func rewriteValuePPC64_OpDiv16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16u x y)
 	// cond:
 	// result: (DIVWU (ZeroExt16to32 x) (ZeroExt16to32 y))
@@ -1340,10 +1340,10 @@ func rewriteValuePPC64_OpDiv16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64DIVWU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1430,8 +1430,8 @@ func rewriteValuePPC64_OpDiv64u(v *Value) bool {
 func rewriteValuePPC64_OpDiv8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8   x y)
 	// cond:
 	// result: (DIVW  (SignExt8to32 x) (SignExt8to32 y))
@@ -1439,10 +1439,10 @@ func rewriteValuePPC64_OpDiv8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64DIVW)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1451,8 +1451,8 @@ func rewriteValuePPC64_OpDiv8(v *Value) bool {
 func rewriteValuePPC64_OpDiv8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8u  x y)
 	// cond:
 	// result: (DIVWU (ZeroExt8to32 x) (ZeroExt8to32 y))
@@ -1460,10 +1460,10 @@ func rewriteValuePPC64_OpDiv8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64DIVWU)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1472,8 +1472,8 @@ func rewriteValuePPC64_OpDiv8u(v *Value) bool {
 func rewriteValuePPC64_OpEq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq16 x y)
 	// cond: isSigned(x.Type) && isSigned(y.Type)
 	// result: (Equal (CMPW (SignExt16to32 x) (SignExt16to32 y)))
@@ -1485,10 +1485,10 @@ func rewriteValuePPC64_OpEq16(v *Value) bool {
 		}
 		v.reset(OpPPC64Equal)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1502,10 +1502,10 @@ func rewriteValuePPC64_OpEq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64Equal)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1583,8 +1583,8 @@ func rewriteValuePPC64_OpEq64F(v *Value) bool {
 func rewriteValuePPC64_OpEq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq8 x y)
 	// cond: isSigned(x.Type) && isSigned(y.Type)
 	// result: (Equal (CMPW (SignExt8to32 x) (SignExt8to32 y)))
@@ -1596,10 +1596,10 @@ func rewriteValuePPC64_OpEq8(v *Value) bool {
 		}
 		v.reset(OpPPC64Equal)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1613,10 +1613,10 @@ func rewriteValuePPC64_OpEq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64Equal)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1626,8 +1626,8 @@ func rewriteValuePPC64_OpEq8(v *Value) bool {
 func rewriteValuePPC64_OpEqB(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (EqB x y)
 	// cond:
 	// result: (ANDconst [1] (EQV x y))
@@ -1636,7 +1636,7 @@ func rewriteValuePPC64_OpEqB(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64ANDconst)
 		v.AuxInt = 1
-		v0 := b.NewValue0(v.Pos, OpPPC64EQV, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64EQV, types.Int64)
 		v0.AddArg(x)
 		v0.AddArg(y)
 		v.AddArg(v0)
@@ -1663,8 +1663,8 @@ func rewriteValuePPC64_OpEqPtr(v *Value) bool {
 func rewriteValuePPC64_OpGeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq16 x y)
 	// cond:
 	// result: (GreaterEqual (CMPW (SignExt16to32 x) (SignExt16to32 y)))
@@ -1673,10 +1673,10 @@ func rewriteValuePPC64_OpGeq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64GreaterEqual)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1686,8 +1686,8 @@ func rewriteValuePPC64_OpGeq16(v *Value) bool {
 func rewriteValuePPC64_OpGeq16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq16U x y)
 	// cond:
 	// result: (GreaterEqual (CMPWU (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -1696,10 +1696,10 @@ func rewriteValuePPC64_OpGeq16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64GreaterEqual)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPWU, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1811,8 +1811,8 @@ func rewriteValuePPC64_OpGeq64U(v *Value) bool {
 func rewriteValuePPC64_OpGeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq8 x y)
 	// cond:
 	// result: (GreaterEqual (CMPW (SignExt8to32 x) (SignExt8to32 y)))
@@ -1821,10 +1821,10 @@ func rewriteValuePPC64_OpGeq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64GreaterEqual)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1834,8 +1834,8 @@ func rewriteValuePPC64_OpGeq8(v *Value) bool {
 func rewriteValuePPC64_OpGeq8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq8U x y)
 	// cond:
 	// result: (GreaterEqual (CMPWU (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -1844,10 +1844,10 @@ func rewriteValuePPC64_OpGeq8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64GreaterEqual)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPWU, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1866,8 +1866,8 @@ func rewriteValuePPC64_OpGetClosurePtr(v *Value) bool {
 func rewriteValuePPC64_OpGreater16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater16 x y)
 	// cond:
 	// result: (GreaterThan (CMPW (SignExt16to32 x) (SignExt16to32 y)))
@@ -1876,10 +1876,10 @@ func rewriteValuePPC64_OpGreater16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64GreaterThan)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -1889,8 +1889,8 @@ func rewriteValuePPC64_OpGreater16(v *Value) bool {
 func rewriteValuePPC64_OpGreater16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater16U x y)
 	// cond:
 	// result: (GreaterThan (CMPWU (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -1899,10 +1899,10 @@ func rewriteValuePPC64_OpGreater16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64GreaterThan)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPWU, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2014,8 +2014,8 @@ func rewriteValuePPC64_OpGreater64U(v *Value) bool {
 func rewriteValuePPC64_OpGreater8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater8 x y)
 	// cond:
 	// result: (GreaterThan (CMPW (SignExt8to32 x) (SignExt8to32 y)))
@@ -2024,10 +2024,10 @@ func rewriteValuePPC64_OpGreater8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64GreaterThan)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2037,8 +2037,8 @@ func rewriteValuePPC64_OpGreater8(v *Value) bool {
 func rewriteValuePPC64_OpGreater8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater8U x y)
 	// cond:
 	// result: (GreaterThan (CMPWU (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -2047,10 +2047,10 @@ func rewriteValuePPC64_OpGreater8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64GreaterThan)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPWU, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2177,8 +2177,8 @@ func rewriteValuePPC64_OpIsSliceInBounds(v *Value) bool {
 func rewriteValuePPC64_OpLeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq16 x y)
 	// cond:
 	// result: (LessEqual (CMPW (SignExt16to32 x) (SignExt16to32 y)))
@@ -2187,10 +2187,10 @@ func rewriteValuePPC64_OpLeq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64LessEqual)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2200,8 +2200,8 @@ func rewriteValuePPC64_OpLeq16(v *Value) bool {
 func rewriteValuePPC64_OpLeq16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq16U x y)
 	// cond:
 	// result: (LessEqual (CMPWU (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -2210,10 +2210,10 @@ func rewriteValuePPC64_OpLeq16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64LessEqual)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPWU, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2325,8 +2325,8 @@ func rewriteValuePPC64_OpLeq64U(v *Value) bool {
 func rewriteValuePPC64_OpLeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq8 x y)
 	// cond:
 	// result: (LessEqual (CMPW (SignExt8to32 x) (SignExt8to32 y)))
@@ -2335,10 +2335,10 @@ func rewriteValuePPC64_OpLeq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64LessEqual)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2348,8 +2348,8 @@ func rewriteValuePPC64_OpLeq8(v *Value) bool {
 func rewriteValuePPC64_OpLeq8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq8U x y)
 	// cond:
 	// result: (LessEqual (CMPWU (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -2358,10 +2358,10 @@ func rewriteValuePPC64_OpLeq8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64LessEqual)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPWU, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2371,8 +2371,8 @@ func rewriteValuePPC64_OpLeq8U(v *Value) bool {
 func rewriteValuePPC64_OpLess16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less16 x y)
 	// cond:
 	// result: (LessThan (CMPW (SignExt16to32 x) (SignExt16to32 y)))
@@ -2381,10 +2381,10 @@ func rewriteValuePPC64_OpLess16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64LessThan)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2394,8 +2394,8 @@ func rewriteValuePPC64_OpLess16(v *Value) bool {
 func rewriteValuePPC64_OpLess16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less16U x y)
 	// cond:
 	// result: (LessThan (CMPWU (ZeroExt16to32 x) (ZeroExt16to32 y)))
@@ -2404,10 +2404,10 @@ func rewriteValuePPC64_OpLess16U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64LessThan)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPWU, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2519,8 +2519,8 @@ func rewriteValuePPC64_OpLess64U(v *Value) bool {
 func rewriteValuePPC64_OpLess8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less8 x y)
 	// cond:
 	// result: (LessThan (CMPW (SignExt8to32 x) (SignExt8to32 y)))
@@ -2529,10 +2529,10 @@ func rewriteValuePPC64_OpLess8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64LessThan)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2542,8 +2542,8 @@ func rewriteValuePPC64_OpLess8(v *Value) bool {
 func rewriteValuePPC64_OpLess8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less8U x y)
 	// cond:
 	// result: (LessThan (CMPWU (ZeroExt8to32 x) (ZeroExt8to32 y)))
@@ -2552,10 +2552,10 @@ func rewriteValuePPC64_OpLess8U(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64LessThan)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPWU, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2565,8 +2565,8 @@ func rewriteValuePPC64_OpLess8U(v *Value) bool {
 func rewriteValuePPC64_OpLoad(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Load <t> ptr mem)
 	// cond: (is64BitInt(t) || isPtr(t))
 	// result: (MOVDload ptr mem)
@@ -2668,7 +2668,7 @@ func rewriteValuePPC64_OpLoad(v *Value) bool {
 			break
 		}
 		v.reset(OpPPC64MOVBreg)
-		v0 := b.NewValue0(v.Pos, OpPPC64MOVBZload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpPPC64MOVBZload, types.UInt8)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -2724,22 +2724,22 @@ func rewriteValuePPC64_OpLoad(v *Value) bool {
 func rewriteValuePPC64_OpLsh16x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x16 x y)
 	// cond:
-	// result: (SLW  x                 (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt16to64 y)))))
+	// result: (SLW  x                 (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt16to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -16
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -2751,8 +2751,8 @@ func rewriteValuePPC64_OpLsh16x16(v *Value) bool {
 func rewriteValuePPC64_OpLsh16x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x32  x (Const64 [c]))
 	// cond: uint32(c) < 16
 	// result: (SLWconst x [c])
@@ -2791,18 +2791,18 @@ func rewriteValuePPC64_OpLsh16x32(v *Value) bool {
 	}
 	// match: (Lsh16x32 x y)
 	// cond:
-	// result: (SLW  x                 (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt32to64 y)))))
+	// result: (SLW  x                 (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt32to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -16
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -2814,8 +2814,8 @@ func rewriteValuePPC64_OpLsh16x32(v *Value) bool {
 func rewriteValuePPC64_OpLsh16x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x64  x (Const64 [c]))
 	// cond: uint64(c) < 16
 	// result: (SLWconst x [c])
@@ -2870,15 +2870,15 @@ func rewriteValuePPC64_OpLsh16x64(v *Value) bool {
 	}
 	// match: (Lsh16x64 x y)
 	// cond:
-	// result: (SLW  x                 (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] y))))
+	// result: (SLW  x                 (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -16
 		v2.AddArg(y)
@@ -2891,22 +2891,22 @@ func rewriteValuePPC64_OpLsh16x64(v *Value) bool {
 func rewriteValuePPC64_OpLsh16x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x8 x y)
 	// cond:
-	// result: (SLW  x                 (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt8to64 y)))))
+	// result: (SLW  x                 (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt8to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -16
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -2918,22 +2918,22 @@ func rewriteValuePPC64_OpLsh16x8(v *Value) bool {
 func rewriteValuePPC64_OpLsh32x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x16 x y)
 	// cond:
-	// result: (SLW x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt16to64 y)))))
+	// result: (SLW x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt16to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -32
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -2945,8 +2945,8 @@ func rewriteValuePPC64_OpLsh32x16(v *Value) bool {
 func rewriteValuePPC64_OpLsh32x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x32  x (Const64 [c]))
 	// cond: uint32(c) < 32
 	// result: (SLWconst x [c])
@@ -2985,18 +2985,18 @@ func rewriteValuePPC64_OpLsh32x32(v *Value) bool {
 	}
 	// match: (Lsh32x32 x y)
 	// cond:
-	// result: (SLW x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt32to64 y)))))
+	// result: (SLW x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt32to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -32
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3008,8 +3008,8 @@ func rewriteValuePPC64_OpLsh32x32(v *Value) bool {
 func rewriteValuePPC64_OpLsh32x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x64  x (Const64 [c]))
 	// cond: uint64(c) < 32
 	// result: (SLWconst x [c])
@@ -3064,15 +3064,15 @@ func rewriteValuePPC64_OpLsh32x64(v *Value) bool {
 	}
 	// match: (Lsh32x64 x y)
 	// cond:
-	// result: (SLW  x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] y))))
+	// result: (SLW  x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -32
 		v2.AddArg(y)
@@ -3085,22 +3085,22 @@ func rewriteValuePPC64_OpLsh32x64(v *Value) bool {
 func rewriteValuePPC64_OpLsh32x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x8 x y)
 	// cond:
-	// result: (SLW x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt8to64 y)))))
+	// result: (SLW x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt8to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -32
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3112,22 +3112,22 @@ func rewriteValuePPC64_OpLsh32x8(v *Value) bool {
 func rewriteValuePPC64_OpLsh64x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x16 x y)
 	// cond:
-	// result: (SLD x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt16to64 y)))))
+	// result: (SLD x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt16to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLD)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -64
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3139,8 +3139,8 @@ func rewriteValuePPC64_OpLsh64x16(v *Value) bool {
 func rewriteValuePPC64_OpLsh64x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x32  x (Const64 [c]))
 	// cond: uint32(c) < 64
 	// result: (SLDconst x [c])
@@ -3179,18 +3179,18 @@ func rewriteValuePPC64_OpLsh64x32(v *Value) bool {
 	}
 	// match: (Lsh64x32 x y)
 	// cond:
-	// result: (SLD x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt32to64 y)))))
+	// result: (SLD x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt32to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLD)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -64
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3202,8 +3202,8 @@ func rewriteValuePPC64_OpLsh64x32(v *Value) bool {
 func rewriteValuePPC64_OpLsh64x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x64  x (Const64 [c]))
 	// cond: uint64(c) < 64
 	// result: (SLDconst x [c])
@@ -3258,15 +3258,15 @@ func rewriteValuePPC64_OpLsh64x64(v *Value) bool {
 	}
 	// match: (Lsh64x64 x y)
 	// cond:
-	// result: (SLD  x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] y))))
+	// result: (SLD  x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLD)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -64
 		v2.AddArg(y)
@@ -3279,22 +3279,22 @@ func rewriteValuePPC64_OpLsh64x64(v *Value) bool {
 func rewriteValuePPC64_OpLsh64x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x8 x y)
 	// cond:
-	// result: (SLD x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt8to64 y)))))
+	// result: (SLD x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt8to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLD)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -64
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3306,22 +3306,22 @@ func rewriteValuePPC64_OpLsh64x8(v *Value) bool {
 func rewriteValuePPC64_OpLsh8x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x16 x y)
 	// cond:
-	// result: (SLW  x                (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt16to64 y)))))
+	// result: (SLW  x                (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt16to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -8
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3333,8 +3333,8 @@ func rewriteValuePPC64_OpLsh8x16(v *Value) bool {
 func rewriteValuePPC64_OpLsh8x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x32   x (Const64 [c]))
 	// cond: uint32(c) < 8
 	// result: (SLWconst x [c])
@@ -3373,18 +3373,18 @@ func rewriteValuePPC64_OpLsh8x32(v *Value) bool {
 	}
 	// match: (Lsh8x32 x y)
 	// cond:
-	// result: (SLW  x                (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt32to64 y)))))
+	// result: (SLW  x                (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt32to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -8
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3396,8 +3396,8 @@ func rewriteValuePPC64_OpLsh8x32(v *Value) bool {
 func rewriteValuePPC64_OpLsh8x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x64   x (Const64 [c]))
 	// cond: uint64(c) < 8
 	// result: (SLWconst x [c])
@@ -3452,15 +3452,15 @@ func rewriteValuePPC64_OpLsh8x64(v *Value) bool {
 	}
 	// match: (Lsh8x64 x y)
 	// cond:
-	// result: (SLW  x                (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] y))))
+	// result: (SLW  x                (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -8
 		v2.AddArg(y)
@@ -3473,22 +3473,22 @@ func rewriteValuePPC64_OpLsh8x64(v *Value) bool {
 func rewriteValuePPC64_OpLsh8x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x8 x y)
 	// cond:
-	// result: (SLW  x                (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt8to64 y)))))
+	// result: (SLW  x                (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt8to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SLW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -8
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3500,8 +3500,8 @@ func rewriteValuePPC64_OpLsh8x8(v *Value) bool {
 func rewriteValuePPC64_OpMod16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16 x y)
 	// cond:
 	// result: (Mod32 (SignExt16to32 x) (SignExt16to32 y))
@@ -3509,10 +3509,10 @@ func rewriteValuePPC64_OpMod16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMod32)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -3521,8 +3521,8 @@ func rewriteValuePPC64_OpMod16(v *Value) bool {
 func rewriteValuePPC64_OpMod16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16u x y)
 	// cond:
 	// result: (Mod32u (ZeroExt16to32 x) (ZeroExt16to32 y))
@@ -3530,10 +3530,10 @@ func rewriteValuePPC64_OpMod16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMod32u)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -3542,8 +3542,8 @@ func rewriteValuePPC64_OpMod16u(v *Value) bool {
 func rewriteValuePPC64_OpMod32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod32 x y)
 	// cond:
 	// result: (SUB x (MULLW y (DIVW x y)))
@@ -3552,9 +3552,9 @@ func rewriteValuePPC64_OpMod32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64SUB)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64MULLW, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpPPC64MULLW, types.Int32)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64DIVW, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpPPC64DIVW, types.Int32)
 		v1.AddArg(x)
 		v1.AddArg(y)
 		v0.AddArg(v1)
@@ -3565,8 +3565,8 @@ func rewriteValuePPC64_OpMod32(v *Value) bool {
 func rewriteValuePPC64_OpMod32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod32u x y)
 	// cond:
 	// result: (SUB x (MULLW y (DIVWU x y)))
@@ -3575,9 +3575,9 @@ func rewriteValuePPC64_OpMod32u(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64SUB)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64MULLW, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpPPC64MULLW, types.Int32)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64DIVWU, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpPPC64DIVWU, types.Int32)
 		v1.AddArg(x)
 		v1.AddArg(y)
 		v0.AddArg(v1)
@@ -3588,8 +3588,8 @@ func rewriteValuePPC64_OpMod32u(v *Value) bool {
 func rewriteValuePPC64_OpMod64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod64 x y)
 	// cond:
 	// result: (SUB x (MULLD y (DIVD x y)))
@@ -3598,9 +3598,9 @@ func rewriteValuePPC64_OpMod64(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64SUB)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64MULLD, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64MULLD, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64DIVD, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64DIVD, types.Int64)
 		v1.AddArg(x)
 		v1.AddArg(y)
 		v0.AddArg(v1)
@@ -3611,8 +3611,8 @@ func rewriteValuePPC64_OpMod64(v *Value) bool {
 func rewriteValuePPC64_OpMod64u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod64u x y)
 	// cond:
 	// result: (SUB x (MULLD y (DIVDU x y)))
@@ -3621,9 +3621,9 @@ func rewriteValuePPC64_OpMod64u(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64SUB)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64MULLD, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64MULLD, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64DIVDU, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64DIVDU, types.Int64)
 		v1.AddArg(x)
 		v1.AddArg(y)
 		v0.AddArg(v1)
@@ -3634,8 +3634,8 @@ func rewriteValuePPC64_OpMod64u(v *Value) bool {
 func rewriteValuePPC64_OpMod8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8 x y)
 	// cond:
 	// result: (Mod32 (SignExt8to32 x) (SignExt8to32 y))
@@ -3643,10 +3643,10 @@ func rewriteValuePPC64_OpMod8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMod32)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -3655,8 +3655,8 @@ func rewriteValuePPC64_OpMod8(v *Value) bool {
 func rewriteValuePPC64_OpMod8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8u x y)
 	// cond:
 	// result: (Mod32u (ZeroExt8to32 x) (ZeroExt8to32 y))
@@ -3664,10 +3664,10 @@ func rewriteValuePPC64_OpMod8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpMod32u)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -3678,8 +3678,8 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Move [0] _ _ mem)
 	// cond:
 	// result: mem
@@ -3705,7 +3705,7 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpPPC64MOVBstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpPPC64MOVBZload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpPPC64MOVBZload, types.UInt8)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -3728,7 +3728,7 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		}
 		v.reset(OpPPC64MOVHstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpPPC64MOVHZload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpPPC64MOVHZload, types.UInt16)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -3748,14 +3748,14 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		v.reset(OpPPC64MOVBstore)
 		v.AuxInt = 1
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpPPC64MOVBZload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpPPC64MOVBZload, types.UInt8)
 		v0.AuxInt = 1
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpPPC64MOVBstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpPPC64MOVBZload, fe.TypeUInt8())
+		v2 := b.NewValue0(v.Pos, OpPPC64MOVBZload, types.UInt8)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -3779,7 +3779,7 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		}
 		v.reset(OpPPC64MOVWstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpPPC64MOVWload, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpPPC64MOVWload, types.Int32)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -3803,14 +3803,14 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		v.reset(OpPPC64MOVHstore)
 		v.AuxInt = 2
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpPPC64MOVHZload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpPPC64MOVHZload, types.UInt16)
 		v0.AuxInt = 2
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpPPC64MOVHstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpPPC64MOVHZload, fe.TypeUInt16())
+		v2 := b.NewValue0(v.Pos, OpPPC64MOVHZload, types.UInt16)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -3831,7 +3831,7 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		v.reset(OpPPC64MOVBstore)
 		v.AuxInt = 3
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpPPC64MOVBZload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpPPC64MOVBZload, types.UInt8)
 		v0.AuxInt = 3
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -3839,7 +3839,7 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpPPC64MOVBstore, TypeMem)
 		v1.AuxInt = 2
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpPPC64MOVBZload, fe.TypeUInt8())
+		v2 := b.NewValue0(v.Pos, OpPPC64MOVBZload, types.UInt8)
 		v2.AuxInt = 2
 		v2.AddArg(src)
 		v2.AddArg(mem)
@@ -3847,14 +3847,14 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		v3 := b.NewValue0(v.Pos, OpPPC64MOVBstore, TypeMem)
 		v3.AuxInt = 1
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpPPC64MOVBZload, fe.TypeUInt8())
+		v4 := b.NewValue0(v.Pos, OpPPC64MOVBZload, types.UInt8)
 		v4.AuxInt = 1
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpPPC64MOVBstore, TypeMem)
 		v5.AddArg(dst)
-		v6 := b.NewValue0(v.Pos, OpPPC64MOVBZload, fe.TypeUInt8())
+		v6 := b.NewValue0(v.Pos, OpPPC64MOVBZload, types.UInt8)
 		v6.AddArg(src)
 		v6.AddArg(mem)
 		v5.AddArg(v6)
@@ -3880,7 +3880,7 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		}
 		v.reset(OpPPC64MOVDstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpPPC64MOVDload, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64MOVDload, types.Int64)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -3904,14 +3904,14 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		v.reset(OpPPC64MOVWstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpPPC64MOVWZload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpPPC64MOVWZload, types.UInt32)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpPPC64MOVWstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpPPC64MOVWZload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpPPC64MOVWZload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -3936,7 +3936,7 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		v.reset(OpPPC64MOVHstore)
 		v.AuxInt = 6
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpPPC64MOVHZload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpPPC64MOVHZload, types.UInt16)
 		v0.AuxInt = 6
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -3944,7 +3944,7 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpPPC64MOVHstore, TypeMem)
 		v1.AuxInt = 4
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpPPC64MOVHZload, fe.TypeUInt16())
+		v2 := b.NewValue0(v.Pos, OpPPC64MOVHZload, types.UInt16)
 		v2.AuxInt = 4
 		v2.AddArg(src)
 		v2.AddArg(mem)
@@ -3952,14 +3952,14 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		v3 := b.NewValue0(v.Pos, OpPPC64MOVHstore, TypeMem)
 		v3.AuxInt = 2
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpPPC64MOVHZload, fe.TypeUInt16())
+		v4 := b.NewValue0(v.Pos, OpPPC64MOVHZload, types.UInt16)
 		v4.AuxInt = 2
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
 		v5 := b.NewValue0(v.Pos, OpPPC64MOVHstore, TypeMem)
 		v5.AddArg(dst)
-		v6 := b.NewValue0(v.Pos, OpPPC64MOVHZload, fe.TypeUInt16())
+		v6 := b.NewValue0(v.Pos, OpPPC64MOVHZload, types.UInt16)
 		v6.AddArg(src)
 		v6.AddArg(mem)
 		v5.AddArg(v6)
@@ -3982,7 +3982,7 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		v.reset(OpPPC64MOVBstore)
 		v.AuxInt = 2
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpPPC64MOVBZload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpPPC64MOVBZload, types.UInt8)
 		v0.AuxInt = 2
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -3990,14 +3990,14 @@ func rewriteValuePPC64_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpPPC64MOVBstore, TypeMem)
 		v1.AuxInt = 1
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpPPC64MOVBZload, fe.TypeUInt8())
+		v2 := b.NewValue0(v.Pos, OpPPC64MOVBZload, types.UInt8)
 		v2.AuxInt = 1
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpPPC64MOVBstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpPPC64MOVBZload, fe.TypeUInt8())
+		v4 := b.NewValue0(v.Pos, OpPPC64MOVBZload, types.UInt8)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -4178,8 +4178,8 @@ func rewriteValuePPC64_OpNeg8(v *Value) bool {
 func rewriteValuePPC64_OpNeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq16 x y)
 	// cond: isSigned(x.Type) && isSigned(y.Type)
 	// result: (NotEqual (CMPW (SignExt16to32 x) (SignExt16to32 y)))
@@ -4191,10 +4191,10 @@ func rewriteValuePPC64_OpNeq16(v *Value) bool {
 		}
 		v.reset(OpPPC64NotEqual)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -4208,10 +4208,10 @@ func rewriteValuePPC64_OpNeq16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64NotEqual)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -4289,8 +4289,8 @@ func rewriteValuePPC64_OpNeq64F(v *Value) bool {
 func rewriteValuePPC64_OpNeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq8 x y)
 	// cond: isSigned(x.Type) && isSigned(y.Type)
 	// result: (NotEqual (CMPW (SignExt8to32 x) (SignExt8to32 y)))
@@ -4302,10 +4302,10 @@ func rewriteValuePPC64_OpNeq8(v *Value) bool {
 		}
 		v.reset(OpPPC64NotEqual)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -4319,10 +4319,10 @@ func rewriteValuePPC64_OpNeq8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpPPC64NotEqual)
 		v0 := b.NewValue0(v.Pos, OpPPC64CMPW, TypeFlags)
-		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -4387,16 +4387,16 @@ func rewriteValuePPC64_OpNot(v *Value) bool {
 func rewriteValuePPC64_OpOffPtr(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (OffPtr [off] ptr)
 	// cond:
-	// result: (ADD (MOVDconst <fe.TypeInt64()> [off]) ptr)
+	// result: (ADD (MOVDconst <types.Int64> [off]) ptr)
 	for {
 		off := v.AuxInt
 		ptr := v.Args[0]
 		v.reset(OpPPC64ADD)
-		v0 := b.NewValue0(v.Pos, OpPPC64MOVDconst, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64MOVDconst, types.Int64)
 		v0.AuxInt = off
 		v.AddArg(v0)
 		v.AddArg(ptr)
@@ -7468,24 +7468,24 @@ func rewriteValuePPC64_OpRound64F(v *Value) bool {
 func rewriteValuePPC64_OpRsh16Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux16 x y)
 	// cond:
-	// result: (SRW  (ZeroExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt16to64 y)))))
+	// result: (SRW  (ZeroExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt16to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRW)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -16
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -7497,8 +7497,8 @@ func rewriteValuePPC64_OpRsh16Ux16(v *Value) bool {
 func rewriteValuePPC64_OpRsh16Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux32 x (Const64 [c]))
 	// cond: uint32(c) < 16
 	// result: (SRWconst (ZeroExt16to32 x) [c])
@@ -7514,7 +7514,7 @@ func rewriteValuePPC64_OpRsh16Ux32(v *Value) bool {
 		}
 		v.reset(OpPPC64SRWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -7534,27 +7534,27 @@ func rewriteValuePPC64_OpRsh16Ux32(v *Value) bool {
 		}
 		v.reset(OpPPC64SRWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh16Ux32 x y)
 	// cond:
-	// result: (SRW  (ZeroExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt32to64 y)))))
+	// result: (SRW  (ZeroExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt32to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRW)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -16
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -7566,8 +7566,8 @@ func rewriteValuePPC64_OpRsh16Ux32(v *Value) bool {
 func rewriteValuePPC64_OpRsh16Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux64 x (Const64 [c]))
 	// cond: uint64(c) < 16
 	// result: (SRWconst (ZeroExt16to32 x) [c])
@@ -7583,7 +7583,7 @@ func rewriteValuePPC64_OpRsh16Ux64(v *Value) bool {
 		}
 		v.reset(OpPPC64SRWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -7619,24 +7619,24 @@ func rewriteValuePPC64_OpRsh16Ux64(v *Value) bool {
 		}
 		v.reset(OpPPC64SRWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh16Ux64 x y)
 	// cond:
-	// result: (SRW  (ZeroExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] y))))
+	// result: (SRW  (ZeroExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRW)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -16
 		v3.AddArg(y)
@@ -7649,24 +7649,24 @@ func rewriteValuePPC64_OpRsh16Ux64(v *Value) bool {
 func rewriteValuePPC64_OpRsh16Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux8 x y)
 	// cond:
-	// result: (SRW  (ZeroExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt8to64 y)))))
+	// result: (SRW  (ZeroExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt8to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRW)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -16
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -7678,24 +7678,24 @@ func rewriteValuePPC64_OpRsh16Ux8(v *Value) bool {
 func rewriteValuePPC64_OpRsh16x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x16 x y)
 	// cond:
-	// result: (SRAW (SignExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt16to64 y)))))
+	// result: (SRAW (SignExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt16to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAW)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -16
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -7707,8 +7707,8 @@ func rewriteValuePPC64_OpRsh16x16(v *Value) bool {
 func rewriteValuePPC64_OpRsh16x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x32  x (Const64 [c]))
 	// cond: uint32(c) < 16
 	// result: (SRAWconst (SignExt16to32 x) [c])
@@ -7724,7 +7724,7 @@ func rewriteValuePPC64_OpRsh16x32(v *Value) bool {
 		}
 		v.reset(OpPPC64SRAWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -7744,27 +7744,27 @@ func rewriteValuePPC64_OpRsh16x32(v *Value) bool {
 		}
 		v.reset(OpPPC64SRAWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh16x32 x y)
 	// cond:
-	// result: (SRAW (SignExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt32to64 y)))))
+	// result: (SRAW (SignExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt32to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAW)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -16
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -7776,8 +7776,8 @@ func rewriteValuePPC64_OpRsh16x32(v *Value) bool {
 func rewriteValuePPC64_OpRsh16x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x64  x (Const64 [c]))
 	// cond: uint64(c) < 16
 	// result: (SRAWconst (SignExt16to32 x) [c])
@@ -7793,7 +7793,7 @@ func rewriteValuePPC64_OpRsh16x64(v *Value) bool {
 		}
 		v.reset(OpPPC64SRAWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -7813,7 +7813,7 @@ func rewriteValuePPC64_OpRsh16x64(v *Value) bool {
 		}
 		v.reset(OpPPC64SRAWconst)
 		v.AuxInt = 63
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -7833,24 +7833,24 @@ func rewriteValuePPC64_OpRsh16x64(v *Value) bool {
 		}
 		v.reset(OpPPC64SRAWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh16x64 x y)
 	// cond:
-	// result: (SRAW (SignExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] y))))
+	// result: (SRAW (SignExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAW)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -16
 		v3.AddArg(y)
@@ -7863,24 +7863,24 @@ func rewriteValuePPC64_OpRsh16x64(v *Value) bool {
 func rewriteValuePPC64_OpRsh16x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x8 x y)
 	// cond:
-	// result: (SRAW (SignExt16to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt8to64 y)))))
+	// result: (SRAW (SignExt16to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-16] (ZeroExt8to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAW)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -16
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -7892,22 +7892,22 @@ func rewriteValuePPC64_OpRsh16x8(v *Value) bool {
 func rewriteValuePPC64_OpRsh32Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux16 x y)
 	// cond:
-	// result: (SRW x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt16to64 y)))))
+	// result: (SRW x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt16to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -32
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -7919,8 +7919,8 @@ func rewriteValuePPC64_OpRsh32Ux16(v *Value) bool {
 func rewriteValuePPC64_OpRsh32Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux32 x (Const64 [c]))
 	// cond: uint32(c) < 32
 	// result: (SRWconst x [c])
@@ -7959,18 +7959,18 @@ func rewriteValuePPC64_OpRsh32Ux32(v *Value) bool {
 	}
 	// match: (Rsh32Ux32 x y)
 	// cond:
-	// result: (SRW x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt32to64 y)))))
+	// result: (SRW x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt32to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -32
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -7982,8 +7982,8 @@ func rewriteValuePPC64_OpRsh32Ux32(v *Value) bool {
 func rewriteValuePPC64_OpRsh32Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux64 x (Const64 [c]))
 	// cond: uint64(c) < 32
 	// result: (SRWconst x [c])
@@ -8038,15 +8038,15 @@ func rewriteValuePPC64_OpRsh32Ux64(v *Value) bool {
 	}
 	// match: (Rsh32Ux64 x y)
 	// cond:
-	// result: (SRW  x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] y))))
+	// result: (SRW  x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -32
 		v2.AddArg(y)
@@ -8059,22 +8059,22 @@ func rewriteValuePPC64_OpRsh32Ux64(v *Value) bool {
 func rewriteValuePPC64_OpRsh32Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux8 x y)
 	// cond:
-	// result: (SRW x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt8to64 y)))))
+	// result: (SRW x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt8to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -32
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -8086,22 +8086,22 @@ func rewriteValuePPC64_OpRsh32Ux8(v *Value) bool {
 func rewriteValuePPC64_OpRsh32x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x16 x y)
 	// cond:
-	// result: (SRAW x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt16to64 y)))))
+	// result: (SRAW x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt16to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -32
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -8113,8 +8113,8 @@ func rewriteValuePPC64_OpRsh32x16(v *Value) bool {
 func rewriteValuePPC64_OpRsh32x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x32  x (Const64 [c]))
 	// cond: uint32(c) < 32
 	// result: (SRAWconst x [c])
@@ -8153,18 +8153,18 @@ func rewriteValuePPC64_OpRsh32x32(v *Value) bool {
 	}
 	// match: (Rsh32x32 x y)
 	// cond:
-	// result: (SRAW x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt32to64 y)))))
+	// result: (SRAW x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt32to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -32
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -8176,8 +8176,8 @@ func rewriteValuePPC64_OpRsh32x32(v *Value) bool {
 func rewriteValuePPC64_OpRsh32x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x64  x (Const64 [c]))
 	// cond: uint64(c) < 32
 	// result: (SRAWconst x [c])
@@ -8234,15 +8234,15 @@ func rewriteValuePPC64_OpRsh32x64(v *Value) bool {
 	}
 	// match: (Rsh32x64 x y)
 	// cond:
-	// result: (SRAW x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] y))))
+	// result: (SRAW x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -32
 		v2.AddArg(y)
@@ -8255,22 +8255,22 @@ func rewriteValuePPC64_OpRsh32x64(v *Value) bool {
 func rewriteValuePPC64_OpRsh32x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x8 x y)
 	// cond:
-	// result: (SRAW x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt8to64 y)))))
+	// result: (SRAW x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-32] (ZeroExt8to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAW)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -32
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -8282,22 +8282,22 @@ func rewriteValuePPC64_OpRsh32x8(v *Value) bool {
 func rewriteValuePPC64_OpRsh64Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux16 x y)
 	// cond:
-	// result: (SRD x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt16to64 y)))))
+	// result: (SRD x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt16to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRD)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -64
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -8309,8 +8309,8 @@ func rewriteValuePPC64_OpRsh64Ux16(v *Value) bool {
 func rewriteValuePPC64_OpRsh64Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux32 x (Const64 [c]))
 	// cond: uint32(c) < 64
 	// result: (SRDconst x [c])
@@ -8349,18 +8349,18 @@ func rewriteValuePPC64_OpRsh64Ux32(v *Value) bool {
 	}
 	// match: (Rsh64Ux32 x y)
 	// cond:
-	// result: (SRD x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt32to64 y)))))
+	// result: (SRD x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt32to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRD)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -64
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -8372,8 +8372,8 @@ func rewriteValuePPC64_OpRsh64Ux32(v *Value) bool {
 func rewriteValuePPC64_OpRsh64Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux64 x (Const64 [c]))
 	// cond: uint64(c) < 64
 	// result: (SRDconst x [c])
@@ -8428,15 +8428,15 @@ func rewriteValuePPC64_OpRsh64Ux64(v *Value) bool {
 	}
 	// match: (Rsh64Ux64 x y)
 	// cond:
-	// result: (SRD  x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] y))))
+	// result: (SRD  x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRD)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -64
 		v2.AddArg(y)
@@ -8449,22 +8449,22 @@ func rewriteValuePPC64_OpRsh64Ux64(v *Value) bool {
 func rewriteValuePPC64_OpRsh64Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux8 x y)
 	// cond:
-	// result: (SRD x  (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt8to64 y)))))
+	// result: (SRD x  (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt8to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRD)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -64
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -8476,22 +8476,22 @@ func rewriteValuePPC64_OpRsh64Ux8(v *Value) bool {
 func rewriteValuePPC64_OpRsh64x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x16 x y)
 	// cond:
-	// result: (SRAD x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt16to64 y)))))
+	// result: (SRAD x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt16to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAD)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -64
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -8503,8 +8503,8 @@ func rewriteValuePPC64_OpRsh64x16(v *Value) bool {
 func rewriteValuePPC64_OpRsh64x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x32  x (Const64 [c]))
 	// cond: uint32(c) < 64
 	// result: (SRADconst x [c])
@@ -8543,18 +8543,18 @@ func rewriteValuePPC64_OpRsh64x32(v *Value) bool {
 	}
 	// match: (Rsh64x32 x y)
 	// cond:
-	// result: (SRAD x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt32to64 y)))))
+	// result: (SRAD x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt32to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAD)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -64
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -8566,8 +8566,8 @@ func rewriteValuePPC64_OpRsh64x32(v *Value) bool {
 func rewriteValuePPC64_OpRsh64x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x64  x (Const64 [c]))
 	// cond: uint64(c) < 64
 	// result: (SRADconst x [c])
@@ -8624,15 +8624,15 @@ func rewriteValuePPC64_OpRsh64x64(v *Value) bool {
 	}
 	// match: (Rsh64x64 x y)
 	// cond:
-	// result: (SRAD x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] y))))
+	// result: (SRAD x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAD)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -64
 		v2.AddArg(y)
@@ -8645,22 +8645,22 @@ func rewriteValuePPC64_OpRsh64x64(v *Value) bool {
 func rewriteValuePPC64_OpRsh64x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x8 x y)
 	// cond:
-	// result: (SRAD x (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt8to64 y)))))
+	// result: (SRAD x (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-64] (ZeroExt8to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAD)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v0.AddArg(y)
-		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v2 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v2.AuxInt = -64
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -8672,24 +8672,24 @@ func rewriteValuePPC64_OpRsh64x8(v *Value) bool {
 func rewriteValuePPC64_OpRsh8Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux16 x y)
 	// cond:
-	// result: (SRW  (ZeroExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt16to64 y)))))
+	// result: (SRW  (ZeroExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt16to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRW)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -8
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -8701,8 +8701,8 @@ func rewriteValuePPC64_OpRsh8Ux16(v *Value) bool {
 func rewriteValuePPC64_OpRsh8Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux32  x (Const64 [c]))
 	// cond: uint32(c) < 8
 	// result: (SRWconst (ZeroExt8to32  x) [c])
@@ -8718,7 +8718,7 @@ func rewriteValuePPC64_OpRsh8Ux32(v *Value) bool {
 		}
 		v.reset(OpPPC64SRWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -8738,27 +8738,27 @@ func rewriteValuePPC64_OpRsh8Ux32(v *Value) bool {
 		}
 		v.reset(OpPPC64SRWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh8Ux32 x y)
 	// cond:
-	// result: (SRW  (ZeroExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt32to64 y)))))
+	// result: (SRW  (ZeroExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt32to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRW)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -8
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -8770,8 +8770,8 @@ func rewriteValuePPC64_OpRsh8Ux32(v *Value) bool {
 func rewriteValuePPC64_OpRsh8Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux64  x (Const64 [c]))
 	// cond: uint64(c) < 8
 	// result: (SRWconst (ZeroExt8to32  x) [c])
@@ -8787,7 +8787,7 @@ func rewriteValuePPC64_OpRsh8Ux64(v *Value) bool {
 		}
 		v.reset(OpPPC64SRWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -8823,24 +8823,24 @@ func rewriteValuePPC64_OpRsh8Ux64(v *Value) bool {
 		}
 		v.reset(OpPPC64SRWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh8Ux64 x y)
 	// cond:
-	// result: (SRW  (ZeroExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] y))))
+	// result: (SRW  (ZeroExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRW)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -8
 		v3.AddArg(y)
@@ -8853,24 +8853,24 @@ func rewriteValuePPC64_OpRsh8Ux64(v *Value) bool {
 func rewriteValuePPC64_OpRsh8Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux8 x y)
 	// cond:
-	// result: (SRW  (ZeroExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt8to64 y)))))
+	// result: (SRW  (ZeroExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt8to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRW)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -8
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -8882,24 +8882,24 @@ func rewriteValuePPC64_OpRsh8Ux8(v *Value) bool {
 func rewriteValuePPC64_OpRsh8x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x16 x y)
 	// cond:
-	// result: (SRAW (SignExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt16to64 y)))))
+	// result: (SRAW (SignExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt16to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAW)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -8
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -8911,8 +8911,8 @@ func rewriteValuePPC64_OpRsh8x16(v *Value) bool {
 func rewriteValuePPC64_OpRsh8x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x32   x (Const64 [c]))
 	// cond: uint32(c) < 8
 	// result: (SRAWconst (SignExt8to32  x) [c])
@@ -8928,7 +8928,7 @@ func rewriteValuePPC64_OpRsh8x32(v *Value) bool {
 		}
 		v.reset(OpPPC64SRAWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -8948,27 +8948,27 @@ func rewriteValuePPC64_OpRsh8x32(v *Value) bool {
 		}
 		v.reset(OpPPC64SRAWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh8x32 x y)
 	// cond:
-	// result: (SRAW (SignExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt32to64 y)))))
+	// result: (SRAW (SignExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt32to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAW)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -8
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -8980,8 +8980,8 @@ func rewriteValuePPC64_OpRsh8x32(v *Value) bool {
 func rewriteValuePPC64_OpRsh8x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x64   x (Const64 [c]))
 	// cond: uint64(c) < 8
 	// result: (SRAWconst (SignExt8to32  x) [c])
@@ -8997,7 +8997,7 @@ func rewriteValuePPC64_OpRsh8x64(v *Value) bool {
 		}
 		v.reset(OpPPC64SRAWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -9017,7 +9017,7 @@ func rewriteValuePPC64_OpRsh8x64(v *Value) bool {
 		}
 		v.reset(OpPPC64SRAWconst)
 		v.AuxInt = 63
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -9037,24 +9037,24 @@ func rewriteValuePPC64_OpRsh8x64(v *Value) bool {
 		}
 		v.reset(OpPPC64SRAWconst)
 		v.AuxInt = c
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh8x64 x y)
 	// cond:
-	// result: (SRAW (SignExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] y))))
+	// result: (SRAW (SignExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAW)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -8
 		v3.AddArg(y)
@@ -9067,24 +9067,24 @@ func rewriteValuePPC64_OpRsh8x64(v *Value) bool {
 func rewriteValuePPC64_OpRsh8x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x8 x y)
 	// cond:
-	// result: (SRAW (SignExt8to32 x) (ORN y <fe.TypeInt64()> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt8to64 y)))))
+	// result: (SRAW (SignExt8to32 x) (ORN y <types.Int64> (MaskIfNotCarry (ADDconstForCarry [-8] (ZeroExt8to64 y)))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpPPC64SRAW)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpPPC64ORN, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpPPC64ORN, types.Int64)
 		v1.AddArg(y)
-		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpPPC64MaskIfNotCarry, types.Int64)
 		v3 := b.NewValue0(v.Pos, OpPPC64ADDconstForCarry, TypeFlags)
 		v3.AuxInt = -8
-		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpZeroExt8to64, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -9941,6 +9941,8 @@ func rewriteBlockPPC64(b *Block) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &config.Types
+	_ = types
 	switch b.Kind {
 	case BlockPPC64EQ:
 		// match: (EQ (CMPconst [0] (ANDconst [c] x)) yes no)
diff --git a/src/cmd/compile/internal/ssa/rewriteS390X.go b/src/cmd/compile/internal/ssa/rewriteS390X.go
index c3a1ff4fb6..e2c4547b1f 100644
--- a/src/cmd/compile/internal/ssa/rewriteS390X.go
+++ b/src/cmd/compile/internal/ssa/rewriteS390X.go
@@ -887,8 +887,8 @@ func rewriteValueS390X_OpAndB(v *Value) bool {
 func rewriteValueS390X_OpAtomicAdd32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (AtomicAdd32 ptr val mem)
 	// cond:
 	// result: (AddTupleFirst32 (LAA ptr val mem) val)
@@ -897,7 +897,7 @@ func rewriteValueS390X_OpAtomicAdd32(v *Value) bool {
 		val := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpS390XAddTupleFirst32)
-		v0 := b.NewValue0(v.Pos, OpS390XLAA, MakeTuple(fe.TypeUInt32(), TypeMem))
+		v0 := b.NewValue0(v.Pos, OpS390XLAA, MakeTuple(types.UInt32, TypeMem))
 		v0.AddArg(ptr)
 		v0.AddArg(val)
 		v0.AddArg(mem)
@@ -909,8 +909,8 @@ func rewriteValueS390X_OpAtomicAdd32(v *Value) bool {
 func rewriteValueS390X_OpAtomicAdd64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (AtomicAdd64 ptr val mem)
 	// cond:
 	// result: (AddTupleFirst64 (LAAG ptr val mem) val)
@@ -919,7 +919,7 @@ func rewriteValueS390X_OpAtomicAdd64(v *Value) bool {
 		val := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpS390XAddTupleFirst64)
-		v0 := b.NewValue0(v.Pos, OpS390XLAAG, MakeTuple(fe.TypeUInt64(), TypeMem))
+		v0 := b.NewValue0(v.Pos, OpS390XLAAG, MakeTuple(types.UInt64, TypeMem))
 		v0.AddArg(ptr)
 		v0.AddArg(val)
 		v0.AddArg(mem)
@@ -1101,18 +1101,18 @@ func rewriteValueS390X_OpAvg64u(v *Value) bool {
 func rewriteValueS390X_OpBitLen64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (BitLen64 x)
 	// cond:
 	// result: (SUB (MOVDconst [64]) (FLOGR x))
 	for {
 		x := v.Args[0]
 		v.reset(OpS390XSUB)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 64
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XFLOGR, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XFLOGR, types.UInt64)
 		v1.AddArg(x)
 		v.AddArg(v1)
 		return true
@@ -1306,8 +1306,8 @@ func rewriteValueS390X_OpConvert(v *Value) bool {
 func rewriteValueS390X_OpCtz32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Ctz32 <t> x)
 	// cond:
 	// result: (SUB (MOVDconst [64]) (FLOGR (MOVWZreg (ANDW <t> (SUBWconst <t> [1] x) (NOTW <t> x)))))
@@ -1315,11 +1315,11 @@ func rewriteValueS390X_OpCtz32(v *Value) bool {
 		t := v.Type
 		x := v.Args[0]
 		v.reset(OpS390XSUB)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 64
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XFLOGR, fe.TypeUInt64())
-		v2 := b.NewValue0(v.Pos, OpS390XMOVWZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XFLOGR, types.UInt64)
+		v2 := b.NewValue0(v.Pos, OpS390XMOVWZreg, types.UInt64)
 		v3 := b.NewValue0(v.Pos, OpS390XANDW, t)
 		v4 := b.NewValue0(v.Pos, OpS390XSUBWconst, t)
 		v4.AuxInt = 1
@@ -1337,8 +1337,8 @@ func rewriteValueS390X_OpCtz32(v *Value) bool {
 func rewriteValueS390X_OpCtz64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Ctz64 <t> x)
 	// cond:
 	// result: (SUB (MOVDconst [64]) (FLOGR (AND <t> (SUBconst <t> [1] x) (NOT <t> x))))
@@ -1346,10 +1346,10 @@ func rewriteValueS390X_OpCtz64(v *Value) bool {
 		t := v.Type
 		x := v.Args[0]
 		v.reset(OpS390XSUB)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 64
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XFLOGR, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XFLOGR, types.UInt64)
 		v2 := b.NewValue0(v.Pos, OpS390XAND, t)
 		v3 := b.NewValue0(v.Pos, OpS390XSUBconst, t)
 		v3.AuxInt = 1
@@ -1476,8 +1476,8 @@ func rewriteValueS390X_OpCvt64to64F(v *Value) bool {
 func rewriteValueS390X_OpDiv16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16  x y)
 	// cond:
 	// result: (DIVW  (MOVHreg x) (MOVHreg y))
@@ -1485,10 +1485,10 @@ func rewriteValueS390X_OpDiv16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XDIVW)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1497,8 +1497,8 @@ func rewriteValueS390X_OpDiv16(v *Value) bool {
 func rewriteValueS390X_OpDiv16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16u x y)
 	// cond:
 	// result: (DIVWU (MOVHZreg x) (MOVHZreg y))
@@ -1506,10 +1506,10 @@ func rewriteValueS390X_OpDiv16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XDIVWU)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1518,8 +1518,8 @@ func rewriteValueS390X_OpDiv16u(v *Value) bool {
 func rewriteValueS390X_OpDiv32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div32  x y)
 	// cond:
 	// result: (DIVW  (MOVWreg x) y)
@@ -1527,7 +1527,7 @@ func rewriteValueS390X_OpDiv32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XDIVW)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVWreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVWreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v.AddArg(y)
@@ -1550,8 +1550,8 @@ func rewriteValueS390X_OpDiv32F(v *Value) bool {
 func rewriteValueS390X_OpDiv32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div32u x y)
 	// cond:
 	// result: (DIVWU (MOVWZreg x) y)
@@ -1559,7 +1559,7 @@ func rewriteValueS390X_OpDiv32u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XDIVWU)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVWZreg, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVWZreg, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v.AddArg(y)
@@ -1608,8 +1608,8 @@ func rewriteValueS390X_OpDiv64u(v *Value) bool {
 func rewriteValueS390X_OpDiv8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8   x y)
 	// cond:
 	// result: (DIVW  (MOVBreg x) (MOVBreg y))
@@ -1617,10 +1617,10 @@ func rewriteValueS390X_OpDiv8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XDIVW)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1629,8 +1629,8 @@ func rewriteValueS390X_OpDiv8(v *Value) bool {
 func rewriteValueS390X_OpDiv8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8u  x y)
 	// cond:
 	// result: (DIVWU (MOVBZreg x) (MOVBZreg y))
@@ -1638,10 +1638,10 @@ func rewriteValueS390X_OpDiv8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XDIVWU)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -1650,8 +1650,8 @@ func rewriteValueS390X_OpDiv8u(v *Value) bool {
 func rewriteValueS390X_OpEq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq16  x y)
 	// cond:
 	// result: (MOVDEQ (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVHreg x) (MOVHreg y)))
@@ -1659,17 +1659,17 @@ func rewriteValueS390X_OpEq16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDEQ)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -1679,8 +1679,8 @@ func rewriteValueS390X_OpEq16(v *Value) bool {
 func rewriteValueS390X_OpEq32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq32  x y)
 	// cond:
 	// result: (MOVDEQ (MOVDconst [0]) (MOVDconst [1]) (CMPW x y))
@@ -1688,10 +1688,10 @@ func rewriteValueS390X_OpEq32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDEQ)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPW, TypeFlags)
@@ -1704,8 +1704,8 @@ func rewriteValueS390X_OpEq32(v *Value) bool {
 func rewriteValueS390X_OpEq32F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq32F x y)
 	// cond:
 	// result: (MOVDEQ (MOVDconst [0]) (MOVDconst [1]) (FCMPS x y))
@@ -1713,10 +1713,10 @@ func rewriteValueS390X_OpEq32F(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDEQ)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XFCMPS, TypeFlags)
@@ -1729,8 +1729,8 @@ func rewriteValueS390X_OpEq32F(v *Value) bool {
 func rewriteValueS390X_OpEq64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq64  x y)
 	// cond:
 	// result: (MOVDEQ (MOVDconst [0]) (MOVDconst [1]) (CMP x y))
@@ -1738,10 +1738,10 @@ func rewriteValueS390X_OpEq64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDEQ)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
@@ -1754,8 +1754,8 @@ func rewriteValueS390X_OpEq64(v *Value) bool {
 func rewriteValueS390X_OpEq64F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq64F x y)
 	// cond:
 	// result: (MOVDEQ (MOVDconst [0]) (MOVDconst [1]) (FCMP x y))
@@ -1763,10 +1763,10 @@ func rewriteValueS390X_OpEq64F(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDEQ)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XFCMP, TypeFlags)
@@ -1779,8 +1779,8 @@ func rewriteValueS390X_OpEq64F(v *Value) bool {
 func rewriteValueS390X_OpEq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq8   x y)
 	// cond:
 	// result: (MOVDEQ (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVBreg x) (MOVBreg y)))
@@ -1788,17 +1788,17 @@ func rewriteValueS390X_OpEq8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDEQ)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -1808,8 +1808,8 @@ func rewriteValueS390X_OpEq8(v *Value) bool {
 func rewriteValueS390X_OpEqB(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (EqB   x y)
 	// cond:
 	// result: (MOVDEQ (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVBreg x) (MOVBreg y)))
@@ -1817,17 +1817,17 @@ func rewriteValueS390X_OpEqB(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDEQ)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -1837,8 +1837,8 @@ func rewriteValueS390X_OpEqB(v *Value) bool {
 func rewriteValueS390X_OpEqPtr(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (EqPtr x y)
 	// cond:
 	// result: (MOVDEQ (MOVDconst [0]) (MOVDconst [1]) (CMP x y))
@@ -1846,10 +1846,10 @@ func rewriteValueS390X_OpEqPtr(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDEQ)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
@@ -1862,8 +1862,8 @@ func rewriteValueS390X_OpEqPtr(v *Value) bool {
 func rewriteValueS390X_OpGeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq16  x y)
 	// cond:
 	// result: (MOVDGE (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVHreg x) (MOVHreg y)))
@@ -1871,17 +1871,17 @@ func rewriteValueS390X_OpGeq16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -1891,8 +1891,8 @@ func rewriteValueS390X_OpGeq16(v *Value) bool {
 func rewriteValueS390X_OpGeq16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq16U x y)
 	// cond:
 	// result: (MOVDGE (MOVDconst [0]) (MOVDconst [1]) (CMPU (MOVHZreg x) (MOVHZreg y)))
@@ -1900,17 +1900,17 @@ func rewriteValueS390X_OpGeq16U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -1920,8 +1920,8 @@ func rewriteValueS390X_OpGeq16U(v *Value) bool {
 func rewriteValueS390X_OpGeq32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq32  x y)
 	// cond:
 	// result: (MOVDGE (MOVDconst [0]) (MOVDconst [1]) (CMPW x y))
@@ -1929,10 +1929,10 @@ func rewriteValueS390X_OpGeq32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPW, TypeFlags)
@@ -1945,8 +1945,8 @@ func rewriteValueS390X_OpGeq32(v *Value) bool {
 func rewriteValueS390X_OpGeq32F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq32F x y)
 	// cond:
 	// result: (MOVDGEnoinv (MOVDconst [0]) (MOVDconst [1]) (FCMPS x y))
@@ -1954,10 +1954,10 @@ func rewriteValueS390X_OpGeq32F(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGEnoinv)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XFCMPS, TypeFlags)
@@ -1970,8 +1970,8 @@ func rewriteValueS390X_OpGeq32F(v *Value) bool {
 func rewriteValueS390X_OpGeq32U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq32U x y)
 	// cond:
 	// result: (MOVDGE (MOVDconst [0]) (MOVDconst [1]) (CMPWU x y))
@@ -1979,10 +1979,10 @@ func rewriteValueS390X_OpGeq32U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWU, TypeFlags)
@@ -1995,8 +1995,8 @@ func rewriteValueS390X_OpGeq32U(v *Value) bool {
 func rewriteValueS390X_OpGeq64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq64  x y)
 	// cond:
 	// result: (MOVDGE (MOVDconst [0]) (MOVDconst [1]) (CMP x y))
@@ -2004,10 +2004,10 @@ func rewriteValueS390X_OpGeq64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
@@ -2020,8 +2020,8 @@ func rewriteValueS390X_OpGeq64(v *Value) bool {
 func rewriteValueS390X_OpGeq64F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq64F x y)
 	// cond:
 	// result: (MOVDGEnoinv (MOVDconst [0]) (MOVDconst [1]) (FCMP x y))
@@ -2029,10 +2029,10 @@ func rewriteValueS390X_OpGeq64F(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGEnoinv)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XFCMP, TypeFlags)
@@ -2045,8 +2045,8 @@ func rewriteValueS390X_OpGeq64F(v *Value) bool {
 func rewriteValueS390X_OpGeq64U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq64U x y)
 	// cond:
 	// result: (MOVDGE (MOVDconst [0]) (MOVDconst [1]) (CMPU x y))
@@ -2054,10 +2054,10 @@ func rewriteValueS390X_OpGeq64U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
@@ -2070,8 +2070,8 @@ func rewriteValueS390X_OpGeq64U(v *Value) bool {
 func rewriteValueS390X_OpGeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq8   x y)
 	// cond:
 	// result: (MOVDGE (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVBreg x) (MOVBreg y)))
@@ -2079,17 +2079,17 @@ func rewriteValueS390X_OpGeq8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -2099,8 +2099,8 @@ func rewriteValueS390X_OpGeq8(v *Value) bool {
 func rewriteValueS390X_OpGeq8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq8U  x y)
 	// cond:
 	// result: (MOVDGE (MOVDconst [0]) (MOVDconst [1]) (CMPU (MOVBZreg x) (MOVBZreg y)))
@@ -2108,17 +2108,17 @@ func rewriteValueS390X_OpGeq8U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -2148,8 +2148,8 @@ func rewriteValueS390X_OpGetG(v *Value) bool {
 func rewriteValueS390X_OpGreater16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater16  x y)
 	// cond:
 	// result: (MOVDGT (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVHreg x) (MOVHreg y)))
@@ -2157,17 +2157,17 @@ func rewriteValueS390X_OpGreater16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -2177,8 +2177,8 @@ func rewriteValueS390X_OpGreater16(v *Value) bool {
 func rewriteValueS390X_OpGreater16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater16U x y)
 	// cond:
 	// result: (MOVDGT (MOVDconst [0]) (MOVDconst [1]) (CMPU (MOVHZreg x) (MOVHZreg y)))
@@ -2186,17 +2186,17 @@ func rewriteValueS390X_OpGreater16U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -2206,8 +2206,8 @@ func rewriteValueS390X_OpGreater16U(v *Value) bool {
 func rewriteValueS390X_OpGreater32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater32  x y)
 	// cond:
 	// result: (MOVDGT (MOVDconst [0]) (MOVDconst [1]) (CMPW x y))
@@ -2215,10 +2215,10 @@ func rewriteValueS390X_OpGreater32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPW, TypeFlags)
@@ -2231,8 +2231,8 @@ func rewriteValueS390X_OpGreater32(v *Value) bool {
 func rewriteValueS390X_OpGreater32F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater32F x y)
 	// cond:
 	// result: (MOVDGTnoinv (MOVDconst [0]) (MOVDconst [1]) (FCMPS x y))
@@ -2240,10 +2240,10 @@ func rewriteValueS390X_OpGreater32F(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGTnoinv)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XFCMPS, TypeFlags)
@@ -2256,8 +2256,8 @@ func rewriteValueS390X_OpGreater32F(v *Value) bool {
 func rewriteValueS390X_OpGreater32U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater32U x y)
 	// cond:
 	// result: (MOVDGT (MOVDconst [0]) (MOVDconst [1]) (CMPWU x y))
@@ -2265,10 +2265,10 @@ func rewriteValueS390X_OpGreater32U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWU, TypeFlags)
@@ -2281,8 +2281,8 @@ func rewriteValueS390X_OpGreater32U(v *Value) bool {
 func rewriteValueS390X_OpGreater64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater64  x y)
 	// cond:
 	// result: (MOVDGT (MOVDconst [0]) (MOVDconst [1]) (CMP x y))
@@ -2290,10 +2290,10 @@ func rewriteValueS390X_OpGreater64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
@@ -2306,8 +2306,8 @@ func rewriteValueS390X_OpGreater64(v *Value) bool {
 func rewriteValueS390X_OpGreater64F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater64F x y)
 	// cond:
 	// result: (MOVDGTnoinv (MOVDconst [0]) (MOVDconst [1]) (FCMP x y))
@@ -2315,10 +2315,10 @@ func rewriteValueS390X_OpGreater64F(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGTnoinv)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XFCMP, TypeFlags)
@@ -2331,8 +2331,8 @@ func rewriteValueS390X_OpGreater64F(v *Value) bool {
 func rewriteValueS390X_OpGreater64U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater64U x y)
 	// cond:
 	// result: (MOVDGT (MOVDconst [0]) (MOVDconst [1]) (CMPU x y))
@@ -2340,10 +2340,10 @@ func rewriteValueS390X_OpGreater64U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
@@ -2356,8 +2356,8 @@ func rewriteValueS390X_OpGreater64U(v *Value) bool {
 func rewriteValueS390X_OpGreater8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater8   x y)
 	// cond:
 	// result: (MOVDGT (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVBreg x) (MOVBreg y)))
@@ -2365,17 +2365,17 @@ func rewriteValueS390X_OpGreater8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -2385,8 +2385,8 @@ func rewriteValueS390X_OpGreater8(v *Value) bool {
 func rewriteValueS390X_OpGreater8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater8U  x y)
 	// cond:
 	// result: (MOVDGT (MOVDconst [0]) (MOVDconst [1]) (CMPU (MOVBZreg x) (MOVBZreg y)))
@@ -2394,17 +2394,17 @@ func rewriteValueS390X_OpGreater8U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -2414,8 +2414,8 @@ func rewriteValueS390X_OpGreater8U(v *Value) bool {
 func rewriteValueS390X_OpHmul32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Hmul32  x y)
 	// cond:
 	// result: (SRDconst [32] (MULLD (MOVWreg x) (MOVWreg y)))
@@ -2424,11 +2424,11 @@ func rewriteValueS390X_OpHmul32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XSRDconst)
 		v.AuxInt = 32
-		v0 := b.NewValue0(v.Pos, OpS390XMULLD, fe.TypeInt64())
-		v1 := b.NewValue0(v.Pos, OpS390XMOVWreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMULLD, types.Int64)
+		v1 := b.NewValue0(v.Pos, OpS390XMOVWreg, types.Int64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpS390XMOVWreg, fe.TypeInt64())
+		v2 := b.NewValue0(v.Pos, OpS390XMOVWreg, types.Int64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2438,8 +2438,8 @@ func rewriteValueS390X_OpHmul32(v *Value) bool {
 func rewriteValueS390X_OpHmul32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Hmul32u x y)
 	// cond:
 	// result: (SRDconst [32] (MULLD (MOVWZreg x) (MOVWZreg y)))
@@ -2448,11 +2448,11 @@ func rewriteValueS390X_OpHmul32u(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XSRDconst)
 		v.AuxInt = 32
-		v0 := b.NewValue0(v.Pos, OpS390XMULLD, fe.TypeInt64())
-		v1 := b.NewValue0(v.Pos, OpS390XMOVWZreg, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMULLD, types.Int64)
+		v1 := b.NewValue0(v.Pos, OpS390XMOVWZreg, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpS390XMOVWZreg, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpS390XMOVWZreg, types.UInt64)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
@@ -2521,8 +2521,8 @@ func rewriteValueS390X_OpInterCall(v *Value) bool {
 func rewriteValueS390X_OpIsInBounds(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (IsInBounds idx len)
 	// cond:
 	// result: (MOVDLT (MOVDconst [0]) (MOVDconst [1]) (CMPU idx len))
@@ -2530,10 +2530,10 @@ func rewriteValueS390X_OpIsInBounds(v *Value) bool {
 		idx := v.Args[0]
 		len := v.Args[1]
 		v.reset(OpS390XMOVDLT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
@@ -2546,18 +2546,18 @@ func rewriteValueS390X_OpIsInBounds(v *Value) bool {
 func rewriteValueS390X_OpIsNonNil(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (IsNonNil p)
 	// cond:
 	// result: (MOVDNE (MOVDconst [0]) (MOVDconst [1]) (CMPconst p [0]))
 	for {
 		p := v.Args[0]
 		v.reset(OpS390XMOVDNE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPconst, TypeFlags)
@@ -2570,8 +2570,8 @@ func rewriteValueS390X_OpIsNonNil(v *Value) bool {
 func rewriteValueS390X_OpIsSliceInBounds(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (IsSliceInBounds idx len)
 	// cond:
 	// result: (MOVDLE (MOVDconst [0]) (MOVDconst [1]) (CMPU idx len))
@@ -2579,10 +2579,10 @@ func rewriteValueS390X_OpIsSliceInBounds(v *Value) bool {
 		idx := v.Args[0]
 		len := v.Args[1]
 		v.reset(OpS390XMOVDLE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
@@ -2595,8 +2595,8 @@ func rewriteValueS390X_OpIsSliceInBounds(v *Value) bool {
 func rewriteValueS390X_OpLeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq16  x y)
 	// cond:
 	// result: (MOVDLE (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVHreg x) (MOVHreg y)))
@@ -2604,17 +2604,17 @@ func rewriteValueS390X_OpLeq16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -2624,8 +2624,8 @@ func rewriteValueS390X_OpLeq16(v *Value) bool {
 func rewriteValueS390X_OpLeq16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq16U x y)
 	// cond:
 	// result: (MOVDLE (MOVDconst [0]) (MOVDconst [1]) (CMPU (MOVHZreg x) (MOVHZreg y)))
@@ -2633,17 +2633,17 @@ func rewriteValueS390X_OpLeq16U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -2653,8 +2653,8 @@ func rewriteValueS390X_OpLeq16U(v *Value) bool {
 func rewriteValueS390X_OpLeq32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq32  x y)
 	// cond:
 	// result: (MOVDLE (MOVDconst [0]) (MOVDconst [1]) (CMPW x y))
@@ -2662,10 +2662,10 @@ func rewriteValueS390X_OpLeq32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPW, TypeFlags)
@@ -2678,8 +2678,8 @@ func rewriteValueS390X_OpLeq32(v *Value) bool {
 func rewriteValueS390X_OpLeq32F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq32F x y)
 	// cond:
 	// result: (MOVDGEnoinv (MOVDconst [0]) (MOVDconst [1]) (FCMPS y x))
@@ -2687,10 +2687,10 @@ func rewriteValueS390X_OpLeq32F(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGEnoinv)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XFCMPS, TypeFlags)
@@ -2703,8 +2703,8 @@ func rewriteValueS390X_OpLeq32F(v *Value) bool {
 func rewriteValueS390X_OpLeq32U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq32U x y)
 	// cond:
 	// result: (MOVDLE (MOVDconst [0]) (MOVDconst [1]) (CMPWU x y))
@@ -2712,10 +2712,10 @@ func rewriteValueS390X_OpLeq32U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWU, TypeFlags)
@@ -2728,8 +2728,8 @@ func rewriteValueS390X_OpLeq32U(v *Value) bool {
 func rewriteValueS390X_OpLeq64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq64  x y)
 	// cond:
 	// result: (MOVDLE (MOVDconst [0]) (MOVDconst [1]) (CMP x y))
@@ -2737,10 +2737,10 @@ func rewriteValueS390X_OpLeq64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
@@ -2753,8 +2753,8 @@ func rewriteValueS390X_OpLeq64(v *Value) bool {
 func rewriteValueS390X_OpLeq64F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq64F x y)
 	// cond:
 	// result: (MOVDGEnoinv (MOVDconst [0]) (MOVDconst [1]) (FCMP y x))
@@ -2762,10 +2762,10 @@ func rewriteValueS390X_OpLeq64F(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGEnoinv)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XFCMP, TypeFlags)
@@ -2778,8 +2778,8 @@ func rewriteValueS390X_OpLeq64F(v *Value) bool {
 func rewriteValueS390X_OpLeq64U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq64U x y)
 	// cond:
 	// result: (MOVDLE (MOVDconst [0]) (MOVDconst [1]) (CMPU x y))
@@ -2787,10 +2787,10 @@ func rewriteValueS390X_OpLeq64U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
@@ -2803,8 +2803,8 @@ func rewriteValueS390X_OpLeq64U(v *Value) bool {
 func rewriteValueS390X_OpLeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq8   x y)
 	// cond:
 	// result: (MOVDLE (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVBreg x) (MOVBreg y)))
@@ -2812,17 +2812,17 @@ func rewriteValueS390X_OpLeq8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -2832,8 +2832,8 @@ func rewriteValueS390X_OpLeq8(v *Value) bool {
 func rewriteValueS390X_OpLeq8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq8U  x y)
 	// cond:
 	// result: (MOVDLE (MOVDconst [0]) (MOVDconst [1]) (CMPU (MOVBZreg x) (MOVBZreg y)))
@@ -2841,17 +2841,17 @@ func rewriteValueS390X_OpLeq8U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -2861,8 +2861,8 @@ func rewriteValueS390X_OpLeq8U(v *Value) bool {
 func rewriteValueS390X_OpLess16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less16  x y)
 	// cond:
 	// result: (MOVDLT (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVHreg x) (MOVHreg y)))
@@ -2870,17 +2870,17 @@ func rewriteValueS390X_OpLess16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -2890,8 +2890,8 @@ func rewriteValueS390X_OpLess16(v *Value) bool {
 func rewriteValueS390X_OpLess16U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less16U x y)
 	// cond:
 	// result: (MOVDLT (MOVDconst [0]) (MOVDconst [1]) (CMPU (MOVHZreg x) (MOVHZreg y)))
@@ -2899,17 +2899,17 @@ func rewriteValueS390X_OpLess16U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -2919,8 +2919,8 @@ func rewriteValueS390X_OpLess16U(v *Value) bool {
 func rewriteValueS390X_OpLess32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less32  x y)
 	// cond:
 	// result: (MOVDLT (MOVDconst [0]) (MOVDconst [1]) (CMPW x y))
@@ -2928,10 +2928,10 @@ func rewriteValueS390X_OpLess32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPW, TypeFlags)
@@ -2944,8 +2944,8 @@ func rewriteValueS390X_OpLess32(v *Value) bool {
 func rewriteValueS390X_OpLess32F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less32F x y)
 	// cond:
 	// result: (MOVDGTnoinv (MOVDconst [0]) (MOVDconst [1]) (FCMPS y x))
@@ -2953,10 +2953,10 @@ func rewriteValueS390X_OpLess32F(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGTnoinv)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XFCMPS, TypeFlags)
@@ -2969,8 +2969,8 @@ func rewriteValueS390X_OpLess32F(v *Value) bool {
 func rewriteValueS390X_OpLess32U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less32U x y)
 	// cond:
 	// result: (MOVDLT (MOVDconst [0]) (MOVDconst [1]) (CMPWU x y))
@@ -2978,10 +2978,10 @@ func rewriteValueS390X_OpLess32U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWU, TypeFlags)
@@ -2994,8 +2994,8 @@ func rewriteValueS390X_OpLess32U(v *Value) bool {
 func rewriteValueS390X_OpLess64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less64  x y)
 	// cond:
 	// result: (MOVDLT (MOVDconst [0]) (MOVDconst [1]) (CMP x y))
@@ -3003,10 +3003,10 @@ func rewriteValueS390X_OpLess64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
@@ -3019,8 +3019,8 @@ func rewriteValueS390X_OpLess64(v *Value) bool {
 func rewriteValueS390X_OpLess64F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less64F x y)
 	// cond:
 	// result: (MOVDGTnoinv (MOVDconst [0]) (MOVDconst [1]) (FCMP y x))
@@ -3028,10 +3028,10 @@ func rewriteValueS390X_OpLess64F(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDGTnoinv)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XFCMP, TypeFlags)
@@ -3044,8 +3044,8 @@ func rewriteValueS390X_OpLess64F(v *Value) bool {
 func rewriteValueS390X_OpLess64U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less64U x y)
 	// cond:
 	// result: (MOVDLT (MOVDconst [0]) (MOVDconst [1]) (CMPU x y))
@@ -3053,10 +3053,10 @@ func rewriteValueS390X_OpLess64U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
@@ -3069,8 +3069,8 @@ func rewriteValueS390X_OpLess64U(v *Value) bool {
 func rewriteValueS390X_OpLess8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less8   x y)
 	// cond:
 	// result: (MOVDLT (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVBreg x) (MOVBreg y)))
@@ -3078,17 +3078,17 @@ func rewriteValueS390X_OpLess8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -3098,8 +3098,8 @@ func rewriteValueS390X_OpLess8(v *Value) bool {
 func rewriteValueS390X_OpLess8U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less8U  x y)
 	// cond:
 	// result: (MOVDLT (MOVDconst [0]) (MOVDconst [1]) (CMPU (MOVBZreg x) (MOVBZreg y)))
@@ -3107,17 +3107,17 @@ func rewriteValueS390X_OpLess8U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDLT)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPU, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -3265,8 +3265,8 @@ func rewriteValueS390X_OpLoad(v *Value) bool {
 func rewriteValueS390X_OpLsh16x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x16 <t> x y)
 	// cond:
 	// result: (ANDW (SLW <t> x y) (SUBEWcarrymask <t> (CMPWUconst (MOVHZreg y) [31])))
@@ -3282,7 +3282,7 @@ func rewriteValueS390X_OpLsh16x16(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, t)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v2.AuxInt = 31
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3341,8 +3341,8 @@ func rewriteValueS390X_OpLsh16x64(v *Value) bool {
 func rewriteValueS390X_OpLsh16x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x8  <t> x y)
 	// cond:
 	// result: (ANDW (SLW <t> x y) (SUBEWcarrymask <t> (CMPWUconst (MOVBZreg y) [31])))
@@ -3358,7 +3358,7 @@ func rewriteValueS390X_OpLsh16x8(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, t)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v2.AuxInt = 31
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3369,8 +3369,8 @@ func rewriteValueS390X_OpLsh16x8(v *Value) bool {
 func rewriteValueS390X_OpLsh32x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x16 <t> x y)
 	// cond:
 	// result: (ANDW (SLW <t> x y) (SUBEWcarrymask <t> (CMPWUconst (MOVHZreg y) [31])))
@@ -3386,7 +3386,7 @@ func rewriteValueS390X_OpLsh32x16(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, t)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v2.AuxInt = 31
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3445,8 +3445,8 @@ func rewriteValueS390X_OpLsh32x64(v *Value) bool {
 func rewriteValueS390X_OpLsh32x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x8  <t> x y)
 	// cond:
 	// result: (ANDW (SLW <t> x y) (SUBEWcarrymask <t> (CMPWUconst (MOVBZreg y) [31])))
@@ -3462,7 +3462,7 @@ func rewriteValueS390X_OpLsh32x8(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, t)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v2.AuxInt = 31
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3473,8 +3473,8 @@ func rewriteValueS390X_OpLsh32x8(v *Value) bool {
 func rewriteValueS390X_OpLsh64x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x16 <t> x y)
 	// cond:
 	// result: (AND (SLD <t> x y) (SUBEcarrymask <t> (CMPWUconst (MOVHZreg y) [63])))
@@ -3490,7 +3490,7 @@ func rewriteValueS390X_OpLsh64x16(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XSUBEcarrymask, t)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v2.AuxInt = 63
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3549,8 +3549,8 @@ func rewriteValueS390X_OpLsh64x64(v *Value) bool {
 func rewriteValueS390X_OpLsh64x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x8  <t> x y)
 	// cond:
 	// result: (AND (SLD <t> x y) (SUBEcarrymask <t> (CMPWUconst (MOVBZreg y) [63])))
@@ -3566,7 +3566,7 @@ func rewriteValueS390X_OpLsh64x8(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XSUBEcarrymask, t)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v2.AuxInt = 63
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3577,8 +3577,8 @@ func rewriteValueS390X_OpLsh64x8(v *Value) bool {
 func rewriteValueS390X_OpLsh8x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x16 <t> x y)
 	// cond:
 	// result: (ANDW (SLW <t> x y) (SUBEWcarrymask <t> (CMPWUconst (MOVHZreg y) [31])))
@@ -3594,7 +3594,7 @@ func rewriteValueS390X_OpLsh8x16(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, t)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v2.AuxInt = 31
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3653,8 +3653,8 @@ func rewriteValueS390X_OpLsh8x64(v *Value) bool {
 func rewriteValueS390X_OpLsh8x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x8  <t> x y)
 	// cond:
 	// result: (ANDW (SLW <t> x y) (SUBEWcarrymask <t> (CMPWUconst (MOVBZreg y) [31])))
@@ -3670,7 +3670,7 @@ func rewriteValueS390X_OpLsh8x8(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, t)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v2.AuxInt = 31
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -3681,8 +3681,8 @@ func rewriteValueS390X_OpLsh8x8(v *Value) bool {
 func rewriteValueS390X_OpMod16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16  x y)
 	// cond:
 	// result: (MODW  (MOVHreg x) (MOVHreg y))
@@ -3690,10 +3690,10 @@ func rewriteValueS390X_OpMod16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMODW)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -3702,8 +3702,8 @@ func rewriteValueS390X_OpMod16(v *Value) bool {
 func rewriteValueS390X_OpMod16u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod16u x y)
 	// cond:
 	// result: (MODWU (MOVHZreg x) (MOVHZreg y))
@@ -3711,10 +3711,10 @@ func rewriteValueS390X_OpMod16u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMODWU)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -3723,8 +3723,8 @@ func rewriteValueS390X_OpMod16u(v *Value) bool {
 func rewriteValueS390X_OpMod32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod32  x y)
 	// cond:
 	// result: (MODW  (MOVWreg x) y)
@@ -3732,7 +3732,7 @@ func rewriteValueS390X_OpMod32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMODW)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVWreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVWreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v.AddArg(y)
@@ -3742,8 +3742,8 @@ func rewriteValueS390X_OpMod32(v *Value) bool {
 func rewriteValueS390X_OpMod32u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod32u x y)
 	// cond:
 	// result: (MODWU (MOVWZreg x) y)
@@ -3751,7 +3751,7 @@ func rewriteValueS390X_OpMod32u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMODWU)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVWZreg, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVWZreg, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v.AddArg(y)
@@ -3787,8 +3787,8 @@ func rewriteValueS390X_OpMod64u(v *Value) bool {
 func rewriteValueS390X_OpMod8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8   x y)
 	// cond:
 	// result: (MODW  (MOVBreg x) (MOVBreg y))
@@ -3796,10 +3796,10 @@ func rewriteValueS390X_OpMod8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMODW)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -3808,8 +3808,8 @@ func rewriteValueS390X_OpMod8(v *Value) bool {
 func rewriteValueS390X_OpMod8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mod8u  x y)
 	// cond:
 	// result: (MODWU (MOVBZreg x) (MOVBZreg y))
@@ -3817,10 +3817,10 @@ func rewriteValueS390X_OpMod8u(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMODWU)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -3829,8 +3829,8 @@ func rewriteValueS390X_OpMod8u(v *Value) bool {
 func rewriteValueS390X_OpMove(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Move [0] _ _ mem)
 	// cond:
 	// result: mem
@@ -3856,7 +3856,7 @@ func rewriteValueS390X_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpS390XMOVBstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBZload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBZload, types.UInt8)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -3875,7 +3875,7 @@ func rewriteValueS390X_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpS390XMOVHstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHZload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHZload, types.UInt16)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -3894,7 +3894,7 @@ func rewriteValueS390X_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpS390XMOVWstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVWZload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVWZload, types.UInt32)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -3913,7 +3913,7 @@ func rewriteValueS390X_OpMove(v *Value) bool {
 		mem := v.Args[2]
 		v.reset(OpS390XMOVDstore)
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDload, types.UInt64)
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
@@ -3933,14 +3933,14 @@ func rewriteValueS390X_OpMove(v *Value) bool {
 		v.reset(OpS390XMOVDstore)
 		v.AuxInt = 8
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDload, types.UInt64)
 		v0.AuxInt = 8
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XMOVDstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpS390XMOVDload, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpS390XMOVDload, types.UInt64)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -3961,7 +3961,7 @@ func rewriteValueS390X_OpMove(v *Value) bool {
 		v.reset(OpS390XMOVDstore)
 		v.AuxInt = 16
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDload, types.UInt64)
 		v0.AuxInt = 16
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -3969,14 +3969,14 @@ func rewriteValueS390X_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XMOVDstore, TypeMem)
 		v1.AuxInt = 8
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpS390XMOVDload, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpS390XMOVDload, types.UInt64)
 		v2.AuxInt = 8
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpS390XMOVDstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVDload, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVDload, types.UInt64)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -3998,14 +3998,14 @@ func rewriteValueS390X_OpMove(v *Value) bool {
 		v.reset(OpS390XMOVBstore)
 		v.AuxInt = 2
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBZload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBZload, types.UInt8)
 		v0.AuxInt = 2
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XMOVHstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpS390XMOVHZload, fe.TypeUInt16())
+		v2 := b.NewValue0(v.Pos, OpS390XMOVHZload, types.UInt16)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -4026,14 +4026,14 @@ func rewriteValueS390X_OpMove(v *Value) bool {
 		v.reset(OpS390XMOVBstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBZload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBZload, types.UInt8)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XMOVWstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpS390XMOVWZload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpS390XMOVWZload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -4054,14 +4054,14 @@ func rewriteValueS390X_OpMove(v *Value) bool {
 		v.reset(OpS390XMOVHstore)
 		v.AuxInt = 4
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHZload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHZload, types.UInt16)
 		v0.AuxInt = 4
 		v0.AddArg(src)
 		v0.AddArg(mem)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XMOVWstore, TypeMem)
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpS390XMOVWZload, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpS390XMOVWZload, types.UInt32)
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
@@ -4082,7 +4082,7 @@ func rewriteValueS390X_OpMove(v *Value) bool {
 		v.reset(OpS390XMOVBstore)
 		v.AuxInt = 6
 		v.AddArg(dst)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBZload, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBZload, types.UInt8)
 		v0.AuxInt = 6
 		v0.AddArg(src)
 		v0.AddArg(mem)
@@ -4090,14 +4090,14 @@ func rewriteValueS390X_OpMove(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XMOVHstore, TypeMem)
 		v1.AuxInt = 4
 		v1.AddArg(dst)
-		v2 := b.NewValue0(v.Pos, OpS390XMOVHZload, fe.TypeUInt16())
+		v2 := b.NewValue0(v.Pos, OpS390XMOVHZload, types.UInt16)
 		v2.AuxInt = 4
 		v2.AddArg(src)
 		v2.AddArg(mem)
 		v1.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpS390XMOVWstore, TypeMem)
 		v3.AddArg(dst)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVWZload, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVWZload, types.UInt32)
 		v4.AddArg(src)
 		v4.AddArg(mem)
 		v3.AddArg(v4)
@@ -4313,15 +4313,15 @@ func rewriteValueS390X_OpMul8(v *Value) bool {
 func rewriteValueS390X_OpNeg16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neg16  x)
 	// cond:
 	// result: (NEGW (MOVHreg x))
 	for {
 		x := v.Args[0]
 		v.reset(OpS390XNEGW)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -4374,15 +4374,15 @@ func rewriteValueS390X_OpNeg64F(v *Value) bool {
 func rewriteValueS390X_OpNeg8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neg8   x)
 	// cond:
 	// result: (NEGW (MOVBreg x))
 	for {
 		x := v.Args[0]
 		v.reset(OpS390XNEGW)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -4391,8 +4391,8 @@ func rewriteValueS390X_OpNeg8(v *Value) bool {
 func rewriteValueS390X_OpNeq16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq16  x y)
 	// cond:
 	// result: (MOVDNE (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVHreg x) (MOVHreg y)))
@@ -4400,17 +4400,17 @@ func rewriteValueS390X_OpNeq16(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDNE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -4420,8 +4420,8 @@ func rewriteValueS390X_OpNeq16(v *Value) bool {
 func rewriteValueS390X_OpNeq32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq32  x y)
 	// cond:
 	// result: (MOVDNE (MOVDconst [0]) (MOVDconst [1]) (CMPW x y))
@@ -4429,10 +4429,10 @@ func rewriteValueS390X_OpNeq32(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDNE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPW, TypeFlags)
@@ -4445,8 +4445,8 @@ func rewriteValueS390X_OpNeq32(v *Value) bool {
 func rewriteValueS390X_OpNeq32F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq32F x y)
 	// cond:
 	// result: (MOVDNE (MOVDconst [0]) (MOVDconst [1]) (FCMPS x y))
@@ -4454,10 +4454,10 @@ func rewriteValueS390X_OpNeq32F(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDNE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XFCMPS, TypeFlags)
@@ -4470,8 +4470,8 @@ func rewriteValueS390X_OpNeq32F(v *Value) bool {
 func rewriteValueS390X_OpNeq64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq64  x y)
 	// cond:
 	// result: (MOVDNE (MOVDconst [0]) (MOVDconst [1]) (CMP x y))
@@ -4479,10 +4479,10 @@ func rewriteValueS390X_OpNeq64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDNE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
@@ -4495,8 +4495,8 @@ func rewriteValueS390X_OpNeq64(v *Value) bool {
 func rewriteValueS390X_OpNeq64F(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq64F x y)
 	// cond:
 	// result: (MOVDNE (MOVDconst [0]) (MOVDconst [1]) (FCMP x y))
@@ -4504,10 +4504,10 @@ func rewriteValueS390X_OpNeq64F(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDNE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XFCMP, TypeFlags)
@@ -4520,8 +4520,8 @@ func rewriteValueS390X_OpNeq64F(v *Value) bool {
 func rewriteValueS390X_OpNeq8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq8   x y)
 	// cond:
 	// result: (MOVDNE (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVBreg x) (MOVBreg y)))
@@ -4529,17 +4529,17 @@ func rewriteValueS390X_OpNeq8(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDNE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -4549,8 +4549,8 @@ func rewriteValueS390X_OpNeq8(v *Value) bool {
 func rewriteValueS390X_OpNeqB(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (NeqB   x y)
 	// cond:
 	// result: (MOVDNE (MOVDconst [0]) (MOVDconst [1]) (CMP (MOVBreg x) (MOVBreg y)))
@@ -4558,17 +4558,17 @@ func rewriteValueS390X_OpNeqB(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDNE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v4.AddArg(y)
 		v2.AddArg(v4)
 		v.AddArg(v2)
@@ -4578,8 +4578,8 @@ func rewriteValueS390X_OpNeqB(v *Value) bool {
 func rewriteValueS390X_OpNeqPtr(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (NeqPtr x y)
 	// cond:
 	// result: (MOVDNE (MOVDconst [0]) (MOVDconst [1]) (CMP x y))
@@ -4587,10 +4587,10 @@ func rewriteValueS390X_OpNeqPtr(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpS390XMOVDNE)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = 0
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v1.AuxInt = 1
 		v.AddArg(v1)
 		v2 := b.NewValue0(v.Pos, OpS390XCMP, TypeFlags)
@@ -4628,8 +4628,8 @@ func rewriteValueS390X_OpNot(v *Value) bool {
 func rewriteValueS390X_OpOffPtr(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (OffPtr [off] ptr:(SP))
 	// cond:
 	// result: (MOVDaddr [off] ptr)
@@ -4665,7 +4665,7 @@ func rewriteValueS390X_OpOffPtr(v *Value) bool {
 		off := v.AuxInt
 		ptr := v.Args[0]
 		v.reset(OpS390XADD)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = off
 		v.AddArg(v0)
 		v.AddArg(ptr)
@@ -4762,8 +4762,8 @@ func rewriteValueS390X_OpRound64F(v *Value) bool {
 func rewriteValueS390X_OpRsh16Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux16 <t> x y)
 	// cond:
 	// result: (ANDW (SRW <t> (MOVHZreg x) y) (SUBEWcarrymask <t> (CMPWUconst (MOVHZreg y) [15])))
@@ -4773,7 +4773,7 @@ func rewriteValueS390X_OpRsh16Ux16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XANDW)
 		v0 := b.NewValue0(v.Pos, OpS390XSRW, t)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -4781,7 +4781,7 @@ func rewriteValueS390X_OpRsh16Ux16(v *Value) bool {
 		v2 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, t)
 		v3 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v3.AuxInt = 15
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -4792,8 +4792,8 @@ func rewriteValueS390X_OpRsh16Ux16(v *Value) bool {
 func rewriteValueS390X_OpRsh16Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux32 <t> x y)
 	// cond:
 	// result: (ANDW (SRW <t> (MOVHZreg x) y) (SUBEWcarrymask <t> (CMPWUconst y [15])))
@@ -4803,7 +4803,7 @@ func rewriteValueS390X_OpRsh16Ux32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XANDW)
 		v0 := b.NewValue0(v.Pos, OpS390XSRW, t)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -4820,8 +4820,8 @@ func rewriteValueS390X_OpRsh16Ux32(v *Value) bool {
 func rewriteValueS390X_OpRsh16Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux64 <t> x y)
 	// cond:
 	// result: (ANDW (SRW <t> (MOVHZreg x) y) (SUBEWcarrymask <t> (CMPUconst y [15])))
@@ -4831,7 +4831,7 @@ func rewriteValueS390X_OpRsh16Ux64(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XANDW)
 		v0 := b.NewValue0(v.Pos, OpS390XSRW, t)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -4848,8 +4848,8 @@ func rewriteValueS390X_OpRsh16Ux64(v *Value) bool {
 func rewriteValueS390X_OpRsh16Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux8  <t> x y)
 	// cond:
 	// result: (ANDW (SRW <t> (MOVHZreg x) y) (SUBEWcarrymask <t> (CMPWUconst (MOVBZreg y) [15])))
@@ -4859,7 +4859,7 @@ func rewriteValueS390X_OpRsh16Ux8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XANDW)
 		v0 := b.NewValue0(v.Pos, OpS390XSRW, t)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -4867,7 +4867,7 @@ func rewriteValueS390X_OpRsh16Ux8(v *Value) bool {
 		v2 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, t)
 		v3 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v3.AuxInt = 15
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -4878,8 +4878,8 @@ func rewriteValueS390X_OpRsh16Ux8(v *Value) bool {
 func rewriteValueS390X_OpRsh16x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x16 <t> x y)
 	// cond:
 	// result: (SRAW <t> (MOVHreg x) (ORW <y.Type> y (NOTW <y.Type> (SUBEWcarrymask <y.Type> (CMPWUconst (MOVHZreg y) [15])))))
@@ -4889,7 +4889,7 @@ func rewriteValueS390X_OpRsh16x16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XSRAW)
 		v.Type = t
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XORW, y.Type)
@@ -4898,7 +4898,7 @@ func rewriteValueS390X_OpRsh16x16(v *Value) bool {
 		v3 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, y.Type)
 		v4 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v4.AuxInt = 15
-		v5 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v3.AddArg(v4)
@@ -4911,8 +4911,8 @@ func rewriteValueS390X_OpRsh16x16(v *Value) bool {
 func rewriteValueS390X_OpRsh16x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x32 <t> x y)
 	// cond:
 	// result: (SRAW <t> (MOVHreg x) (ORW <y.Type> y (NOTW <y.Type> (SUBEWcarrymask <y.Type> (CMPWUconst y [15])))))
@@ -4922,7 +4922,7 @@ func rewriteValueS390X_OpRsh16x32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XSRAW)
 		v.Type = t
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XORW, y.Type)
@@ -4942,8 +4942,8 @@ func rewriteValueS390X_OpRsh16x32(v *Value) bool {
 func rewriteValueS390X_OpRsh16x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x64 <t> x y)
 	// cond:
 	// result: (SRAW <t> (MOVHreg x) (OR <y.Type> y (NOT <y.Type> (SUBEcarrymask <y.Type> (CMPUconst y [15])))))
@@ -4953,7 +4953,7 @@ func rewriteValueS390X_OpRsh16x64(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XSRAW)
 		v.Type = t
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XOR, y.Type)
@@ -4973,8 +4973,8 @@ func rewriteValueS390X_OpRsh16x64(v *Value) bool {
 func rewriteValueS390X_OpRsh16x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x8  <t> x y)
 	// cond:
 	// result: (SRAW <t> (MOVHreg x) (ORW <y.Type> y (NOTW <y.Type> (SUBEWcarrymask <y.Type> (CMPWUconst (MOVBZreg y) [15])))))
@@ -4984,7 +4984,7 @@ func rewriteValueS390X_OpRsh16x8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XSRAW)
 		v.Type = t
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XORW, y.Type)
@@ -4993,7 +4993,7 @@ func rewriteValueS390X_OpRsh16x8(v *Value) bool {
 		v3 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, y.Type)
 		v4 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v4.AuxInt = 15
-		v5 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v3.AddArg(v4)
@@ -5006,8 +5006,8 @@ func rewriteValueS390X_OpRsh16x8(v *Value) bool {
 func rewriteValueS390X_OpRsh32Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux16 <t> x y)
 	// cond:
 	// result: (ANDW (SRW <t> x y) (SUBEWcarrymask <t> (CMPWUconst (MOVHZreg y) [31])))
@@ -5023,7 +5023,7 @@ func rewriteValueS390X_OpRsh32Ux16(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, t)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v2.AuxInt = 31
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -5082,8 +5082,8 @@ func rewriteValueS390X_OpRsh32Ux64(v *Value) bool {
 func rewriteValueS390X_OpRsh32Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux8  <t> x y)
 	// cond:
 	// result: (ANDW (SRW <t> x y) (SUBEWcarrymask <t> (CMPWUconst (MOVBZreg y) [31])))
@@ -5099,7 +5099,7 @@ func rewriteValueS390X_OpRsh32Ux8(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, t)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v2.AuxInt = 31
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -5110,8 +5110,8 @@ func rewriteValueS390X_OpRsh32Ux8(v *Value) bool {
 func rewriteValueS390X_OpRsh32x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x16 <t> x y)
 	// cond:
 	// result: (SRAW <t> x (ORW <y.Type> y (NOTW <y.Type> (SUBEWcarrymask <y.Type> (CMPWUconst (MOVHZreg y) [31])))))
@@ -5128,7 +5128,7 @@ func rewriteValueS390X_OpRsh32x16(v *Value) bool {
 		v2 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, y.Type)
 		v3 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v3.AuxInt = 31
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -5195,8 +5195,8 @@ func rewriteValueS390X_OpRsh32x64(v *Value) bool {
 func rewriteValueS390X_OpRsh32x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x8  <t> x y)
 	// cond:
 	// result: (SRAW <t> x (ORW <y.Type> y (NOTW <y.Type> (SUBEWcarrymask <y.Type> (CMPWUconst (MOVBZreg y) [31])))))
@@ -5213,7 +5213,7 @@ func rewriteValueS390X_OpRsh32x8(v *Value) bool {
 		v2 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, y.Type)
 		v3 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v3.AuxInt = 31
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -5226,8 +5226,8 @@ func rewriteValueS390X_OpRsh32x8(v *Value) bool {
 func rewriteValueS390X_OpRsh64Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux16 <t> x y)
 	// cond:
 	// result: (AND (SRD <t> x y) (SUBEcarrymask <t> (CMPWUconst (MOVHZreg y) [63])))
@@ -5243,7 +5243,7 @@ func rewriteValueS390X_OpRsh64Ux16(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XSUBEcarrymask, t)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v2.AuxInt = 63
-		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -5302,8 +5302,8 @@ func rewriteValueS390X_OpRsh64Ux64(v *Value) bool {
 func rewriteValueS390X_OpRsh64Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux8  <t> x y)
 	// cond:
 	// result: (AND (SRD <t> x y) (SUBEcarrymask <t> (CMPWUconst (MOVBZreg y) [63])))
@@ -5319,7 +5319,7 @@ func rewriteValueS390X_OpRsh64Ux8(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpS390XSUBEcarrymask, t)
 		v2 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v2.AuxInt = 63
-		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v3.AddArg(y)
 		v2.AddArg(v3)
 		v1.AddArg(v2)
@@ -5330,8 +5330,8 @@ func rewriteValueS390X_OpRsh64Ux8(v *Value) bool {
 func rewriteValueS390X_OpRsh64x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x16 <t> x y)
 	// cond:
 	// result: (SRAD <t> x (ORW <y.Type> y (NOTW <y.Type> (SUBEWcarrymask <y.Type> (CMPWUconst (MOVHZreg y) [63])))))
@@ -5348,7 +5348,7 @@ func rewriteValueS390X_OpRsh64x16(v *Value) bool {
 		v2 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, y.Type)
 		v3 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v3.AuxInt = 63
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -5415,8 +5415,8 @@ func rewriteValueS390X_OpRsh64x64(v *Value) bool {
 func rewriteValueS390X_OpRsh64x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x8  <t> x y)
 	// cond:
 	// result: (SRAD <t> x (ORW <y.Type> y (NOTW <y.Type> (SUBEWcarrymask <y.Type> (CMPWUconst (MOVBZreg y) [63])))))
@@ -5433,7 +5433,7 @@ func rewriteValueS390X_OpRsh64x8(v *Value) bool {
 		v2 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, y.Type)
 		v3 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v3.AuxInt = 63
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -5446,8 +5446,8 @@ func rewriteValueS390X_OpRsh64x8(v *Value) bool {
 func rewriteValueS390X_OpRsh8Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux16 <t> x y)
 	// cond:
 	// result: (ANDW (SRW <t> (MOVBZreg x) y) (SUBEWcarrymask <t> (CMPWUconst (MOVHZreg y) [7])))
@@ -5457,7 +5457,7 @@ func rewriteValueS390X_OpRsh8Ux16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XANDW)
 		v0 := b.NewValue0(v.Pos, OpS390XSRW, t)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -5465,7 +5465,7 @@ func rewriteValueS390X_OpRsh8Ux16(v *Value) bool {
 		v2 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, t)
 		v3 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v3.AuxInt = 7
-		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -5476,8 +5476,8 @@ func rewriteValueS390X_OpRsh8Ux16(v *Value) bool {
 func rewriteValueS390X_OpRsh8Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux32 <t> x y)
 	// cond:
 	// result: (ANDW (SRW <t> (MOVBZreg x) y) (SUBEWcarrymask <t> (CMPWUconst y [7])))
@@ -5487,7 +5487,7 @@ func rewriteValueS390X_OpRsh8Ux32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XANDW)
 		v0 := b.NewValue0(v.Pos, OpS390XSRW, t)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -5504,8 +5504,8 @@ func rewriteValueS390X_OpRsh8Ux32(v *Value) bool {
 func rewriteValueS390X_OpRsh8Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux64 <t> x y)
 	// cond:
 	// result: (ANDW (SRW <t> (MOVBZreg x) y) (SUBEWcarrymask <t> (CMPUconst y [7])))
@@ -5515,7 +5515,7 @@ func rewriteValueS390X_OpRsh8Ux64(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XANDW)
 		v0 := b.NewValue0(v.Pos, OpS390XSRW, t)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -5532,8 +5532,8 @@ func rewriteValueS390X_OpRsh8Ux64(v *Value) bool {
 func rewriteValueS390X_OpRsh8Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux8  <t> x y)
 	// cond:
 	// result: (ANDW (SRW <t> (MOVBZreg x) y) (SUBEWcarrymask <t> (CMPWUconst (MOVBZreg y) [7])))
@@ -5543,7 +5543,7 @@ func rewriteValueS390X_OpRsh8Ux8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XANDW)
 		v0 := b.NewValue0(v.Pos, OpS390XSRW, t)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v0.AddArg(y)
@@ -5551,7 +5551,7 @@ func rewriteValueS390X_OpRsh8Ux8(v *Value) bool {
 		v2 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, t)
 		v3 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v3.AuxInt = 7
-		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v4.AddArg(y)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
@@ -5562,8 +5562,8 @@ func rewriteValueS390X_OpRsh8Ux8(v *Value) bool {
 func rewriteValueS390X_OpRsh8x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x16 <t> x y)
 	// cond:
 	// result: (SRAW <t> (MOVBreg x) (ORW <y.Type> y (NOTW <y.Type> (SUBEWcarrymask <y.Type> (CMPWUconst (MOVHZreg y) [7])))))
@@ -5573,7 +5573,7 @@ func rewriteValueS390X_OpRsh8x16(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XSRAW)
 		v.Type = t
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XORW, y.Type)
@@ -5582,7 +5582,7 @@ func rewriteValueS390X_OpRsh8x16(v *Value) bool {
 		v3 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, y.Type)
 		v4 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v4.AuxInt = 7
-		v5 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v3.AddArg(v4)
@@ -5595,8 +5595,8 @@ func rewriteValueS390X_OpRsh8x16(v *Value) bool {
 func rewriteValueS390X_OpRsh8x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x32 <t> x y)
 	// cond:
 	// result: (SRAW <t> (MOVBreg x) (ORW <y.Type> y (NOTW <y.Type> (SUBEWcarrymask <y.Type> (CMPWUconst y [7])))))
@@ -5606,7 +5606,7 @@ func rewriteValueS390X_OpRsh8x32(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XSRAW)
 		v.Type = t
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XORW, y.Type)
@@ -5626,8 +5626,8 @@ func rewriteValueS390X_OpRsh8x32(v *Value) bool {
 func rewriteValueS390X_OpRsh8x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x64 <t> x y)
 	// cond:
 	// result: (SRAW <t> (MOVBreg x) (OR <y.Type> y (NOT <y.Type> (SUBEcarrymask <y.Type> (CMPUconst y [7])))))
@@ -5637,7 +5637,7 @@ func rewriteValueS390X_OpRsh8x64(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XSRAW)
 		v.Type = t
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XOR, y.Type)
@@ -5657,8 +5657,8 @@ func rewriteValueS390X_OpRsh8x64(v *Value) bool {
 func rewriteValueS390X_OpRsh8x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x8  <t> x y)
 	// cond:
 	// result: (SRAW <t> (MOVBreg x) (ORW <y.Type> y (NOTW <y.Type> (SUBEWcarrymask <y.Type> (CMPWUconst (MOVBZreg y) [7])))))
@@ -5668,7 +5668,7 @@ func rewriteValueS390X_OpRsh8x8(v *Value) bool {
 		y := v.Args[1]
 		v.reset(OpS390XSRAW)
 		v.Type = t
-		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, fe.TypeInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVBreg, types.Int64)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XORW, y.Type)
@@ -5677,7 +5677,7 @@ func rewriteValueS390X_OpRsh8x8(v *Value) bool {
 		v3 := b.NewValue0(v.Pos, OpS390XSUBEWcarrymask, y.Type)
 		v4 := b.NewValue0(v.Pos, OpS390XCMPWUconst, TypeFlags)
 		v4.AuxInt = 7
-		v5 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.UInt64)
 		v5.AddArg(y)
 		v4.AddArg(v5)
 		v3.AddArg(v4)
@@ -14013,8 +14013,8 @@ func rewriteValueS390X_OpS390XMOVWstore(v *Value) bool {
 func rewriteValueS390X_OpS390XMOVWstoreconst(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (MOVWstoreconst [sc] {s} (ADDconst [off] ptr) mem)
 	// cond: ValAndOff(sc).canAdd(off)
 	// result: (MOVWstoreconst [ValAndOff(sc).add(off)] {s} ptr mem)
@@ -14088,7 +14088,7 @@ func rewriteValueS390X_OpS390XMOVWstoreconst(v *Value) bool {
 		v.AuxInt = ValAndOff(a).Off()
 		v.Aux = s
 		v.AddArg(p)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = ValAndOff(c).Val()&0xffffffff | ValAndOff(a).Val()<<32
 		v.AddArg(v0)
 		v.AddArg(mem)
@@ -14739,8 +14739,8 @@ func rewriteValueS390X_OpS390XNEGW(v *Value) bool {
 func rewriteValueS390X_OpS390XNOT(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (NOT x)
 	// cond: true
 	// result: (XOR (MOVDconst [-1]) x)
@@ -14750,7 +14750,7 @@ func rewriteValueS390X_OpS390XNOT(v *Value) bool {
 			break
 		}
 		v.reset(OpS390XXOR)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDconst, types.UInt64)
 		v0.AuxInt = -1
 		v.AddArg(v0)
 		v.AddArg(x)
@@ -14777,8 +14777,8 @@ func rewriteValueS390X_OpS390XNOTW(v *Value) bool {
 func rewriteValueS390X_OpS390XOR(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (OR x (MOVDconst [c]))
 	// cond: isU32Bit(c)
 	// result: (ORconst [c] x)
@@ -15166,7 +15166,7 @@ func rewriteValueS390X_OpS390XOR(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1, x2, x3, x4, x5, x6, x7)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDBRload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDBRload, types.UInt64)
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v0.AuxInt = i
@@ -15609,7 +15609,7 @@ func rewriteValueS390X_OpS390XOR(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1, x2, x3, x4, x5, x6, x7)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVDload, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVDload, types.UInt64)
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v0.AuxInt = i - 7
@@ -15856,8 +15856,8 @@ func rewriteValueS390X_OpS390XOR(v *Value) bool {
 func rewriteValueS390X_OpS390XORW(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (ORW x (MOVDconst [c]))
 	// cond:
 	// result: (ORWconst [c] x)
@@ -16111,10 +16111,10 @@ func rewriteValueS390X_OpS390XORW(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v.reset(OpCopy)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpS390XMOVHBRload, fe.TypeUInt16())
+		v1 := b.NewValue0(v.Pos, OpS390XMOVHBRload, types.UInt16)
 		v1.AuxInt = i
 		v1.Aux = s
 		v1.AddArg(p)
@@ -16192,7 +16192,7 @@ func rewriteValueS390X_OpS390XORW(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1, x2)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVWBRload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVWBRload, types.UInt32)
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v0.AuxInt = i
@@ -16244,7 +16244,7 @@ func rewriteValueS390X_OpS390XORW(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHZreg, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHZreg, types.UInt64)
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XMOVHBRloadidx, v.Type)
@@ -16333,7 +16333,7 @@ func rewriteValueS390X_OpS390XORW(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1, x2)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVWZreg, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVWZreg, types.UInt64)
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v1 := b.NewValue0(v.Pos, OpS390XMOVWBRloadidx, v.Type)
@@ -16384,7 +16384,7 @@ func rewriteValueS390X_OpS390XORW(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVHZload, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVHZload, types.UInt16)
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v0.AuxInt = i - 1
@@ -16459,7 +16459,7 @@ func rewriteValueS390X_OpS390XORW(v *Value) bool {
 			break
 		}
 		b = mergePoint(b, x0, x1, x2)
-		v0 := b.NewValue0(v.Pos, OpS390XMOVWZload, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpS390XMOVWZload, types.UInt32)
 		v.reset(OpCopy)
 		v.AddArg(v0)
 		v0.AuxInt = i - 2
@@ -18592,6 +18592,8 @@ func rewriteBlockS390X(b *Block) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &config.Types
+	_ = types
 	switch b.Kind {
 	case BlockS390XEQ:
 		// match: (EQ (InvertFlags cmp) yes no)
@@ -19047,7 +19049,7 @@ func rewriteBlockS390X(b *Block) bool {
 		}
 		// match: (If cond yes no)
 		// cond:
-		// result: (NE (CMPWconst [0] (MOVBZreg <fe.TypeBool()> cond)) yes no)
+		// result: (NE (CMPWconst [0] (MOVBZreg <types.Bool> cond)) yes no)
 		for {
 			v := b.Control
 			_ = v
@@ -19057,7 +19059,7 @@ func rewriteBlockS390X(b *Block) bool {
 			b.Kind = BlockS390XNE
 			v0 := b.NewValue0(v.Pos, OpS390XCMPWconst, TypeFlags)
 			v0.AuxInt = 0
-			v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, fe.TypeBool())
+			v1 := b.NewValue0(v.Pos, OpS390XMOVBZreg, types.Bool)
 			v1.AddArg(cond)
 			v0.AddArg(v1)
 			b.SetControl(v0)
diff --git a/src/cmd/compile/internal/ssa/rewritedec.go b/src/cmd/compile/internal/ssa/rewritedec.go
index eedf61f536..3946dca922 100644
--- a/src/cmd/compile/internal/ssa/rewritedec.go
+++ b/src/cmd/compile/internal/ssa/rewritedec.go
@@ -108,11 +108,11 @@ func rewriteValuedec_OpLoad(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Load <t> ptr mem)
 	// cond: t.IsComplex() && t.Size() == 8
-	// result: (ComplexMake     (Load <fe.TypeFloat32()> ptr mem)     (Load <fe.TypeFloat32()>       (OffPtr <fe.TypeFloat32().PtrTo()> [4] ptr)       mem)     )
+	// result: (ComplexMake     (Load <types.Float32> ptr mem)     (Load <types.Float32>       (OffPtr <types.Float32.PtrTo()> [4] ptr)       mem)     )
 	for {
 		t := v.Type
 		ptr := v.Args[0]
@@ -121,12 +121,12 @@ func rewriteValuedec_OpLoad(v *Value) bool {
 			break
 		}
 		v.reset(OpComplexMake)
-		v0 := b.NewValue0(v.Pos, OpLoad, fe.TypeFloat32())
+		v0 := b.NewValue0(v.Pos, OpLoad, types.Float32)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpLoad, fe.TypeFloat32())
-		v2 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeFloat32().PtrTo())
+		v1 := b.NewValue0(v.Pos, OpLoad, types.Float32)
+		v2 := b.NewValue0(v.Pos, OpOffPtr, types.Float32.PtrTo())
 		v2.AuxInt = 4
 		v2.AddArg(ptr)
 		v1.AddArg(v2)
@@ -136,7 +136,7 @@ func rewriteValuedec_OpLoad(v *Value) bool {
 	}
 	// match: (Load <t> ptr mem)
 	// cond: t.IsComplex() && t.Size() == 16
-	// result: (ComplexMake     (Load <fe.TypeFloat64()> ptr mem)     (Load <fe.TypeFloat64()>       (OffPtr <fe.TypeFloat64().PtrTo()> [8] ptr)       mem)     )
+	// result: (ComplexMake     (Load <types.Float64> ptr mem)     (Load <types.Float64>       (OffPtr <types.Float64.PtrTo()> [8] ptr)       mem)     )
 	for {
 		t := v.Type
 		ptr := v.Args[0]
@@ -145,12 +145,12 @@ func rewriteValuedec_OpLoad(v *Value) bool {
 			break
 		}
 		v.reset(OpComplexMake)
-		v0 := b.NewValue0(v.Pos, OpLoad, fe.TypeFloat64())
+		v0 := b.NewValue0(v.Pos, OpLoad, types.Float64)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpLoad, fe.TypeFloat64())
-		v2 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeFloat64().PtrTo())
+		v1 := b.NewValue0(v.Pos, OpLoad, types.Float64)
+		v2 := b.NewValue0(v.Pos, OpOffPtr, types.Float64.PtrTo())
 		v2.AuxInt = 8
 		v2.AddArg(ptr)
 		v1.AddArg(v2)
@@ -160,7 +160,7 @@ func rewriteValuedec_OpLoad(v *Value) bool {
 	}
 	// match: (Load <t> ptr mem)
 	// cond: t.IsString()
-	// result: (StringMake     (Load <fe.TypeBytePtr()> ptr mem)     (Load <fe.TypeInt()>       (OffPtr <fe.TypeInt().PtrTo()> [config.PtrSize] ptr)       mem))
+	// result: (StringMake     (Load <types.BytePtr> ptr mem)     (Load <types.Int>       (OffPtr <types.Int.PtrTo()> [config.PtrSize] ptr)       mem))
 	for {
 		t := v.Type
 		ptr := v.Args[0]
@@ -169,12 +169,12 @@ func rewriteValuedec_OpLoad(v *Value) bool {
 			break
 		}
 		v.reset(OpStringMake)
-		v0 := b.NewValue0(v.Pos, OpLoad, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpLoad, types.BytePtr)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpLoad, fe.TypeInt())
-		v2 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeInt().PtrTo())
+		v1 := b.NewValue0(v.Pos, OpLoad, types.Int)
+		v2 := b.NewValue0(v.Pos, OpOffPtr, types.Int.PtrTo())
 		v2.AuxInt = config.PtrSize
 		v2.AddArg(ptr)
 		v1.AddArg(v2)
@@ -184,7 +184,7 @@ func rewriteValuedec_OpLoad(v *Value) bool {
 	}
 	// match: (Load <t> ptr mem)
 	// cond: t.IsSlice()
-	// result: (SliceMake     (Load <t.ElemType().PtrTo()> ptr mem)     (Load <fe.TypeInt()>       (OffPtr <fe.TypeInt().PtrTo()> [config.PtrSize] ptr)       mem)     (Load <fe.TypeInt()>       (OffPtr <fe.TypeInt().PtrTo()> [2*config.PtrSize] ptr)       mem))
+	// result: (SliceMake     (Load <t.ElemType().PtrTo()> ptr mem)     (Load <types.Int>       (OffPtr <types.Int.PtrTo()> [config.PtrSize] ptr)       mem)     (Load <types.Int>       (OffPtr <types.Int.PtrTo()> [2*config.PtrSize] ptr)       mem))
 	for {
 		t := v.Type
 		ptr := v.Args[0]
@@ -197,15 +197,15 @@ func rewriteValuedec_OpLoad(v *Value) bool {
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpLoad, fe.TypeInt())
-		v2 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeInt().PtrTo())
+		v1 := b.NewValue0(v.Pos, OpLoad, types.Int)
+		v2 := b.NewValue0(v.Pos, OpOffPtr, types.Int.PtrTo())
 		v2.AuxInt = config.PtrSize
 		v2.AddArg(ptr)
 		v1.AddArg(v2)
 		v1.AddArg(mem)
 		v.AddArg(v1)
-		v3 := b.NewValue0(v.Pos, OpLoad, fe.TypeInt())
-		v4 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeInt().PtrTo())
+		v3 := b.NewValue0(v.Pos, OpLoad, types.Int)
+		v4 := b.NewValue0(v.Pos, OpOffPtr, types.Int.PtrTo())
 		v4.AuxInt = 2 * config.PtrSize
 		v4.AddArg(ptr)
 		v3.AddArg(v4)
@@ -215,7 +215,7 @@ func rewriteValuedec_OpLoad(v *Value) bool {
 	}
 	// match: (Load <t> ptr mem)
 	// cond: t.IsInterface()
-	// result: (IMake     (Load <fe.TypeBytePtr()> ptr mem)     (Load <fe.TypeBytePtr()>       (OffPtr <fe.TypeBytePtr().PtrTo()> [config.PtrSize] ptr)       mem))
+	// result: (IMake     (Load <types.BytePtr> ptr mem)     (Load <types.BytePtr>       (OffPtr <types.BytePtr.PtrTo()> [config.PtrSize] ptr)       mem))
 	for {
 		t := v.Type
 		ptr := v.Args[0]
@@ -224,12 +224,12 @@ func rewriteValuedec_OpLoad(v *Value) bool {
 			break
 		}
 		v.reset(OpIMake)
-		v0 := b.NewValue0(v.Pos, OpLoad, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpLoad, types.BytePtr)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpLoad, fe.TypeBytePtr())
-		v2 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeBytePtr().PtrTo())
+		v1 := b.NewValue0(v.Pos, OpLoad, types.BytePtr)
+		v2 := b.NewValue0(v.Pos, OpOffPtr, types.BytePtr.PtrTo())
 		v2.AuxInt = config.PtrSize
 		v2.AddArg(ptr)
 		v1.AddArg(v2)
@@ -295,11 +295,11 @@ func rewriteValuedec_OpStore(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Store {t} dst (ComplexMake real imag) mem)
 	// cond: t.(Type).Size() == 8
-	// result: (Store {fe.TypeFloat32()}     (OffPtr <fe.TypeFloat32().PtrTo()> [4] dst)     imag     (Store {fe.TypeFloat32()} dst real mem))
+	// result: (Store {types.Float32}     (OffPtr <types.Float32.PtrTo()> [4] dst)     imag     (Store {types.Float32} dst real mem))
 	for {
 		t := v.Aux
 		dst := v.Args[0]
@@ -314,14 +314,14 @@ func rewriteValuedec_OpStore(v *Value) bool {
 			break
 		}
 		v.reset(OpStore)
-		v.Aux = fe.TypeFloat32()
-		v0 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeFloat32().PtrTo())
+		v.Aux = types.Float32
+		v0 := b.NewValue0(v.Pos, OpOffPtr, types.Float32.PtrTo())
 		v0.AuxInt = 4
 		v0.AddArg(dst)
 		v.AddArg(v0)
 		v.AddArg(imag)
 		v1 := b.NewValue0(v.Pos, OpStore, TypeMem)
-		v1.Aux = fe.TypeFloat32()
+		v1.Aux = types.Float32
 		v1.AddArg(dst)
 		v1.AddArg(real)
 		v1.AddArg(mem)
@@ -330,7 +330,7 @@ func rewriteValuedec_OpStore(v *Value) bool {
 	}
 	// match: (Store {t} dst (ComplexMake real imag) mem)
 	// cond: t.(Type).Size() == 16
-	// result: (Store {fe.TypeFloat64()}     (OffPtr <fe.TypeFloat64().PtrTo()> [8] dst)     imag     (Store {fe.TypeFloat64()} dst real mem))
+	// result: (Store {types.Float64}     (OffPtr <types.Float64.PtrTo()> [8] dst)     imag     (Store {types.Float64} dst real mem))
 	for {
 		t := v.Aux
 		dst := v.Args[0]
@@ -345,14 +345,14 @@ func rewriteValuedec_OpStore(v *Value) bool {
 			break
 		}
 		v.reset(OpStore)
-		v.Aux = fe.TypeFloat64()
-		v0 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeFloat64().PtrTo())
+		v.Aux = types.Float64
+		v0 := b.NewValue0(v.Pos, OpOffPtr, types.Float64.PtrTo())
 		v0.AuxInt = 8
 		v0.AddArg(dst)
 		v.AddArg(v0)
 		v.AddArg(imag)
 		v1 := b.NewValue0(v.Pos, OpStore, TypeMem)
-		v1.Aux = fe.TypeFloat64()
+		v1.Aux = types.Float64
 		v1.AddArg(dst)
 		v1.AddArg(real)
 		v1.AddArg(mem)
@@ -361,7 +361,7 @@ func rewriteValuedec_OpStore(v *Value) bool {
 	}
 	// match: (Store dst (StringMake ptr len) mem)
 	// cond:
-	// result: (Store {fe.TypeInt()}     (OffPtr <fe.TypeInt().PtrTo()> [config.PtrSize] dst)     len     (Store {fe.TypeBytePtr()} dst ptr mem))
+	// result: (Store {types.Int}     (OffPtr <types.Int.PtrTo()> [config.PtrSize] dst)     len     (Store {types.BytePtr} dst ptr mem))
 	for {
 		dst := v.Args[0]
 		v_1 := v.Args[1]
@@ -372,14 +372,14 @@ func rewriteValuedec_OpStore(v *Value) bool {
 		len := v_1.Args[1]
 		mem := v.Args[2]
 		v.reset(OpStore)
-		v.Aux = fe.TypeInt()
-		v0 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeInt().PtrTo())
+		v.Aux = types.Int
+		v0 := b.NewValue0(v.Pos, OpOffPtr, types.Int.PtrTo())
 		v0.AuxInt = config.PtrSize
 		v0.AddArg(dst)
 		v.AddArg(v0)
 		v.AddArg(len)
 		v1 := b.NewValue0(v.Pos, OpStore, TypeMem)
-		v1.Aux = fe.TypeBytePtr()
+		v1.Aux = types.BytePtr
 		v1.AddArg(dst)
 		v1.AddArg(ptr)
 		v1.AddArg(mem)
@@ -388,7 +388,7 @@ func rewriteValuedec_OpStore(v *Value) bool {
 	}
 	// match: (Store dst (SliceMake ptr len cap) mem)
 	// cond:
-	// result: (Store {fe.TypeInt()}     (OffPtr <fe.TypeInt().PtrTo()> [2*config.PtrSize] dst)     cap     (Store {fe.TypeInt()}       (OffPtr <fe.TypeInt().PtrTo()> [config.PtrSize] dst)       len       (Store {fe.TypeBytePtr()} dst ptr mem)))
+	// result: (Store {types.Int}     (OffPtr <types.Int.PtrTo()> [2*config.PtrSize] dst)     cap     (Store {types.Int}       (OffPtr <types.Int.PtrTo()> [config.PtrSize] dst)       len       (Store {types.BytePtr} dst ptr mem)))
 	for {
 		dst := v.Args[0]
 		v_1 := v.Args[1]
@@ -400,21 +400,21 @@ func rewriteValuedec_OpStore(v *Value) bool {
 		cap := v_1.Args[2]
 		mem := v.Args[2]
 		v.reset(OpStore)
-		v.Aux = fe.TypeInt()
-		v0 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeInt().PtrTo())
+		v.Aux = types.Int
+		v0 := b.NewValue0(v.Pos, OpOffPtr, types.Int.PtrTo())
 		v0.AuxInt = 2 * config.PtrSize
 		v0.AddArg(dst)
 		v.AddArg(v0)
 		v.AddArg(cap)
 		v1 := b.NewValue0(v.Pos, OpStore, TypeMem)
-		v1.Aux = fe.TypeInt()
-		v2 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeInt().PtrTo())
+		v1.Aux = types.Int
+		v2 := b.NewValue0(v.Pos, OpOffPtr, types.Int.PtrTo())
 		v2.AuxInt = config.PtrSize
 		v2.AddArg(dst)
 		v1.AddArg(v2)
 		v1.AddArg(len)
 		v3 := b.NewValue0(v.Pos, OpStore, TypeMem)
-		v3.Aux = fe.TypeBytePtr()
+		v3.Aux = types.BytePtr
 		v3.AddArg(dst)
 		v3.AddArg(ptr)
 		v3.AddArg(mem)
@@ -424,7 +424,7 @@ func rewriteValuedec_OpStore(v *Value) bool {
 	}
 	// match: (Store dst (IMake itab data) mem)
 	// cond:
-	// result: (Store {fe.TypeBytePtr()}     (OffPtr <fe.TypeBytePtr().PtrTo()> [config.PtrSize] dst)     data     (Store {fe.TypeUintptr()} dst itab mem))
+	// result: (Store {types.BytePtr}     (OffPtr <types.BytePtr.PtrTo()> [config.PtrSize] dst)     data     (Store {types.Uintptr} dst itab mem))
 	for {
 		dst := v.Args[0]
 		v_1 := v.Args[1]
@@ -435,14 +435,14 @@ func rewriteValuedec_OpStore(v *Value) bool {
 		data := v_1.Args[1]
 		mem := v.Args[2]
 		v.reset(OpStore)
-		v.Aux = fe.TypeBytePtr()
-		v0 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeBytePtr().PtrTo())
+		v.Aux = types.BytePtr
+		v0 := b.NewValue0(v.Pos, OpOffPtr, types.BytePtr.PtrTo())
 		v0.AuxInt = config.PtrSize
 		v0.AddArg(dst)
 		v.AddArg(v0)
 		v.AddArg(data)
 		v1 := b.NewValue0(v.Pos, OpStore, TypeMem)
-		v1.Aux = fe.TypeUintptr()
+		v1.Aux = types.Uintptr
 		v1.AddArg(dst)
 		v1.AddArg(itab)
 		v1.AddArg(mem)
@@ -490,6 +490,8 @@ func rewriteBlockdec(b *Block) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &config.Types
+	_ = types
 	switch b.Kind {
 	}
 	return false
diff --git a/src/cmd/compile/internal/ssa/rewritedec64.go b/src/cmd/compile/internal/ssa/rewritedec64.go
index 97e4db0fe3..610018c9b6 100644
--- a/src/cmd/compile/internal/ssa/rewritedec64.go
+++ b/src/cmd/compile/internal/ssa/rewritedec64.go
@@ -128,39 +128,39 @@ func rewriteValuedec64(v *Value) bool {
 func rewriteValuedec64_OpAdd64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Add64 x y)
 	// cond:
-	// result: (Int64Make 		(Add32withcarry <fe.TypeInt32()> 			(Int64Hi x) 			(Int64Hi y) 			(Select1 <TypeFlags> (Add32carry (Int64Lo x) (Int64Lo y)))) 		(Select0 <fe.TypeUInt32()> (Add32carry (Int64Lo x) (Int64Lo y))))
+	// result: (Int64Make 		(Add32withcarry <types.Int32> 			(Int64Hi x) 			(Int64Hi y) 			(Select1 <TypeFlags> (Add32carry (Int64Lo x) (Int64Lo y)))) 		(Select0 <types.UInt32> (Add32carry (Int64Lo x) (Int64Lo y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpAdd32withcarry, fe.TypeInt32())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpAdd32withcarry, types.Int32)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpSelect1, TypeFlags)
-		v4 := b.NewValue0(v.Pos, OpAdd32carry, MakeTuple(fe.TypeUInt32(), TypeFlags))
-		v5 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpAdd32carry, MakeTuple(types.UInt32, TypeFlags))
+		v5 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v3.AddArg(v4)
 		v0.AddArg(v3)
 		v.AddArg(v0)
-		v7 := b.NewValue0(v.Pos, OpSelect0, fe.TypeUInt32())
-		v8 := b.NewValue0(v.Pos, OpAdd32carry, MakeTuple(fe.TypeUInt32(), TypeFlags))
-		v9 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpSelect0, types.UInt32)
+		v8 := b.NewValue0(v.Pos, OpAdd32carry, MakeTuple(types.UInt32, TypeFlags))
+		v9 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v9.AddArg(x)
 		v8.AddArg(v9)
-		v10 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v10 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v10.AddArg(y)
 		v8.AddArg(v10)
 		v7.AddArg(v8)
@@ -171,28 +171,28 @@ func rewriteValuedec64_OpAdd64(v *Value) bool {
 func rewriteValuedec64_OpAnd64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (And64 x y)
 	// cond:
-	// result: (Int64Make 		(And32 <fe.TypeUInt32()> (Int64Hi x) (Int64Hi y)) 		(And32 <fe.TypeUInt32()> (Int64Lo x) (Int64Lo y)))
+	// result: (Int64Make 		(And32 <types.UInt32> (Int64Hi x) (Int64Hi y)) 		(And32 <types.UInt32> (Int64Lo x) (Int64Lo y)))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpAnd32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpAnd32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpAnd32, fe.TypeUInt32())
-		v4 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpAnd32, types.UInt32)
+		v4 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v4.AddArg(x)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v5.AddArg(y)
 		v3.AddArg(v5)
 		v.AddArg(v3)
@@ -204,11 +204,11 @@ func rewriteValuedec64_OpArg(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Arg {n} [off])
 	// cond: is64BitInt(v.Type) && !config.BigEndian && v.Type.IsSigned()
-	// result: (Int64Make     (Arg <fe.TypeInt32()> {n} [off+4])     (Arg <fe.TypeUInt32()> {n} [off]))
+	// result: (Int64Make     (Arg <types.Int32> {n} [off+4])     (Arg <types.UInt32> {n} [off]))
 	for {
 		off := v.AuxInt
 		n := v.Aux
@@ -216,11 +216,11 @@ func rewriteValuedec64_OpArg(v *Value) bool {
 			break
 		}
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpArg, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpArg, types.Int32)
 		v0.AuxInt = off + 4
 		v0.Aux = n
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpArg, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpArg, types.UInt32)
 		v1.AuxInt = off
 		v1.Aux = n
 		v.AddArg(v1)
@@ -228,7 +228,7 @@ func rewriteValuedec64_OpArg(v *Value) bool {
 	}
 	// match: (Arg {n} [off])
 	// cond: is64BitInt(v.Type) && !config.BigEndian && !v.Type.IsSigned()
-	// result: (Int64Make     (Arg <fe.TypeUInt32()> {n} [off+4])     (Arg <fe.TypeUInt32()> {n} [off]))
+	// result: (Int64Make     (Arg <types.UInt32> {n} [off+4])     (Arg <types.UInt32> {n} [off]))
 	for {
 		off := v.AuxInt
 		n := v.Aux
@@ -236,11 +236,11 @@ func rewriteValuedec64_OpArg(v *Value) bool {
 			break
 		}
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpArg, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpArg, types.UInt32)
 		v0.AuxInt = off + 4
 		v0.Aux = n
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpArg, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpArg, types.UInt32)
 		v1.AuxInt = off
 		v1.Aux = n
 		v.AddArg(v1)
@@ -248,7 +248,7 @@ func rewriteValuedec64_OpArg(v *Value) bool {
 	}
 	// match: (Arg {n} [off])
 	// cond: is64BitInt(v.Type) && config.BigEndian && v.Type.IsSigned()
-	// result: (Int64Make     (Arg <fe.TypeInt32()> {n} [off])     (Arg <fe.TypeUInt32()> {n} [off+4]))
+	// result: (Int64Make     (Arg <types.Int32> {n} [off])     (Arg <types.UInt32> {n} [off+4]))
 	for {
 		off := v.AuxInt
 		n := v.Aux
@@ -256,11 +256,11 @@ func rewriteValuedec64_OpArg(v *Value) bool {
 			break
 		}
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpArg, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpArg, types.Int32)
 		v0.AuxInt = off
 		v0.Aux = n
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpArg, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpArg, types.UInt32)
 		v1.AuxInt = off + 4
 		v1.Aux = n
 		v.AddArg(v1)
@@ -268,7 +268,7 @@ func rewriteValuedec64_OpArg(v *Value) bool {
 	}
 	// match: (Arg {n} [off])
 	// cond: is64BitInt(v.Type) && config.BigEndian && !v.Type.IsSigned()
-	// result: (Int64Make     (Arg <fe.TypeUInt32()> {n} [off])     (Arg <fe.TypeUInt32()> {n} [off+4]))
+	// result: (Int64Make     (Arg <types.UInt32> {n} [off])     (Arg <types.UInt32> {n} [off+4]))
 	for {
 		off := v.AuxInt
 		n := v.Aux
@@ -276,11 +276,11 @@ func rewriteValuedec64_OpArg(v *Value) bool {
 			break
 		}
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpArg, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpArg, types.UInt32)
 		v0.AuxInt = off
 		v0.Aux = n
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpArg, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpArg, types.UInt32)
 		v1.AuxInt = off + 4
 		v1.Aux = n
 		v.AddArg(v1)
@@ -291,27 +291,27 @@ func rewriteValuedec64_OpArg(v *Value) bool {
 func rewriteValuedec64_OpBitLen64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (BitLen64 x)
 	// cond:
-	// result: (Add32 <fe.TypeInt()> 		(BitLen32 <fe.TypeInt()> (Int64Hi x)) 		(BitLen32 <fe.TypeInt()> 			(Or32 <fe.TypeUInt32()> 				(Int64Lo x) 				(Zeromask (Int64Hi x)))))
+	// result: (Add32 <types.Int> 		(BitLen32 <types.Int> (Int64Hi x)) 		(BitLen32 <types.Int> 			(Or32 <types.UInt32> 				(Int64Lo x) 				(Zeromask (Int64Hi x)))))
 	for {
 		x := v.Args[0]
 		v.reset(OpAdd32)
-		v.Type = fe.TypeInt()
-		v0 := b.NewValue0(v.Pos, OpBitLen32, fe.TypeInt())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v.Type = types.Int
+		v0 := b.NewValue0(v.Pos, OpBitLen32, types.Int)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpBitLen32, fe.TypeInt())
-		v3 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v4 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpBitLen32, types.Int)
+		v3 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v4 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v4.AddArg(x)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
-		v6 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
+		v6 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v6.AddArg(x)
 		v5.AddArg(v6)
 		v3.AddArg(v5)
@@ -323,21 +323,21 @@ func rewriteValuedec64_OpBitLen64(v *Value) bool {
 func rewriteValuedec64_OpBswap64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Bswap64 x)
 	// cond:
-	// result: (Int64Make 		(Bswap32 <fe.TypeUInt32()> (Int64Lo x)) 		(Bswap32 <fe.TypeUInt32()> (Int64Hi x)))
+	// result: (Int64Make 		(Bswap32 <types.UInt32> (Int64Lo x)) 		(Bswap32 <types.UInt32> (Int64Hi x)))
 	for {
 		x := v.Args[0]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpBswap32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpBswap32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpBswap32, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpBswap32, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v3.AddArg(x)
 		v2.AddArg(v3)
 		v.AddArg(v2)
@@ -347,21 +347,21 @@ func rewriteValuedec64_OpBswap64(v *Value) bool {
 func rewriteValuedec64_OpCom64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Com64 x)
 	// cond:
-	// result: (Int64Make 		(Com32 <fe.TypeUInt32()> (Int64Hi x)) 		(Com32 <fe.TypeUInt32()> (Int64Lo x)))
+	// result: (Int64Make 		(Com32 <types.UInt32> (Int64Hi x)) 		(Com32 <types.UInt32> (Int64Lo x)))
 	for {
 		x := v.Args[0]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpCom32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpCom32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpCom32, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpCom32, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v3.AddArg(x)
 		v2.AddArg(v3)
 		v.AddArg(v2)
@@ -371,11 +371,11 @@ func rewriteValuedec64_OpCom64(v *Value) bool {
 func rewriteValuedec64_OpConst64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Const64 <t> [c])
 	// cond: t.IsSigned()
-	// result: (Int64Make (Const32 <fe.TypeInt32()> [c>>32]) (Const32 <fe.TypeUInt32()> [int64(int32(c))]))
+	// result: (Int64Make (Const32 <types.Int32> [c>>32]) (Const32 <types.UInt32> [int64(int32(c))]))
 	for {
 		t := v.Type
 		c := v.AuxInt
@@ -383,17 +383,17 @@ func rewriteValuedec64_OpConst64(v *Value) bool {
 			break
 		}
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpConst32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpConst32, types.Int32)
 		v0.AuxInt = c >> 32
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v1.AuxInt = int64(int32(c))
 		v.AddArg(v1)
 		return true
 	}
 	// match: (Const64 <t> [c])
 	// cond: !t.IsSigned()
-	// result: (Int64Make (Const32 <fe.TypeUInt32()> [c>>32]) (Const32 <fe.TypeUInt32()> [int64(int32(c))]))
+	// result: (Int64Make (Const32 <types.UInt32> [c>>32]) (Const32 <types.UInt32> [int64(int32(c))]))
 	for {
 		t := v.Type
 		c := v.AuxInt
@@ -401,10 +401,10 @@ func rewriteValuedec64_OpConst64(v *Value) bool {
 			break
 		}
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v0.AuxInt = c >> 32
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v1.AuxInt = int64(int32(c))
 		v.AddArg(v1)
 		return true
@@ -414,30 +414,30 @@ func rewriteValuedec64_OpConst64(v *Value) bool {
 func rewriteValuedec64_OpCtz64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Ctz64 x)
 	// cond:
-	// result: (Add32 <fe.TypeUInt32()> 		(Ctz32 <fe.TypeUInt32()> (Int64Lo x)) 		(And32 <fe.TypeUInt32()> 			(Com32 <fe.TypeUInt32()> (Zeromask (Int64Lo x))) 			(Ctz32 <fe.TypeUInt32()> (Int64Hi x))))
+	// result: (Add32 <types.UInt32> 		(Ctz32 <types.UInt32> (Int64Lo x)) 		(And32 <types.UInt32> 			(Com32 <types.UInt32> (Zeromask (Int64Lo x))) 			(Ctz32 <types.UInt32> (Int64Hi x))))
 	for {
 		x := v.Args[0]
 		v.reset(OpAdd32)
-		v.Type = fe.TypeUInt32()
-		v0 := b.NewValue0(v.Pos, OpCtz32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v.Type = types.UInt32
+		v0 := b.NewValue0(v.Pos, OpCtz32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpAnd32, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpCom32, fe.TypeUInt32())
-		v4 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
-		v5 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpAnd32, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpCom32, types.UInt32)
+		v4 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
+		v5 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
 		v3.AddArg(v4)
 		v2.AddArg(v3)
-		v6 := b.NewValue0(v.Pos, OpCtz32, fe.TypeUInt32())
-		v7 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpCtz32, types.UInt32)
+		v7 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v7.AddArg(x)
 		v6.AddArg(v7)
 		v2.AddArg(v6)
@@ -448,8 +448,8 @@ func rewriteValuedec64_OpCtz64(v *Value) bool {
 func rewriteValuedec64_OpEq64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Eq64 x y)
 	// cond:
 	// result: (AndB 		(Eq32 (Int64Hi x) (Int64Hi y)) 		(Eq32 (Int64Lo x) (Int64Lo y)))
@@ -457,19 +457,19 @@ func rewriteValuedec64_OpEq64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpAndB)
-		v0 := b.NewValue0(v.Pos, OpEq32, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpEq32, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpEq32, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpEq32, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v4.AddArg(x)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v5.AddArg(y)
 		v3.AddArg(v5)
 		v.AddArg(v3)
@@ -479,8 +479,8 @@ func rewriteValuedec64_OpEq64(v *Value) bool {
 func rewriteValuedec64_OpGeq64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq64 x y)
 	// cond:
 	// result: (OrB 		(Greater32 (Int64Hi x) (Int64Hi y)) 		(AndB 			(Eq32 (Int64Hi x) (Int64Hi y)) 			(Geq32U (Int64Lo x) (Int64Lo y))))
@@ -488,28 +488,28 @@ func rewriteValuedec64_OpGeq64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpOrB)
-		v0 := b.NewValue0(v.Pos, OpGreater32, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpGreater32, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpAndB, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpEq32, fe.TypeBool())
-		v5 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpAndB, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpEq32, types.Bool)
+		v5 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v3.AddArg(v4)
-		v7 := b.NewValue0(v.Pos, OpGeq32U, fe.TypeBool())
-		v8 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpGeq32U, types.Bool)
+		v8 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v8.AddArg(x)
 		v7.AddArg(v8)
-		v9 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v9.AddArg(y)
 		v7.AddArg(v9)
 		v3.AddArg(v7)
@@ -520,8 +520,8 @@ func rewriteValuedec64_OpGeq64(v *Value) bool {
 func rewriteValuedec64_OpGeq64U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Geq64U x y)
 	// cond:
 	// result: (OrB 		(Greater32U (Int64Hi x) (Int64Hi y)) 		(AndB 			(Eq32 (Int64Hi x) (Int64Hi y)) 			(Geq32U (Int64Lo x) (Int64Lo y))))
@@ -529,28 +529,28 @@ func rewriteValuedec64_OpGeq64U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpOrB)
-		v0 := b.NewValue0(v.Pos, OpGreater32U, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpGreater32U, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpAndB, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpEq32, fe.TypeBool())
-		v5 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpAndB, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpEq32, types.Bool)
+		v5 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v3.AddArg(v4)
-		v7 := b.NewValue0(v.Pos, OpGeq32U, fe.TypeBool())
-		v8 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpGeq32U, types.Bool)
+		v8 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v8.AddArg(x)
 		v7.AddArg(v8)
-		v9 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v9.AddArg(y)
 		v7.AddArg(v9)
 		v3.AddArg(v7)
@@ -561,8 +561,8 @@ func rewriteValuedec64_OpGeq64U(v *Value) bool {
 func rewriteValuedec64_OpGreater64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater64 x y)
 	// cond:
 	// result: (OrB 		(Greater32 (Int64Hi x) (Int64Hi y)) 		(AndB 			(Eq32 (Int64Hi x) (Int64Hi y)) 			(Greater32U (Int64Lo x) (Int64Lo y))))
@@ -570,28 +570,28 @@ func rewriteValuedec64_OpGreater64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpOrB)
-		v0 := b.NewValue0(v.Pos, OpGreater32, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpGreater32, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpAndB, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpEq32, fe.TypeBool())
-		v5 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpAndB, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpEq32, types.Bool)
+		v5 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v3.AddArg(v4)
-		v7 := b.NewValue0(v.Pos, OpGreater32U, fe.TypeBool())
-		v8 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpGreater32U, types.Bool)
+		v8 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v8.AddArg(x)
 		v7.AddArg(v8)
-		v9 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v9.AddArg(y)
 		v7.AddArg(v9)
 		v3.AddArg(v7)
@@ -602,8 +602,8 @@ func rewriteValuedec64_OpGreater64(v *Value) bool {
 func rewriteValuedec64_OpGreater64U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Greater64U x y)
 	// cond:
 	// result: (OrB 		(Greater32U (Int64Hi x) (Int64Hi y)) 		(AndB 			(Eq32 (Int64Hi x) (Int64Hi y)) 			(Greater32U (Int64Lo x) (Int64Lo y))))
@@ -611,28 +611,28 @@ func rewriteValuedec64_OpGreater64U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpOrB)
-		v0 := b.NewValue0(v.Pos, OpGreater32U, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpGreater32U, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpAndB, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpEq32, fe.TypeBool())
-		v5 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpAndB, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpEq32, types.Bool)
+		v5 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v3.AddArg(v4)
-		v7 := b.NewValue0(v.Pos, OpGreater32U, fe.TypeBool())
-		v8 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpGreater32U, types.Bool)
+		v8 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v8.AddArg(x)
 		v7.AddArg(v8)
-		v9 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v9.AddArg(y)
 		v7.AddArg(v9)
 		v3.AddArg(v7)
@@ -677,8 +677,8 @@ func rewriteValuedec64_OpInt64Lo(v *Value) bool {
 func rewriteValuedec64_OpLeq64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq64 x y)
 	// cond:
 	// result: (OrB 		(Less32 (Int64Hi x) (Int64Hi y)) 		(AndB 			(Eq32 (Int64Hi x) (Int64Hi y)) 			(Leq32U (Int64Lo x) (Int64Lo y))))
@@ -686,28 +686,28 @@ func rewriteValuedec64_OpLeq64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpOrB)
-		v0 := b.NewValue0(v.Pos, OpLess32, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpLess32, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpAndB, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpEq32, fe.TypeBool())
-		v5 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpAndB, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpEq32, types.Bool)
+		v5 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v3.AddArg(v4)
-		v7 := b.NewValue0(v.Pos, OpLeq32U, fe.TypeBool())
-		v8 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpLeq32U, types.Bool)
+		v8 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v8.AddArg(x)
 		v7.AddArg(v8)
-		v9 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v9.AddArg(y)
 		v7.AddArg(v9)
 		v3.AddArg(v7)
@@ -718,8 +718,8 @@ func rewriteValuedec64_OpLeq64(v *Value) bool {
 func rewriteValuedec64_OpLeq64U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Leq64U x y)
 	// cond:
 	// result: (OrB 		(Less32U (Int64Hi x) (Int64Hi y)) 		(AndB 			(Eq32 (Int64Hi x) (Int64Hi y)) 			(Leq32U (Int64Lo x) (Int64Lo y))))
@@ -727,28 +727,28 @@ func rewriteValuedec64_OpLeq64U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpOrB)
-		v0 := b.NewValue0(v.Pos, OpLess32U, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpLess32U, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpAndB, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpEq32, fe.TypeBool())
-		v5 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpAndB, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpEq32, types.Bool)
+		v5 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v3.AddArg(v4)
-		v7 := b.NewValue0(v.Pos, OpLeq32U, fe.TypeBool())
-		v8 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpLeq32U, types.Bool)
+		v8 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v8.AddArg(x)
 		v7.AddArg(v8)
-		v9 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v9.AddArg(y)
 		v7.AddArg(v9)
 		v3.AddArg(v7)
@@ -759,8 +759,8 @@ func rewriteValuedec64_OpLeq64U(v *Value) bool {
 func rewriteValuedec64_OpLess64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less64 x y)
 	// cond:
 	// result: (OrB 		(Less32 (Int64Hi x) (Int64Hi y)) 		(AndB 			(Eq32 (Int64Hi x) (Int64Hi y)) 			(Less32U (Int64Lo x) (Int64Lo y))))
@@ -768,28 +768,28 @@ func rewriteValuedec64_OpLess64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpOrB)
-		v0 := b.NewValue0(v.Pos, OpLess32, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpLess32, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpAndB, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpEq32, fe.TypeBool())
-		v5 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpAndB, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpEq32, types.Bool)
+		v5 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v3.AddArg(v4)
-		v7 := b.NewValue0(v.Pos, OpLess32U, fe.TypeBool())
-		v8 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpLess32U, types.Bool)
+		v8 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v8.AddArg(x)
 		v7.AddArg(v8)
-		v9 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v9.AddArg(y)
 		v7.AddArg(v9)
 		v3.AddArg(v7)
@@ -800,8 +800,8 @@ func rewriteValuedec64_OpLess64(v *Value) bool {
 func rewriteValuedec64_OpLess64U(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Less64U x y)
 	// cond:
 	// result: (OrB 		(Less32U (Int64Hi x) (Int64Hi y)) 		(AndB 			(Eq32 (Int64Hi x) (Int64Hi y)) 			(Less32U (Int64Lo x) (Int64Lo y))))
@@ -809,28 +809,28 @@ func rewriteValuedec64_OpLess64U(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpOrB)
-		v0 := b.NewValue0(v.Pos, OpLess32U, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpLess32U, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpAndB, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpEq32, fe.TypeBool())
-		v5 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpAndB, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpEq32, types.Bool)
+		v5 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v3.AddArg(v4)
-		v7 := b.NewValue0(v.Pos, OpLess32U, fe.TypeBool())
-		v8 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpLess32U, types.Bool)
+		v8 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v8.AddArg(x)
 		v7.AddArg(v8)
-		v9 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v9.AddArg(y)
 		v7.AddArg(v9)
 		v3.AddArg(v7)
@@ -843,11 +843,11 @@ func rewriteValuedec64_OpLoad(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Load <t> ptr mem)
 	// cond: is64BitInt(t) && !config.BigEndian && t.IsSigned()
-	// result: (Int64Make 		(Load <fe.TypeInt32()> (OffPtr <fe.TypeInt32().PtrTo()> [4] ptr) mem) 		(Load <fe.TypeUInt32()> ptr mem))
+	// result: (Int64Make 		(Load <types.Int32> (OffPtr <types.Int32.PtrTo()> [4] ptr) mem) 		(Load <types.UInt32> ptr mem))
 	for {
 		t := v.Type
 		ptr := v.Args[0]
@@ -856,14 +856,14 @@ func rewriteValuedec64_OpLoad(v *Value) bool {
 			break
 		}
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpLoad, fe.TypeInt32())
-		v1 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeInt32().PtrTo())
+		v0 := b.NewValue0(v.Pos, OpLoad, types.Int32)
+		v1 := b.NewValue0(v.Pos, OpOffPtr, types.Int32.PtrTo())
 		v1.AuxInt = 4
 		v1.AddArg(ptr)
 		v0.AddArg(v1)
 		v0.AddArg(mem)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpLoad, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpLoad, types.UInt32)
 		v2.AddArg(ptr)
 		v2.AddArg(mem)
 		v.AddArg(v2)
@@ -871,7 +871,7 @@ func rewriteValuedec64_OpLoad(v *Value) bool {
 	}
 	// match: (Load <t> ptr mem)
 	// cond: is64BitInt(t) && !config.BigEndian && !t.IsSigned()
-	// result: (Int64Make 		(Load <fe.TypeUInt32()> (OffPtr <fe.TypeUInt32().PtrTo()> [4] ptr) mem) 		(Load <fe.TypeUInt32()> ptr mem))
+	// result: (Int64Make 		(Load <types.UInt32> (OffPtr <types.UInt32.PtrTo()> [4] ptr) mem) 		(Load <types.UInt32> ptr mem))
 	for {
 		t := v.Type
 		ptr := v.Args[0]
@@ -880,14 +880,14 @@ func rewriteValuedec64_OpLoad(v *Value) bool {
 			break
 		}
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpLoad, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeUInt32().PtrTo())
+		v0 := b.NewValue0(v.Pos, OpLoad, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpOffPtr, types.UInt32.PtrTo())
 		v1.AuxInt = 4
 		v1.AddArg(ptr)
 		v0.AddArg(v1)
 		v0.AddArg(mem)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpLoad, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpLoad, types.UInt32)
 		v2.AddArg(ptr)
 		v2.AddArg(mem)
 		v.AddArg(v2)
@@ -895,7 +895,7 @@ func rewriteValuedec64_OpLoad(v *Value) bool {
 	}
 	// match: (Load <t> ptr mem)
 	// cond: is64BitInt(t) && config.BigEndian && t.IsSigned()
-	// result: (Int64Make 		(Load <fe.TypeInt32()> ptr mem) 		(Load <fe.TypeUInt32()> (OffPtr <fe.TypeUInt32().PtrTo()> [4] ptr) mem))
+	// result: (Int64Make 		(Load <types.Int32> ptr mem) 		(Load <types.UInt32> (OffPtr <types.UInt32.PtrTo()> [4] ptr) mem))
 	for {
 		t := v.Type
 		ptr := v.Args[0]
@@ -904,12 +904,12 @@ func rewriteValuedec64_OpLoad(v *Value) bool {
 			break
 		}
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpLoad, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpLoad, types.Int32)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpLoad, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeUInt32().PtrTo())
+		v1 := b.NewValue0(v.Pos, OpLoad, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpOffPtr, types.UInt32.PtrTo())
 		v2.AuxInt = 4
 		v2.AddArg(ptr)
 		v1.AddArg(v2)
@@ -919,7 +919,7 @@ func rewriteValuedec64_OpLoad(v *Value) bool {
 	}
 	// match: (Load <t> ptr mem)
 	// cond: is64BitInt(t) && config.BigEndian && !t.IsSigned()
-	// result: (Int64Make 		(Load <fe.TypeUInt32()> ptr mem) 		(Load <fe.TypeUInt32()> (OffPtr <fe.TypeUInt32().PtrTo()> [4] ptr) mem))
+	// result: (Int64Make 		(Load <types.UInt32> ptr mem) 		(Load <types.UInt32> (OffPtr <types.UInt32.PtrTo()> [4] ptr) mem))
 	for {
 		t := v.Type
 		ptr := v.Args[0]
@@ -928,12 +928,12 @@ func rewriteValuedec64_OpLoad(v *Value) bool {
 			break
 		}
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpLoad, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpLoad, types.UInt32)
 		v0.AddArg(ptr)
 		v0.AddArg(mem)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpLoad, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpOffPtr, fe.TypeUInt32().PtrTo())
+		v1 := b.NewValue0(v.Pos, OpLoad, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpOffPtr, types.UInt32.PtrTo())
 		v2.AuxInt = 4
 		v2.AddArg(ptr)
 		v1.AddArg(v2)
@@ -946,8 +946,8 @@ func rewriteValuedec64_OpLoad(v *Value) bool {
 func rewriteValuedec64_OpLsh16x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x64 _ (Int64Make (Const32 [c]) _))
 	// cond: c != 0
 	// result: (Const32 [0])
@@ -992,7 +992,7 @@ func rewriteValuedec64_OpLsh16x64(v *Value) bool {
 	}
 	// match: (Lsh16x64 x (Int64Make hi lo))
 	// cond: hi.Op != OpConst32
-	// result: (Lsh16x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	// result: (Lsh16x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -1006,8 +1006,8 @@ func rewriteValuedec64_OpLsh16x64(v *Value) bool {
 		}
 		v.reset(OpLsh16x32)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
 		v1.AddArg(hi)
 		v0.AddArg(v1)
 		v0.AddArg(lo)
@@ -1019,8 +1019,8 @@ func rewriteValuedec64_OpLsh16x64(v *Value) bool {
 func rewriteValuedec64_OpLsh32x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x64 _ (Int64Make (Const32 [c]) _))
 	// cond: c != 0
 	// result: (Const32 [0])
@@ -1065,7 +1065,7 @@ func rewriteValuedec64_OpLsh32x64(v *Value) bool {
 	}
 	// match: (Lsh32x64 x (Int64Make hi lo))
 	// cond: hi.Op != OpConst32
-	// result: (Lsh32x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	// result: (Lsh32x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -1079,8 +1079,8 @@ func rewriteValuedec64_OpLsh32x64(v *Value) bool {
 		}
 		v.reset(OpLsh32x32)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
 		v1.AddArg(hi)
 		v0.AddArg(v1)
 		v0.AddArg(lo)
@@ -1092,11 +1092,11 @@ func rewriteValuedec64_OpLsh32x64(v *Value) bool {
 func rewriteValuedec64_OpLsh64x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x16 (Int64Make hi lo) s)
 	// cond:
-	// result: (Int64Make 		(Or32 <fe.TypeUInt32()> 			(Or32 <fe.TypeUInt32()> 				(Lsh32x16 <fe.TypeUInt32()> hi s) 				(Rsh32Ux16 <fe.TypeUInt32()> 					lo 					(Sub16 <fe.TypeUInt16()> (Const16 <fe.TypeUInt16()> [32]) s))) 			(Lsh32x16 <fe.TypeUInt32()> 				lo 				(Sub16 <fe.TypeUInt16()> s (Const16 <fe.TypeUInt16()> [32])))) 		(Lsh32x16 <fe.TypeUInt32()> lo s))
+	// result: (Int64Make 		(Or32 <types.UInt32> 			(Or32 <types.UInt32> 				(Lsh32x16 <types.UInt32> hi s) 				(Rsh32Ux16 <types.UInt32> 					lo 					(Sub16 <types.UInt16> (Const16 <types.UInt16> [32]) s))) 			(Lsh32x16 <types.UInt32> 				lo 				(Sub16 <types.UInt16> s (Const16 <types.UInt16> [32])))) 		(Lsh32x16 <types.UInt32> lo s))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpInt64Make {
@@ -1106,33 +1106,33 @@ func rewriteValuedec64_OpLsh64x16(v *Value) bool {
 		lo := v_0.Args[1]
 		s := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpLsh32x16, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpLsh32x16, types.UInt32)
 		v2.AddArg(hi)
 		v2.AddArg(s)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpRsh32Ux16, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpRsh32Ux16, types.UInt32)
 		v3.AddArg(lo)
-		v4 := b.NewValue0(v.Pos, OpSub16, fe.TypeUInt16())
-		v5 := b.NewValue0(v.Pos, OpConst16, fe.TypeUInt16())
+		v4 := b.NewValue0(v.Pos, OpSub16, types.UInt16)
+		v5 := b.NewValue0(v.Pos, OpConst16, types.UInt16)
 		v5.AuxInt = 32
 		v4.AddArg(v5)
 		v4.AddArg(s)
 		v3.AddArg(v4)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v6 := b.NewValue0(v.Pos, OpLsh32x16, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpLsh32x16, types.UInt32)
 		v6.AddArg(lo)
-		v7 := b.NewValue0(v.Pos, OpSub16, fe.TypeUInt16())
+		v7 := b.NewValue0(v.Pos, OpSub16, types.UInt16)
 		v7.AddArg(s)
-		v8 := b.NewValue0(v.Pos, OpConst16, fe.TypeUInt16())
+		v8 := b.NewValue0(v.Pos, OpConst16, types.UInt16)
 		v8.AuxInt = 32
 		v7.AddArg(v8)
 		v6.AddArg(v7)
 		v0.AddArg(v6)
 		v.AddArg(v0)
-		v9 := b.NewValue0(v.Pos, OpLsh32x16, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpLsh32x16, types.UInt32)
 		v9.AddArg(lo)
 		v9.AddArg(s)
 		v.AddArg(v9)
@@ -1143,11 +1143,11 @@ func rewriteValuedec64_OpLsh64x16(v *Value) bool {
 func rewriteValuedec64_OpLsh64x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x32 (Int64Make hi lo) s)
 	// cond:
-	// result: (Int64Make 		(Or32 <fe.TypeUInt32()> 			(Or32 <fe.TypeUInt32()> 				(Lsh32x32 <fe.TypeUInt32()> hi s) 				(Rsh32Ux32 <fe.TypeUInt32()> 					lo 					(Sub32 <fe.TypeUInt32()> (Const32 <fe.TypeUInt32()> [32]) s))) 			(Lsh32x32 <fe.TypeUInt32()> 				lo 				(Sub32 <fe.TypeUInt32()> s (Const32 <fe.TypeUInt32()> [32])))) 		(Lsh32x32 <fe.TypeUInt32()> lo s))
+	// result: (Int64Make 		(Or32 <types.UInt32> 			(Or32 <types.UInt32> 				(Lsh32x32 <types.UInt32> hi s) 				(Rsh32Ux32 <types.UInt32> 					lo 					(Sub32 <types.UInt32> (Const32 <types.UInt32> [32]) s))) 			(Lsh32x32 <types.UInt32> 				lo 				(Sub32 <types.UInt32> s (Const32 <types.UInt32> [32])))) 		(Lsh32x32 <types.UInt32> lo s))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpInt64Make {
@@ -1157,33 +1157,33 @@ func rewriteValuedec64_OpLsh64x32(v *Value) bool {
 		lo := v_0.Args[1]
 		s := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpLsh32x32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpLsh32x32, types.UInt32)
 		v2.AddArg(hi)
 		v2.AddArg(s)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpRsh32Ux32, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpRsh32Ux32, types.UInt32)
 		v3.AddArg(lo)
-		v4 := b.NewValue0(v.Pos, OpSub32, fe.TypeUInt32())
-		v5 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpSub32, types.UInt32)
+		v5 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v5.AuxInt = 32
 		v4.AddArg(v5)
 		v4.AddArg(s)
 		v3.AddArg(v4)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v6 := b.NewValue0(v.Pos, OpLsh32x32, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpLsh32x32, types.UInt32)
 		v6.AddArg(lo)
-		v7 := b.NewValue0(v.Pos, OpSub32, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpSub32, types.UInt32)
 		v7.AddArg(s)
-		v8 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v8 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v8.AuxInt = 32
 		v7.AddArg(v8)
 		v6.AddArg(v7)
 		v0.AddArg(v6)
 		v.AddArg(v0)
-		v9 := b.NewValue0(v.Pos, OpLsh32x32, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpLsh32x32, types.UInt32)
 		v9.AddArg(lo)
 		v9.AddArg(s)
 		v.AddArg(v9)
@@ -1194,8 +1194,8 @@ func rewriteValuedec64_OpLsh64x32(v *Value) bool {
 func rewriteValuedec64_OpLsh64x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x64 _ (Int64Make (Const32 [c]) _))
 	// cond: c != 0
 	// result: (Const64 [0])
@@ -1240,7 +1240,7 @@ func rewriteValuedec64_OpLsh64x64(v *Value) bool {
 	}
 	// match: (Lsh64x64 x (Int64Make hi lo))
 	// cond: hi.Op != OpConst32
-	// result: (Lsh64x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	// result: (Lsh64x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -1254,8 +1254,8 @@ func rewriteValuedec64_OpLsh64x64(v *Value) bool {
 		}
 		v.reset(OpLsh64x32)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
 		v1.AddArg(hi)
 		v0.AddArg(v1)
 		v0.AddArg(lo)
@@ -1267,11 +1267,11 @@ func rewriteValuedec64_OpLsh64x64(v *Value) bool {
 func rewriteValuedec64_OpLsh64x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x8 (Int64Make hi lo) s)
 	// cond:
-	// result: (Int64Make 		(Or32 <fe.TypeUInt32()> 			(Or32 <fe.TypeUInt32()> 				(Lsh32x8 <fe.TypeUInt32()> hi s) 				(Rsh32Ux8 <fe.TypeUInt32()> 					lo 					(Sub8 <fe.TypeUInt8()> (Const8 <fe.TypeUInt8()> [32]) s))) 			(Lsh32x8 <fe.TypeUInt32()> 				lo 				(Sub8 <fe.TypeUInt8()> s (Const8 <fe.TypeUInt8()> [32])))) 		(Lsh32x8 <fe.TypeUInt32()> lo s))
+	// result: (Int64Make 		(Or32 <types.UInt32> 			(Or32 <types.UInt32> 				(Lsh32x8 <types.UInt32> hi s) 				(Rsh32Ux8 <types.UInt32> 					lo 					(Sub8 <types.UInt8> (Const8 <types.UInt8> [32]) s))) 			(Lsh32x8 <types.UInt32> 				lo 				(Sub8 <types.UInt8> s (Const8 <types.UInt8> [32])))) 		(Lsh32x8 <types.UInt32> lo s))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpInt64Make {
@@ -1281,33 +1281,33 @@ func rewriteValuedec64_OpLsh64x8(v *Value) bool {
 		lo := v_0.Args[1]
 		s := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpLsh32x8, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpLsh32x8, types.UInt32)
 		v2.AddArg(hi)
 		v2.AddArg(s)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpRsh32Ux8, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpRsh32Ux8, types.UInt32)
 		v3.AddArg(lo)
-		v4 := b.NewValue0(v.Pos, OpSub8, fe.TypeUInt8())
-		v5 := b.NewValue0(v.Pos, OpConst8, fe.TypeUInt8())
+		v4 := b.NewValue0(v.Pos, OpSub8, types.UInt8)
+		v5 := b.NewValue0(v.Pos, OpConst8, types.UInt8)
 		v5.AuxInt = 32
 		v4.AddArg(v5)
 		v4.AddArg(s)
 		v3.AddArg(v4)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v6 := b.NewValue0(v.Pos, OpLsh32x8, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpLsh32x8, types.UInt32)
 		v6.AddArg(lo)
-		v7 := b.NewValue0(v.Pos, OpSub8, fe.TypeUInt8())
+		v7 := b.NewValue0(v.Pos, OpSub8, types.UInt8)
 		v7.AddArg(s)
-		v8 := b.NewValue0(v.Pos, OpConst8, fe.TypeUInt8())
+		v8 := b.NewValue0(v.Pos, OpConst8, types.UInt8)
 		v8.AuxInt = 32
 		v7.AddArg(v8)
 		v6.AddArg(v7)
 		v0.AddArg(v6)
 		v.AddArg(v0)
-		v9 := b.NewValue0(v.Pos, OpLsh32x8, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpLsh32x8, types.UInt32)
 		v9.AddArg(lo)
 		v9.AddArg(s)
 		v.AddArg(v9)
@@ -1318,8 +1318,8 @@ func rewriteValuedec64_OpLsh64x8(v *Value) bool {
 func rewriteValuedec64_OpLsh8x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x64 _ (Int64Make (Const32 [c]) _))
 	// cond: c != 0
 	// result: (Const32 [0])
@@ -1364,7 +1364,7 @@ func rewriteValuedec64_OpLsh8x64(v *Value) bool {
 	}
 	// match: (Lsh8x64 x (Int64Make hi lo))
 	// cond: hi.Op != OpConst32
-	// result: (Lsh8x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	// result: (Lsh8x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -1378,8 +1378,8 @@ func rewriteValuedec64_OpLsh8x64(v *Value) bool {
 		}
 		v.reset(OpLsh8x32)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
 		v1.AddArg(hi)
 		v0.AddArg(v1)
 		v0.AddArg(lo)
@@ -1391,51 +1391,51 @@ func rewriteValuedec64_OpLsh8x64(v *Value) bool {
 func rewriteValuedec64_OpMul64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mul64 x y)
 	// cond:
-	// result: (Int64Make 		(Add32 <fe.TypeUInt32()> 			(Mul32 <fe.TypeUInt32()> (Int64Lo x) (Int64Hi y)) 			(Add32 <fe.TypeUInt32()> 				(Mul32 <fe.TypeUInt32()> (Int64Hi x) (Int64Lo y)) 				(Select0 <fe.TypeUInt32()> (Mul32uhilo (Int64Lo x) (Int64Lo y))))) 		(Select1 <fe.TypeUInt32()> (Mul32uhilo (Int64Lo x) (Int64Lo y))))
+	// result: (Int64Make 		(Add32 <types.UInt32> 			(Mul32 <types.UInt32> (Int64Lo x) (Int64Hi y)) 			(Add32 <types.UInt32> 				(Mul32 <types.UInt32> (Int64Hi x) (Int64Lo y)) 				(Select0 <types.UInt32> (Mul32uhilo (Int64Lo x) (Int64Lo y))))) 		(Select1 <types.UInt32> (Mul32uhilo (Int64Lo x) (Int64Lo y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpAdd32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpMul32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpAdd32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpMul32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v2.AddArg(x)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v3.AddArg(y)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v4 := b.NewValue0(v.Pos, OpAdd32, fe.TypeUInt32())
-		v5 := b.NewValue0(v.Pos, OpMul32, fe.TypeUInt32())
-		v6 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpAdd32, types.UInt32)
+		v5 := b.NewValue0(v.Pos, OpMul32, types.UInt32)
+		v6 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v6.AddArg(x)
 		v5.AddArg(v6)
-		v7 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v7.AddArg(y)
 		v5.AddArg(v7)
 		v4.AddArg(v5)
-		v8 := b.NewValue0(v.Pos, OpSelect0, fe.TypeUInt32())
-		v9 := b.NewValue0(v.Pos, OpMul32uhilo, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
-		v10 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v8 := b.NewValue0(v.Pos, OpSelect0, types.UInt32)
+		v9 := b.NewValue0(v.Pos, OpMul32uhilo, MakeTuple(types.UInt32, types.UInt32))
+		v10 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v10.AddArg(x)
 		v9.AddArg(v10)
-		v11 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v11 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v11.AddArg(y)
 		v9.AddArg(v11)
 		v8.AddArg(v9)
 		v4.AddArg(v8)
 		v0.AddArg(v4)
 		v.AddArg(v0)
-		v12 := b.NewValue0(v.Pos, OpSelect1, fe.TypeUInt32())
-		v13 := b.NewValue0(v.Pos, OpMul32uhilo, MakeTuple(fe.TypeUInt32(), fe.TypeUInt32()))
-		v14 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v12 := b.NewValue0(v.Pos, OpSelect1, types.UInt32)
+		v13 := b.NewValue0(v.Pos, OpMul32uhilo, MakeTuple(types.UInt32, types.UInt32))
+		v14 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v14.AddArg(x)
 		v13.AddArg(v14)
-		v15 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v15 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v15.AddArg(y)
 		v13.AddArg(v15)
 		v12.AddArg(v13)
@@ -1463,8 +1463,8 @@ func rewriteValuedec64_OpNeg64(v *Value) bool {
 func rewriteValuedec64_OpNeq64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Neq64 x y)
 	// cond:
 	// result: (OrB 		(Neq32 (Int64Hi x) (Int64Hi y)) 		(Neq32 (Int64Lo x) (Int64Lo y)))
@@ -1472,19 +1472,19 @@ func rewriteValuedec64_OpNeq64(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpOrB)
-		v0 := b.NewValue0(v.Pos, OpNeq32, fe.TypeBool())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpNeq32, types.Bool)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpNeq32, fe.TypeBool())
-		v4 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpNeq32, types.Bool)
+		v4 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v4.AddArg(x)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v5.AddArg(y)
 		v3.AddArg(v5)
 		v.AddArg(v3)
@@ -1494,28 +1494,28 @@ func rewriteValuedec64_OpNeq64(v *Value) bool {
 func rewriteValuedec64_OpOr64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Or64 x y)
 	// cond:
-	// result: (Int64Make 		(Or32 <fe.TypeUInt32()> (Int64Hi x) (Int64Hi y)) 		(Or32 <fe.TypeUInt32()> (Int64Lo x) (Int64Lo y)))
+	// result: (Int64Make 		(Or32 <types.UInt32> (Int64Hi x) (Int64Hi y)) 		(Or32 <types.UInt32> (Int64Lo x) (Int64Lo y)))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v4 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v4 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v4.AddArg(x)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v5.AddArg(y)
 		v3.AddArg(v5)
 		v.AddArg(v3)
@@ -1525,8 +1525,8 @@ func rewriteValuedec64_OpOr64(v *Value) bool {
 func rewriteValuedec64_OpRsh16Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux64 _ (Int64Make (Const32 [c]) _))
 	// cond: c != 0
 	// result: (Const32 [0])
@@ -1571,7 +1571,7 @@ func rewriteValuedec64_OpRsh16Ux64(v *Value) bool {
 	}
 	// match: (Rsh16Ux64 x (Int64Make hi lo))
 	// cond: hi.Op != OpConst32
-	// result: (Rsh16Ux32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	// result: (Rsh16Ux32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -1585,8 +1585,8 @@ func rewriteValuedec64_OpRsh16Ux64(v *Value) bool {
 		}
 		v.reset(OpRsh16Ux32)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
 		v1.AddArg(hi)
 		v0.AddArg(v1)
 		v0.AddArg(lo)
@@ -1598,8 +1598,8 @@ func rewriteValuedec64_OpRsh16Ux64(v *Value) bool {
 func rewriteValuedec64_OpRsh16x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x64 x (Int64Make (Const32 [c]) _))
 	// cond: c != 0
 	// result: (Signmask (SignExt16to32 x))
@@ -1618,7 +1618,7 @@ func rewriteValuedec64_OpRsh16x64(v *Value) bool {
 			break
 		}
 		v.reset(OpSignmask)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -1647,7 +1647,7 @@ func rewriteValuedec64_OpRsh16x64(v *Value) bool {
 	}
 	// match: (Rsh16x64 x (Int64Make hi lo))
 	// cond: hi.Op != OpConst32
-	// result: (Rsh16x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	// result: (Rsh16x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -1661,8 +1661,8 @@ func rewriteValuedec64_OpRsh16x64(v *Value) bool {
 		}
 		v.reset(OpRsh16x32)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
 		v1.AddArg(hi)
 		v0.AddArg(v1)
 		v0.AddArg(lo)
@@ -1674,8 +1674,8 @@ func rewriteValuedec64_OpRsh16x64(v *Value) bool {
 func rewriteValuedec64_OpRsh32Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux64 _ (Int64Make (Const32 [c]) _))
 	// cond: c != 0
 	// result: (Const32 [0])
@@ -1720,7 +1720,7 @@ func rewriteValuedec64_OpRsh32Ux64(v *Value) bool {
 	}
 	// match: (Rsh32Ux64 x (Int64Make hi lo))
 	// cond: hi.Op != OpConst32
-	// result: (Rsh32Ux32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	// result: (Rsh32Ux32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -1734,8 +1734,8 @@ func rewriteValuedec64_OpRsh32Ux64(v *Value) bool {
 		}
 		v.reset(OpRsh32Ux32)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
 		v1.AddArg(hi)
 		v0.AddArg(v1)
 		v0.AddArg(lo)
@@ -1747,8 +1747,8 @@ func rewriteValuedec64_OpRsh32Ux64(v *Value) bool {
 func rewriteValuedec64_OpRsh32x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x64 x (Int64Make (Const32 [c]) _))
 	// cond: c != 0
 	// result: (Signmask x)
@@ -1794,7 +1794,7 @@ func rewriteValuedec64_OpRsh32x64(v *Value) bool {
 	}
 	// match: (Rsh32x64 x (Int64Make hi lo))
 	// cond: hi.Op != OpConst32
-	// result: (Rsh32x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	// result: (Rsh32x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -1808,8 +1808,8 @@ func rewriteValuedec64_OpRsh32x64(v *Value) bool {
 		}
 		v.reset(OpRsh32x32)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
 		v1.AddArg(hi)
 		v0.AddArg(v1)
 		v0.AddArg(lo)
@@ -1821,11 +1821,11 @@ func rewriteValuedec64_OpRsh32x64(v *Value) bool {
 func rewriteValuedec64_OpRsh64Ux16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux16 (Int64Make hi lo) s)
 	// cond:
-	// result: (Int64Make 		(Rsh32Ux16 <fe.TypeUInt32()> hi s) 		(Or32 <fe.TypeUInt32()> 			(Or32 <fe.TypeUInt32()> 				(Rsh32Ux16 <fe.TypeUInt32()> lo s) 				(Lsh32x16 <fe.TypeUInt32()> 					hi 					(Sub16 <fe.TypeUInt16()> (Const16 <fe.TypeUInt16()> [32]) s))) 			(Rsh32Ux16 <fe.TypeUInt32()> 				hi 				(Sub16 <fe.TypeUInt16()> s (Const16 <fe.TypeUInt16()> [32])))))
+	// result: (Int64Make 		(Rsh32Ux16 <types.UInt32> hi s) 		(Or32 <types.UInt32> 			(Or32 <types.UInt32> 				(Rsh32Ux16 <types.UInt32> lo s) 				(Lsh32x16 <types.UInt32> 					hi 					(Sub16 <types.UInt16> (Const16 <types.UInt16> [32]) s))) 			(Rsh32Ux16 <types.UInt32> 				hi 				(Sub16 <types.UInt16> s (Const16 <types.UInt16> [32])))))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpInt64Make {
@@ -1835,31 +1835,31 @@ func rewriteValuedec64_OpRsh64Ux16(v *Value) bool {
 		lo := v_0.Args[1]
 		s := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpRsh32Ux16, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpRsh32Ux16, types.UInt32)
 		v0.AddArg(hi)
 		v0.AddArg(s)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpRsh32Ux16, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpRsh32Ux16, types.UInt32)
 		v3.AddArg(lo)
 		v3.AddArg(s)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpLsh32x16, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpLsh32x16, types.UInt32)
 		v4.AddArg(hi)
-		v5 := b.NewValue0(v.Pos, OpSub16, fe.TypeUInt16())
-		v6 := b.NewValue0(v.Pos, OpConst16, fe.TypeUInt16())
+		v5 := b.NewValue0(v.Pos, OpSub16, types.UInt16)
+		v6 := b.NewValue0(v.Pos, OpConst16, types.UInt16)
 		v6.AuxInt = 32
 		v5.AddArg(v6)
 		v5.AddArg(s)
 		v4.AddArg(v5)
 		v2.AddArg(v4)
 		v1.AddArg(v2)
-		v7 := b.NewValue0(v.Pos, OpRsh32Ux16, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpRsh32Ux16, types.UInt32)
 		v7.AddArg(hi)
-		v8 := b.NewValue0(v.Pos, OpSub16, fe.TypeUInt16())
+		v8 := b.NewValue0(v.Pos, OpSub16, types.UInt16)
 		v8.AddArg(s)
-		v9 := b.NewValue0(v.Pos, OpConst16, fe.TypeUInt16())
+		v9 := b.NewValue0(v.Pos, OpConst16, types.UInt16)
 		v9.AuxInt = 32
 		v8.AddArg(v9)
 		v7.AddArg(v8)
@@ -1872,11 +1872,11 @@ func rewriteValuedec64_OpRsh64Ux16(v *Value) bool {
 func rewriteValuedec64_OpRsh64Ux32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux32 (Int64Make hi lo) s)
 	// cond:
-	// result: (Int64Make 		(Rsh32Ux32 <fe.TypeUInt32()> hi s) 		(Or32 <fe.TypeUInt32()> 			(Or32 <fe.TypeUInt32()> 				(Rsh32Ux32 <fe.TypeUInt32()> lo s) 				(Lsh32x32 <fe.TypeUInt32()> 					hi 					(Sub32 <fe.TypeUInt32()> (Const32 <fe.TypeUInt32()> [32]) s))) 			(Rsh32Ux32 <fe.TypeUInt32()> 				hi 				(Sub32 <fe.TypeUInt32()> s (Const32 <fe.TypeUInt32()> [32])))))
+	// result: (Int64Make 		(Rsh32Ux32 <types.UInt32> hi s) 		(Or32 <types.UInt32> 			(Or32 <types.UInt32> 				(Rsh32Ux32 <types.UInt32> lo s) 				(Lsh32x32 <types.UInt32> 					hi 					(Sub32 <types.UInt32> (Const32 <types.UInt32> [32]) s))) 			(Rsh32Ux32 <types.UInt32> 				hi 				(Sub32 <types.UInt32> s (Const32 <types.UInt32> [32])))))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpInt64Make {
@@ -1886,31 +1886,31 @@ func rewriteValuedec64_OpRsh64Ux32(v *Value) bool {
 		lo := v_0.Args[1]
 		s := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpRsh32Ux32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpRsh32Ux32, types.UInt32)
 		v0.AddArg(hi)
 		v0.AddArg(s)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpRsh32Ux32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpRsh32Ux32, types.UInt32)
 		v3.AddArg(lo)
 		v3.AddArg(s)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpLsh32x32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpLsh32x32, types.UInt32)
 		v4.AddArg(hi)
-		v5 := b.NewValue0(v.Pos, OpSub32, fe.TypeUInt32())
-		v6 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpSub32, types.UInt32)
+		v6 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v6.AuxInt = 32
 		v5.AddArg(v6)
 		v5.AddArg(s)
 		v4.AddArg(v5)
 		v2.AddArg(v4)
 		v1.AddArg(v2)
-		v7 := b.NewValue0(v.Pos, OpRsh32Ux32, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpRsh32Ux32, types.UInt32)
 		v7.AddArg(hi)
-		v8 := b.NewValue0(v.Pos, OpSub32, fe.TypeUInt32())
+		v8 := b.NewValue0(v.Pos, OpSub32, types.UInt32)
 		v8.AddArg(s)
-		v9 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v9.AuxInt = 32
 		v8.AddArg(v9)
 		v7.AddArg(v8)
@@ -1923,8 +1923,8 @@ func rewriteValuedec64_OpRsh64Ux32(v *Value) bool {
 func rewriteValuedec64_OpRsh64Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux64 _ (Int64Make (Const32 [c]) _))
 	// cond: c != 0
 	// result: (Const64 [0])
@@ -1969,7 +1969,7 @@ func rewriteValuedec64_OpRsh64Ux64(v *Value) bool {
 	}
 	// match: (Rsh64Ux64 x (Int64Make hi lo))
 	// cond: hi.Op != OpConst32
-	// result: (Rsh64Ux32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	// result: (Rsh64Ux32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -1983,8 +1983,8 @@ func rewriteValuedec64_OpRsh64Ux64(v *Value) bool {
 		}
 		v.reset(OpRsh64Ux32)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
 		v1.AddArg(hi)
 		v0.AddArg(v1)
 		v0.AddArg(lo)
@@ -1996,11 +1996,11 @@ func rewriteValuedec64_OpRsh64Ux64(v *Value) bool {
 func rewriteValuedec64_OpRsh64Ux8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux8 (Int64Make hi lo) s)
 	// cond:
-	// result: (Int64Make 		(Rsh32Ux8 <fe.TypeUInt32()> hi s) 		(Or32 <fe.TypeUInt32()> 			(Or32 <fe.TypeUInt32()> 				(Rsh32Ux8 <fe.TypeUInt32()> lo s) 				(Lsh32x8 <fe.TypeUInt32()> 					hi 					(Sub8 <fe.TypeUInt8()> (Const8 <fe.TypeUInt8()> [32]) s))) 			(Rsh32Ux8 <fe.TypeUInt32()> 				hi 				(Sub8 <fe.TypeUInt8()> s (Const8 <fe.TypeUInt8()> [32])))))
+	// result: (Int64Make 		(Rsh32Ux8 <types.UInt32> hi s) 		(Or32 <types.UInt32> 			(Or32 <types.UInt32> 				(Rsh32Ux8 <types.UInt32> lo s) 				(Lsh32x8 <types.UInt32> 					hi 					(Sub8 <types.UInt8> (Const8 <types.UInt8> [32]) s))) 			(Rsh32Ux8 <types.UInt32> 				hi 				(Sub8 <types.UInt8> s (Const8 <types.UInt8> [32])))))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpInt64Make {
@@ -2010,31 +2010,31 @@ func rewriteValuedec64_OpRsh64Ux8(v *Value) bool {
 		lo := v_0.Args[1]
 		s := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpRsh32Ux8, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpRsh32Ux8, types.UInt32)
 		v0.AddArg(hi)
 		v0.AddArg(s)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpRsh32Ux8, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpRsh32Ux8, types.UInt32)
 		v3.AddArg(lo)
 		v3.AddArg(s)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpLsh32x8, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpLsh32x8, types.UInt32)
 		v4.AddArg(hi)
-		v5 := b.NewValue0(v.Pos, OpSub8, fe.TypeUInt8())
-		v6 := b.NewValue0(v.Pos, OpConst8, fe.TypeUInt8())
+		v5 := b.NewValue0(v.Pos, OpSub8, types.UInt8)
+		v6 := b.NewValue0(v.Pos, OpConst8, types.UInt8)
 		v6.AuxInt = 32
 		v5.AddArg(v6)
 		v5.AddArg(s)
 		v4.AddArg(v5)
 		v2.AddArg(v4)
 		v1.AddArg(v2)
-		v7 := b.NewValue0(v.Pos, OpRsh32Ux8, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpRsh32Ux8, types.UInt32)
 		v7.AddArg(hi)
-		v8 := b.NewValue0(v.Pos, OpSub8, fe.TypeUInt8())
+		v8 := b.NewValue0(v.Pos, OpSub8, types.UInt8)
 		v8.AddArg(s)
-		v9 := b.NewValue0(v.Pos, OpConst8, fe.TypeUInt8())
+		v9 := b.NewValue0(v.Pos, OpConst8, types.UInt8)
 		v9.AuxInt = 32
 		v8.AddArg(v9)
 		v7.AddArg(v8)
@@ -2047,11 +2047,11 @@ func rewriteValuedec64_OpRsh64Ux8(v *Value) bool {
 func rewriteValuedec64_OpRsh64x16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x16 (Int64Make hi lo) s)
 	// cond:
-	// result: (Int64Make 		(Rsh32x16 <fe.TypeUInt32()> hi s) 		(Or32 <fe.TypeUInt32()> 			(Or32 <fe.TypeUInt32()> 				(Rsh32Ux16 <fe.TypeUInt32()> lo s) 				(Lsh32x16 <fe.TypeUInt32()> 					hi 					(Sub16 <fe.TypeUInt16()> (Const16 <fe.TypeUInt16()> [32]) s))) 			(And32 <fe.TypeUInt32()> 				(Rsh32x16 <fe.TypeUInt32()> 					hi 					(Sub16 <fe.TypeUInt16()> s (Const16 <fe.TypeUInt16()> [32]))) 				(Zeromask 					(ZeroExt16to32 						(Rsh16Ux32 <fe.TypeUInt16()> s (Const32 <fe.TypeUInt32()> [5])))))))
+	// result: (Int64Make 		(Rsh32x16 <types.UInt32> hi s) 		(Or32 <types.UInt32> 			(Or32 <types.UInt32> 				(Rsh32Ux16 <types.UInt32> lo s) 				(Lsh32x16 <types.UInt32> 					hi 					(Sub16 <types.UInt16> (Const16 <types.UInt16> [32]) s))) 			(And32 <types.UInt32> 				(Rsh32x16 <types.UInt32> 					hi 					(Sub16 <types.UInt16> s (Const16 <types.UInt16> [32]))) 				(Zeromask 					(ZeroExt16to32 						(Rsh16Ux32 <types.UInt16> s (Const32 <types.UInt32> [5])))))))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpInt64Make {
@@ -2061,41 +2061,41 @@ func rewriteValuedec64_OpRsh64x16(v *Value) bool {
 		lo := v_0.Args[1]
 		s := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpRsh32x16, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpRsh32x16, types.UInt32)
 		v0.AddArg(hi)
 		v0.AddArg(s)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpRsh32Ux16, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpRsh32Ux16, types.UInt32)
 		v3.AddArg(lo)
 		v3.AddArg(s)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpLsh32x16, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpLsh32x16, types.UInt32)
 		v4.AddArg(hi)
-		v5 := b.NewValue0(v.Pos, OpSub16, fe.TypeUInt16())
-		v6 := b.NewValue0(v.Pos, OpConst16, fe.TypeUInt16())
+		v5 := b.NewValue0(v.Pos, OpSub16, types.UInt16)
+		v6 := b.NewValue0(v.Pos, OpConst16, types.UInt16)
 		v6.AuxInt = 32
 		v5.AddArg(v6)
 		v5.AddArg(s)
 		v4.AddArg(v5)
 		v2.AddArg(v4)
 		v1.AddArg(v2)
-		v7 := b.NewValue0(v.Pos, OpAnd32, fe.TypeUInt32())
-		v8 := b.NewValue0(v.Pos, OpRsh32x16, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpAnd32, types.UInt32)
+		v8 := b.NewValue0(v.Pos, OpRsh32x16, types.UInt32)
 		v8.AddArg(hi)
-		v9 := b.NewValue0(v.Pos, OpSub16, fe.TypeUInt16())
+		v9 := b.NewValue0(v.Pos, OpSub16, types.UInt16)
 		v9.AddArg(s)
-		v10 := b.NewValue0(v.Pos, OpConst16, fe.TypeUInt16())
+		v10 := b.NewValue0(v.Pos, OpConst16, types.UInt16)
 		v10.AuxInt = 32
 		v9.AddArg(v10)
 		v8.AddArg(v9)
 		v7.AddArg(v8)
-		v11 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
-		v12 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
-		v13 := b.NewValue0(v.Pos, OpRsh16Ux32, fe.TypeUInt16())
+		v11 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
+		v12 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
+		v13 := b.NewValue0(v.Pos, OpRsh16Ux32, types.UInt16)
 		v13.AddArg(s)
-		v14 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v14 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v14.AuxInt = 5
 		v13.AddArg(v14)
 		v12.AddArg(v13)
@@ -2110,11 +2110,11 @@ func rewriteValuedec64_OpRsh64x16(v *Value) bool {
 func rewriteValuedec64_OpRsh64x32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x32 (Int64Make hi lo) s)
 	// cond:
-	// result: (Int64Make 		(Rsh32x32 <fe.TypeUInt32()> hi s) 		(Or32 <fe.TypeUInt32()> 			(Or32 <fe.TypeUInt32()> 				(Rsh32Ux32 <fe.TypeUInt32()> lo s) 				(Lsh32x32 <fe.TypeUInt32()> 					hi 					(Sub32 <fe.TypeUInt32()> (Const32 <fe.TypeUInt32()> [32]) s))) 			(And32 <fe.TypeUInt32()> 				(Rsh32x32 <fe.TypeUInt32()> 					hi 					(Sub32 <fe.TypeUInt32()> s (Const32 <fe.TypeUInt32()> [32]))) 				(Zeromask 					(Rsh32Ux32 <fe.TypeUInt32()> s (Const32 <fe.TypeUInt32()> [5]))))))
+	// result: (Int64Make 		(Rsh32x32 <types.UInt32> hi s) 		(Or32 <types.UInt32> 			(Or32 <types.UInt32> 				(Rsh32Ux32 <types.UInt32> lo s) 				(Lsh32x32 <types.UInt32> 					hi 					(Sub32 <types.UInt32> (Const32 <types.UInt32> [32]) s))) 			(And32 <types.UInt32> 				(Rsh32x32 <types.UInt32> 					hi 					(Sub32 <types.UInt32> s (Const32 <types.UInt32> [32]))) 				(Zeromask 					(Rsh32Ux32 <types.UInt32> s (Const32 <types.UInt32> [5]))))))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpInt64Make {
@@ -2124,40 +2124,40 @@ func rewriteValuedec64_OpRsh64x32(v *Value) bool {
 		lo := v_0.Args[1]
 		s := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpRsh32x32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpRsh32x32, types.UInt32)
 		v0.AddArg(hi)
 		v0.AddArg(s)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpRsh32Ux32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpRsh32Ux32, types.UInt32)
 		v3.AddArg(lo)
 		v3.AddArg(s)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpLsh32x32, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpLsh32x32, types.UInt32)
 		v4.AddArg(hi)
-		v5 := b.NewValue0(v.Pos, OpSub32, fe.TypeUInt32())
-		v6 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpSub32, types.UInt32)
+		v6 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v6.AuxInt = 32
 		v5.AddArg(v6)
 		v5.AddArg(s)
 		v4.AddArg(v5)
 		v2.AddArg(v4)
 		v1.AddArg(v2)
-		v7 := b.NewValue0(v.Pos, OpAnd32, fe.TypeUInt32())
-		v8 := b.NewValue0(v.Pos, OpRsh32x32, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpAnd32, types.UInt32)
+		v8 := b.NewValue0(v.Pos, OpRsh32x32, types.UInt32)
 		v8.AddArg(hi)
-		v9 := b.NewValue0(v.Pos, OpSub32, fe.TypeUInt32())
+		v9 := b.NewValue0(v.Pos, OpSub32, types.UInt32)
 		v9.AddArg(s)
-		v10 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v10 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v10.AuxInt = 32
 		v9.AddArg(v10)
 		v8.AddArg(v9)
 		v7.AddArg(v8)
-		v11 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
-		v12 := b.NewValue0(v.Pos, OpRsh32Ux32, fe.TypeUInt32())
+		v11 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
+		v12 := b.NewValue0(v.Pos, OpRsh32Ux32, types.UInt32)
 		v12.AddArg(s)
-		v13 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v13 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v13.AuxInt = 5
 		v12.AddArg(v13)
 		v11.AddArg(v12)
@@ -2171,8 +2171,8 @@ func rewriteValuedec64_OpRsh64x32(v *Value) bool {
 func rewriteValuedec64_OpRsh64x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x64 x (Int64Make (Const32 [c]) _))
 	// cond: c != 0
 	// result: (Int64Make (Signmask (Int64Hi x)) (Signmask (Int64Hi x)))
@@ -2191,13 +2191,13 @@ func rewriteValuedec64_OpRsh64x64(v *Value) bool {
 			break
 		}
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
-		v3 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
+		v3 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v3.AddArg(x)
 		v2.AddArg(v3)
 		v.AddArg(v2)
@@ -2227,7 +2227,7 @@ func rewriteValuedec64_OpRsh64x64(v *Value) bool {
 	}
 	// match: (Rsh64x64 x (Int64Make hi lo))
 	// cond: hi.Op != OpConst32
-	// result: (Rsh64x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	// result: (Rsh64x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -2241,8 +2241,8 @@ func rewriteValuedec64_OpRsh64x64(v *Value) bool {
 		}
 		v.reset(OpRsh64x32)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
 		v1.AddArg(hi)
 		v0.AddArg(v1)
 		v0.AddArg(lo)
@@ -2254,11 +2254,11 @@ func rewriteValuedec64_OpRsh64x64(v *Value) bool {
 func rewriteValuedec64_OpRsh64x8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x8 (Int64Make hi lo) s)
 	// cond:
-	// result: (Int64Make 		(Rsh32x8 <fe.TypeUInt32()> hi s) 		(Or32 <fe.TypeUInt32()> 			(Or32 <fe.TypeUInt32()> 				(Rsh32Ux8 <fe.TypeUInt32()> lo s) 				(Lsh32x8 <fe.TypeUInt32()> 					hi 					(Sub8 <fe.TypeUInt8()> (Const8 <fe.TypeUInt8()> [32]) s))) 			(And32 <fe.TypeUInt32()> 				(Rsh32x8 <fe.TypeUInt32()> 					hi 					(Sub8 <fe.TypeUInt8()> s (Const8 <fe.TypeUInt8()> [32]))) 				(Zeromask 					(ZeroExt8to32 						(Rsh8Ux32 <fe.TypeUInt8()> s (Const32 <fe.TypeUInt32()> [5])))))))
+	// result: (Int64Make 		(Rsh32x8 <types.UInt32> hi s) 		(Or32 <types.UInt32> 			(Or32 <types.UInt32> 				(Rsh32Ux8 <types.UInt32> lo s) 				(Lsh32x8 <types.UInt32> 					hi 					(Sub8 <types.UInt8> (Const8 <types.UInt8> [32]) s))) 			(And32 <types.UInt32> 				(Rsh32x8 <types.UInt32> 					hi 					(Sub8 <types.UInt8> s (Const8 <types.UInt8> [32]))) 				(Zeromask 					(ZeroExt8to32 						(Rsh8Ux32 <types.UInt8> s (Const32 <types.UInt32> [5])))))))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpInt64Make {
@@ -2268,41 +2268,41 @@ func rewriteValuedec64_OpRsh64x8(v *Value) bool {
 		lo := v_0.Args[1]
 		s := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpRsh32x8, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpRsh32x8, types.UInt32)
 		v0.AddArg(hi)
 		v0.AddArg(s)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpRsh32Ux8, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpRsh32Ux8, types.UInt32)
 		v3.AddArg(lo)
 		v3.AddArg(s)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpLsh32x8, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpLsh32x8, types.UInt32)
 		v4.AddArg(hi)
-		v5 := b.NewValue0(v.Pos, OpSub8, fe.TypeUInt8())
-		v6 := b.NewValue0(v.Pos, OpConst8, fe.TypeUInt8())
+		v5 := b.NewValue0(v.Pos, OpSub8, types.UInt8)
+		v6 := b.NewValue0(v.Pos, OpConst8, types.UInt8)
 		v6.AuxInt = 32
 		v5.AddArg(v6)
 		v5.AddArg(s)
 		v4.AddArg(v5)
 		v2.AddArg(v4)
 		v1.AddArg(v2)
-		v7 := b.NewValue0(v.Pos, OpAnd32, fe.TypeUInt32())
-		v8 := b.NewValue0(v.Pos, OpRsh32x8, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpAnd32, types.UInt32)
+		v8 := b.NewValue0(v.Pos, OpRsh32x8, types.UInt32)
 		v8.AddArg(hi)
-		v9 := b.NewValue0(v.Pos, OpSub8, fe.TypeUInt8())
+		v9 := b.NewValue0(v.Pos, OpSub8, types.UInt8)
 		v9.AddArg(s)
-		v10 := b.NewValue0(v.Pos, OpConst8, fe.TypeUInt8())
+		v10 := b.NewValue0(v.Pos, OpConst8, types.UInt8)
 		v10.AuxInt = 32
 		v9.AddArg(v10)
 		v8.AddArg(v9)
 		v7.AddArg(v8)
-		v11 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
-		v12 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
-		v13 := b.NewValue0(v.Pos, OpRsh8Ux32, fe.TypeUInt8())
+		v11 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
+		v12 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
+		v13 := b.NewValue0(v.Pos, OpRsh8Ux32, types.UInt8)
 		v13.AddArg(s)
-		v14 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v14 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v14.AuxInt = 5
 		v13.AddArg(v14)
 		v12.AddArg(v13)
@@ -2317,8 +2317,8 @@ func rewriteValuedec64_OpRsh64x8(v *Value) bool {
 func rewriteValuedec64_OpRsh8Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux64 _ (Int64Make (Const32 [c]) _))
 	// cond: c != 0
 	// result: (Const32 [0])
@@ -2363,7 +2363,7 @@ func rewriteValuedec64_OpRsh8Ux64(v *Value) bool {
 	}
 	// match: (Rsh8Ux64 x (Int64Make hi lo))
 	// cond: hi.Op != OpConst32
-	// result: (Rsh8Ux32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	// result: (Rsh8Ux32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -2377,8 +2377,8 @@ func rewriteValuedec64_OpRsh8Ux64(v *Value) bool {
 		}
 		v.reset(OpRsh8Ux32)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
 		v1.AddArg(hi)
 		v0.AddArg(v1)
 		v0.AddArg(lo)
@@ -2390,8 +2390,8 @@ func rewriteValuedec64_OpRsh8Ux64(v *Value) bool {
 func rewriteValuedec64_OpRsh8x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8x64 x (Int64Make (Const32 [c]) _))
 	// cond: c != 0
 	// result: (Signmask (SignExt8to32 x))
@@ -2410,7 +2410,7 @@ func rewriteValuedec64_OpRsh8x64(v *Value) bool {
 			break
 		}
 		v.reset(OpSignmask)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -2439,7 +2439,7 @@ func rewriteValuedec64_OpRsh8x64(v *Value) bool {
 	}
 	// match: (Rsh8x64 x (Int64Make hi lo))
 	// cond: hi.Op != OpConst32
-	// result: (Rsh8x32 x (Or32 <fe.TypeUInt32()> (Zeromask hi) lo))
+	// result: (Rsh8x32 x (Or32 <types.UInt32> (Zeromask hi) lo))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -2453,8 +2453,8 @@ func rewriteValuedec64_OpRsh8x64(v *Value) bool {
 		}
 		v.reset(OpRsh8x32)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpOr32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpZeromask, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpOr32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpZeromask, types.UInt32)
 		v1.AddArg(hi)
 		v0.AddArg(v1)
 		v0.AddArg(lo)
@@ -2466,15 +2466,15 @@ func rewriteValuedec64_OpRsh8x64(v *Value) bool {
 func rewriteValuedec64_OpSignExt16to64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (SignExt16to64 x)
 	// cond:
 	// result: (SignExt32to64 (SignExt16to32 x))
 	for {
 		x := v.Args[0]
 		v.reset(OpSignExt32to64)
-		v0 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -2483,15 +2483,15 @@ func rewriteValuedec64_OpSignExt16to64(v *Value) bool {
 func rewriteValuedec64_OpSignExt32to64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (SignExt32to64 x)
 	// cond:
 	// result: (Int64Make (Signmask x) x)
 	for {
 		x := v.Args[0]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpSignmask, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignmask, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		v.AddArg(x)
@@ -2501,15 +2501,15 @@ func rewriteValuedec64_OpSignExt32to64(v *Value) bool {
 func rewriteValuedec64_OpSignExt8to64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (SignExt8to64 x)
 	// cond:
 	// result: (SignExt32to64 (SignExt8to32 x))
 	for {
 		x := v.Args[0]
 		v.reset(OpSignExt32to64)
-		v0 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -2587,39 +2587,39 @@ func rewriteValuedec64_OpStore(v *Value) bool {
 func rewriteValuedec64_OpSub64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Sub64 x y)
 	// cond:
-	// result: (Int64Make 		(Sub32withcarry <fe.TypeInt32()> 			(Int64Hi x) 			(Int64Hi y) 			(Select1 <TypeFlags> (Sub32carry (Int64Lo x) (Int64Lo y)))) 		(Select0 <fe.TypeUInt32()> (Sub32carry (Int64Lo x) (Int64Lo y))))
+	// result: (Int64Make 		(Sub32withcarry <types.Int32> 			(Int64Hi x) 			(Int64Hi y) 			(Select1 <TypeFlags> (Sub32carry (Int64Lo x) (Int64Lo y)))) 		(Select0 <types.UInt32> (Sub32carry (Int64Lo x) (Int64Lo y))))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpSub32withcarry, fe.TypeInt32())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpSub32withcarry, types.Int32)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v3 := b.NewValue0(v.Pos, OpSelect1, TypeFlags)
-		v4 := b.NewValue0(v.Pos, OpSub32carry, MakeTuple(fe.TypeUInt32(), TypeFlags))
-		v5 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v4 := b.NewValue0(v.Pos, OpSub32carry, MakeTuple(types.UInt32, TypeFlags))
+		v5 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v5.AddArg(x)
 		v4.AddArg(v5)
-		v6 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v6 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v6.AddArg(y)
 		v4.AddArg(v6)
 		v3.AddArg(v4)
 		v0.AddArg(v3)
 		v.AddArg(v0)
-		v7 := b.NewValue0(v.Pos, OpSelect0, fe.TypeUInt32())
-		v8 := b.NewValue0(v.Pos, OpSub32carry, MakeTuple(fe.TypeUInt32(), TypeFlags))
-		v9 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpSelect0, types.UInt32)
+		v8 := b.NewValue0(v.Pos, OpSub32carry, MakeTuple(types.UInt32, TypeFlags))
+		v9 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v9.AddArg(x)
 		v8.AddArg(v9)
-		v10 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v10 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v10.AddArg(y)
 		v8.AddArg(v10)
 		v7.AddArg(v8)
@@ -2679,28 +2679,28 @@ func rewriteValuedec64_OpTrunc64to8(v *Value) bool {
 func rewriteValuedec64_OpXor64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Xor64 x y)
 	// cond:
-	// result: (Int64Make 		(Xor32 <fe.TypeUInt32()> (Int64Hi x) (Int64Hi y)) 		(Xor32 <fe.TypeUInt32()> (Int64Lo x) (Int64Lo y)))
+	// result: (Int64Make 		(Xor32 <types.UInt32> (Int64Hi x) (Int64Hi y)) 		(Xor32 <types.UInt32> (Int64Lo x) (Int64Lo y)))
 	for {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpXor32, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpXor32, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpInt64Hi, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpInt64Hi, types.UInt32)
 		v2.AddArg(y)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpXor32, fe.TypeUInt32())
-		v4 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpXor32, types.UInt32)
+		v4 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v4.AddArg(x)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpInt64Lo, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpInt64Lo, types.UInt32)
 		v5.AddArg(y)
 		v3.AddArg(v5)
 		v.AddArg(v3)
@@ -2710,15 +2710,15 @@ func rewriteValuedec64_OpXor64(v *Value) bool {
 func rewriteValuedec64_OpZeroExt16to64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (ZeroExt16to64 x)
 	// cond:
 	// result: (ZeroExt32to64 (ZeroExt16to32 x))
 	for {
 		x := v.Args[0]
 		v.reset(OpZeroExt32to64)
-		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -2727,15 +2727,15 @@ func rewriteValuedec64_OpZeroExt16to64(v *Value) bool {
 func rewriteValuedec64_OpZeroExt32to64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (ZeroExt32to64 x)
 	// cond:
-	// result: (Int64Make (Const32 <fe.TypeUInt32()> [0]) x)
+	// result: (Int64Make (Const32 <types.UInt32> [0]) x)
 	for {
 		x := v.Args[0]
 		v.reset(OpInt64Make)
-		v0 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v0.AuxInt = 0
 		v.AddArg(v0)
 		v.AddArg(x)
@@ -2745,15 +2745,15 @@ func rewriteValuedec64_OpZeroExt32to64(v *Value) bool {
 func rewriteValuedec64_OpZeroExt8to64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (ZeroExt8to64 x)
 	// cond:
 	// result: (ZeroExt32to64 (ZeroExt8to32 x))
 	for {
 		x := v.Args[0]
 		v.reset(OpZeroExt32to64)
-		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -2764,6 +2764,8 @@ func rewriteBlockdec64(b *Block) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &config.Types
+	_ = types
 	switch b.Kind {
 	}
 	return false
diff --git a/src/cmd/compile/internal/ssa/rewritegeneric.go b/src/cmd/compile/internal/ssa/rewritegeneric.go
index eb769020f0..81f48b48ff 100644
--- a/src/cmd/compile/internal/ssa/rewritegeneric.go
+++ b/src/cmd/compile/internal/ssa/rewritegeneric.go
@@ -2833,9 +2833,11 @@ func rewriteValuegeneric_OpArg(v *Value) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Arg {n} [off])
 	// cond: v.Type.IsString()
-	// result: (StringMake     (Arg <fe.TypeBytePtr()> {n} [off])     (Arg <fe.TypeInt()> {n} [off+config.PtrSize]))
+	// result: (StringMake     (Arg <types.BytePtr> {n} [off])     (Arg <types.Int> {n} [off+config.PtrSize]))
 	for {
 		off := v.AuxInt
 		n := v.Aux
@@ -2843,11 +2845,11 @@ func rewriteValuegeneric_OpArg(v *Value) bool {
 			break
 		}
 		v.reset(OpStringMake)
-		v0 := b.NewValue0(v.Pos, OpArg, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpArg, types.BytePtr)
 		v0.AuxInt = off
 		v0.Aux = n
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpArg, fe.TypeInt())
+		v1 := b.NewValue0(v.Pos, OpArg, types.Int)
 		v1.AuxInt = off + config.PtrSize
 		v1.Aux = n
 		v.AddArg(v1)
@@ -2855,7 +2857,7 @@ func rewriteValuegeneric_OpArg(v *Value) bool {
 	}
 	// match: (Arg {n} [off])
 	// cond: v.Type.IsSlice()
-	// result: (SliceMake     (Arg <v.Type.ElemType().PtrTo()> {n} [off])     (Arg <fe.TypeInt()> {n} [off+config.PtrSize])     (Arg <fe.TypeInt()> {n} [off+2*config.PtrSize]))
+	// result: (SliceMake     (Arg <v.Type.ElemType().PtrTo()> {n} [off])     (Arg <types.Int> {n} [off+config.PtrSize])     (Arg <types.Int> {n} [off+2*config.PtrSize]))
 	for {
 		off := v.AuxInt
 		n := v.Aux
@@ -2867,11 +2869,11 @@ func rewriteValuegeneric_OpArg(v *Value) bool {
 		v0.AuxInt = off
 		v0.Aux = n
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpArg, fe.TypeInt())
+		v1 := b.NewValue0(v.Pos, OpArg, types.Int)
 		v1.AuxInt = off + config.PtrSize
 		v1.Aux = n
 		v.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpArg, fe.TypeInt())
+		v2 := b.NewValue0(v.Pos, OpArg, types.Int)
 		v2.AuxInt = off + 2*config.PtrSize
 		v2.Aux = n
 		v.AddArg(v2)
@@ -2879,7 +2881,7 @@ func rewriteValuegeneric_OpArg(v *Value) bool {
 	}
 	// match: (Arg {n} [off])
 	// cond: v.Type.IsInterface()
-	// result: (IMake     (Arg <fe.TypeBytePtr()> {n} [off])     (Arg <fe.TypeBytePtr()> {n} [off+config.PtrSize]))
+	// result: (IMake     (Arg <types.BytePtr> {n} [off])     (Arg <types.BytePtr> {n} [off+config.PtrSize]))
 	for {
 		off := v.AuxInt
 		n := v.Aux
@@ -2887,11 +2889,11 @@ func rewriteValuegeneric_OpArg(v *Value) bool {
 			break
 		}
 		v.reset(OpIMake)
-		v0 := b.NewValue0(v.Pos, OpArg, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpArg, types.BytePtr)
 		v0.AuxInt = off
 		v0.Aux = n
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpArg, fe.TypeBytePtr())
+		v1 := b.NewValue0(v.Pos, OpArg, types.BytePtr)
 		v1.AuxInt = off + config.PtrSize
 		v1.Aux = n
 		v.AddArg(v1)
@@ -2899,7 +2901,7 @@ func rewriteValuegeneric_OpArg(v *Value) bool {
 	}
 	// match: (Arg {n} [off])
 	// cond: v.Type.IsComplex() && v.Type.Size() == 16
-	// result: (ComplexMake     (Arg <fe.TypeFloat64()> {n} [off])     (Arg <fe.TypeFloat64()> {n} [off+8]))
+	// result: (ComplexMake     (Arg <types.Float64> {n} [off])     (Arg <types.Float64> {n} [off+8]))
 	for {
 		off := v.AuxInt
 		n := v.Aux
@@ -2907,11 +2909,11 @@ func rewriteValuegeneric_OpArg(v *Value) bool {
 			break
 		}
 		v.reset(OpComplexMake)
-		v0 := b.NewValue0(v.Pos, OpArg, fe.TypeFloat64())
+		v0 := b.NewValue0(v.Pos, OpArg, types.Float64)
 		v0.AuxInt = off
 		v0.Aux = n
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpArg, fe.TypeFloat64())
+		v1 := b.NewValue0(v.Pos, OpArg, types.Float64)
 		v1.AuxInt = off + 8
 		v1.Aux = n
 		v.AddArg(v1)
@@ -2919,7 +2921,7 @@ func rewriteValuegeneric_OpArg(v *Value) bool {
 	}
 	// match: (Arg {n} [off])
 	// cond: v.Type.IsComplex() && v.Type.Size() == 8
-	// result: (ComplexMake     (Arg <fe.TypeFloat32()> {n} [off])     (Arg <fe.TypeFloat32()> {n} [off+4]))
+	// result: (ComplexMake     (Arg <types.Float32> {n} [off])     (Arg <types.Float32> {n} [off+4]))
 	for {
 		off := v.AuxInt
 		n := v.Aux
@@ -2927,11 +2929,11 @@ func rewriteValuegeneric_OpArg(v *Value) bool {
 			break
 		}
 		v.reset(OpComplexMake)
-		v0 := b.NewValue0(v.Pos, OpArg, fe.TypeFloat32())
+		v0 := b.NewValue0(v.Pos, OpArg, types.Float32)
 		v0.AuxInt = off
 		v0.Aux = n
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpArg, fe.TypeFloat32())
+		v1 := b.NewValue0(v.Pos, OpArg, types.Float32)
 		v1.AuxInt = off + 4
 		v1.Aux = n
 		v.AddArg(v1)
@@ -3192,16 +3194,16 @@ func rewriteValuegeneric_OpCom8(v *Value) bool {
 func rewriteValuegeneric_OpConstInterface(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (ConstInterface)
 	// cond:
-	// result: (IMake     (ConstNil <fe.TypeBytePtr()>)     (ConstNil <fe.TypeBytePtr()>))
+	// result: (IMake     (ConstNil <types.BytePtr>)     (ConstNil <types.BytePtr>))
 	for {
 		v.reset(OpIMake)
-		v0 := b.NewValue0(v.Pos, OpConstNil, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpConstNil, types.BytePtr)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpConstNil, fe.TypeBytePtr())
+		v1 := b.NewValue0(v.Pos, OpConstNil, types.BytePtr)
 		v.AddArg(v1)
 		return true
 	}
@@ -3211,11 +3213,11 @@ func rewriteValuegeneric_OpConstSlice(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (ConstSlice)
 	// cond: config.PtrSize == 4
-	// result: (SliceMake     (ConstNil <v.Type.ElemType().PtrTo()>)     (Const32 <fe.TypeInt()> [0])     (Const32 <fe.TypeInt()> [0]))
+	// result: (SliceMake     (ConstNil <v.Type.ElemType().PtrTo()>)     (Const32 <types.Int> [0])     (Const32 <types.Int> [0]))
 	for {
 		if !(config.PtrSize == 4) {
 			break
@@ -3223,17 +3225,17 @@ func rewriteValuegeneric_OpConstSlice(v *Value) bool {
 		v.reset(OpSliceMake)
 		v0 := b.NewValue0(v.Pos, OpConstNil, v.Type.ElemType().PtrTo())
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpConst32, fe.TypeInt())
+		v1 := b.NewValue0(v.Pos, OpConst32, types.Int)
 		v1.AuxInt = 0
 		v.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpConst32, fe.TypeInt())
+		v2 := b.NewValue0(v.Pos, OpConst32, types.Int)
 		v2.AuxInt = 0
 		v.AddArg(v2)
 		return true
 	}
 	// match: (ConstSlice)
 	// cond: config.PtrSize == 8
-	// result: (SliceMake     (ConstNil <v.Type.ElemType().PtrTo()>)     (Const64 <fe.TypeInt()> [0])     (Const64 <fe.TypeInt()> [0]))
+	// result: (SliceMake     (ConstNil <v.Type.ElemType().PtrTo()>)     (Const64 <types.Int> [0])     (Const64 <types.Int> [0]))
 	for {
 		if !(config.PtrSize == 8) {
 			break
@@ -3241,10 +3243,10 @@ func rewriteValuegeneric_OpConstSlice(v *Value) bool {
 		v.reset(OpSliceMake)
 		v0 := b.NewValue0(v.Pos, OpConstNil, v.Type.ElemType().PtrTo())
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpConst64, fe.TypeInt())
+		v1 := b.NewValue0(v.Pos, OpConst64, types.Int)
 		v1.AuxInt = 0
 		v.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeInt())
+		v2 := b.NewValue0(v.Pos, OpConst64, types.Int)
 		v2.AuxInt = 0
 		v.AddArg(v2)
 		return true
@@ -3258,72 +3260,74 @@ func rewriteValuegeneric_OpConstString(v *Value) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (ConstString {s})
 	// cond: config.PtrSize == 4 && s.(string) == ""
-	// result: (StringMake (ConstNil) (Const32 <fe.TypeInt()> [0]))
+	// result: (StringMake (ConstNil) (Const32 <types.Int> [0]))
 	for {
 		s := v.Aux
 		if !(config.PtrSize == 4 && s.(string) == "") {
 			break
 		}
 		v.reset(OpStringMake)
-		v0 := b.NewValue0(v.Pos, OpConstNil, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpConstNil, types.BytePtr)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpConst32, fe.TypeInt())
+		v1 := b.NewValue0(v.Pos, OpConst32, types.Int)
 		v1.AuxInt = 0
 		v.AddArg(v1)
 		return true
 	}
 	// match: (ConstString {s})
 	// cond: config.PtrSize == 8 && s.(string) == ""
-	// result: (StringMake (ConstNil) (Const64 <fe.TypeInt()> [0]))
+	// result: (StringMake (ConstNil) (Const64 <types.Int> [0]))
 	for {
 		s := v.Aux
 		if !(config.PtrSize == 8 && s.(string) == "") {
 			break
 		}
 		v.reset(OpStringMake)
-		v0 := b.NewValue0(v.Pos, OpConstNil, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpConstNil, types.BytePtr)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpConst64, fe.TypeInt())
+		v1 := b.NewValue0(v.Pos, OpConst64, types.Int)
 		v1.AuxInt = 0
 		v.AddArg(v1)
 		return true
 	}
 	// match: (ConstString {s})
 	// cond: config.PtrSize == 4 && s.(string) != ""
-	// result: (StringMake     (Addr <fe.TypeBytePtr()> {fe.StringData(s.(string))}       (SB))     (Const32 <fe.TypeInt()> [int64(len(s.(string)))]))
+	// result: (StringMake     (Addr <types.BytePtr> {fe.StringData(s.(string))}       (SB))     (Const32 <types.Int> [int64(len(s.(string)))]))
 	for {
 		s := v.Aux
 		if !(config.PtrSize == 4 && s.(string) != "") {
 			break
 		}
 		v.reset(OpStringMake)
-		v0 := b.NewValue0(v.Pos, OpAddr, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpAddr, types.BytePtr)
 		v0.Aux = fe.StringData(s.(string))
-		v1 := b.NewValue0(v.Pos, OpSB, fe.TypeUintptr())
+		v1 := b.NewValue0(v.Pos, OpSB, types.Uintptr)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpConst32, fe.TypeInt())
+		v2 := b.NewValue0(v.Pos, OpConst32, types.Int)
 		v2.AuxInt = int64(len(s.(string)))
 		v.AddArg(v2)
 		return true
 	}
 	// match: (ConstString {s})
 	// cond: config.PtrSize == 8 && s.(string) != ""
-	// result: (StringMake     (Addr <fe.TypeBytePtr()> {fe.StringData(s.(string))}       (SB))     (Const64 <fe.TypeInt()> [int64(len(s.(string)))]))
+	// result: (StringMake     (Addr <types.BytePtr> {fe.StringData(s.(string))}       (SB))     (Const64 <types.Int> [int64(len(s.(string)))]))
 	for {
 		s := v.Aux
 		if !(config.PtrSize == 8 && s.(string) != "") {
 			break
 		}
 		v.reset(OpStringMake)
-		v0 := b.NewValue0(v.Pos, OpAddr, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpAddr, types.BytePtr)
 		v0.Aux = fe.StringData(s.(string))
-		v1 := b.NewValue0(v.Pos, OpSB, fe.TypeUintptr())
+		v1 := b.NewValue0(v.Pos, OpSB, types.Uintptr)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeInt())
+		v2 := b.NewValue0(v.Pos, OpConst64, types.Int)
 		v2.AuxInt = int64(len(s.(string)))
 		v.AddArg(v2)
 		return true
@@ -3432,8 +3436,8 @@ func rewriteValuegeneric_OpCvt64Fto32F(v *Value) bool {
 func rewriteValuegeneric_OpDiv16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16  (Const16 [c])  (Const16 [d]))
 	// cond: d != 0
 	// result: (Const16 [int64(int16(c)/int16(d))])
@@ -3480,7 +3484,7 @@ func rewriteValuegeneric_OpDiv16(v *Value) bool {
 	}
 	// match: (Div16 <t> x (Const16 [-1<<15]))
 	// cond:
-	// result: (Rsh16Ux64 (And16 <t> x (Neg16 <t> x)) (Const64 <fe.TypeUInt64()> [15]))
+	// result: (Rsh16Ux64 (And16 <t> x (Neg16 <t> x)) (Const64 <types.UInt64> [15]))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -3498,14 +3502,14 @@ func rewriteValuegeneric_OpDiv16(v *Value) bool {
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 15
 		v.AddArg(v2)
 		return true
 	}
 	// match: (Div16 <t> n (Const16 [c]))
 	// cond: isPowerOfTwo(c)
-	// result: (Rsh16x64     (Add16 <t> n (Rsh16Ux64 <t> (Rsh16x64 <t> n (Const64 <fe.TypeUInt64()> [15])) (Const64 <fe.TypeUInt64()> [16-log2(c)])))     (Const64 <fe.TypeUInt64()> [log2(c)]))
+	// result: (Rsh16x64     (Add16 <t> n (Rsh16Ux64 <t> (Rsh16x64 <t> n (Const64 <types.UInt64> [15])) (Const64 <types.UInt64> [16-log2(c)])))     (Const64 <types.UInt64> [log2(c)]))
 	for {
 		t := v.Type
 		n := v.Args[0]
@@ -3523,23 +3527,23 @@ func rewriteValuegeneric_OpDiv16(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpRsh16Ux64, t)
 		v2 := b.NewValue0(v.Pos, OpRsh16x64, t)
 		v2.AddArg(n)
-		v3 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v3.AuxInt = 15
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 16 - log2(c)
 		v1.AddArg(v4)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = log2(c)
 		v.AddArg(v5)
 		return true
 	}
 	// match: (Div16 <t> x (Const16 [c]))
 	// cond: smagicOK(16,c)
-	// result: (Sub16 <t>     (Rsh32x64 <t>       (Mul32 <fe.TypeUInt32()>         (Const32 <fe.TypeUInt32()> [int64(smagic(16,c).m)])         (SignExt16to32 x))       (Const64 <fe.TypeUInt64()> [16+smagic(16,c).s]))     (Rsh32x64 <t>       (SignExt16to32 x)       (Const64 <fe.TypeUInt64()> [31])))
+	// result: (Sub16 <t>     (Rsh32x64 <t>       (Mul32 <types.UInt32>         (Const32 <types.UInt32> [int64(smagic(16,c).m)])         (SignExt16to32 x))       (Const64 <types.UInt64> [16+smagic(16,c).s]))     (Rsh32x64 <t>       (SignExt16to32 x)       (Const64 <types.UInt64> [31])))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -3554,23 +3558,23 @@ func rewriteValuegeneric_OpDiv16(v *Value) bool {
 		v.reset(OpSub16)
 		v.Type = t
 		v0 := b.NewValue0(v.Pos, OpRsh32x64, t)
-		v1 := b.NewValue0(v.Pos, OpMul32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMul32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v2.AuxInt = int64(smagic(16, c).m)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v3 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 16 + smagic(16, c).s
 		v0.AddArg(v4)
 		v.AddArg(v0)
 		v5 := b.NewValue0(v.Pos, OpRsh32x64, t)
-		v6 := b.NewValue0(v.Pos, OpSignExt16to32, fe.TypeInt32())
+		v6 := b.NewValue0(v.Pos, OpSignExt16to32, types.Int32)
 		v6.AddArg(x)
 		v5.AddArg(v6)
-		v7 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v7 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v7.AuxInt = 31
 		v5.AddArg(v7)
 		v.AddArg(v5)
@@ -3583,8 +3587,8 @@ func rewriteValuegeneric_OpDiv16u(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div16u (Const16 [c])  (Const16 [d]))
 	// cond: d != 0
 	// result: (Const16 [int64(int16(uint16(c)/uint16(d)))])
@@ -3608,7 +3612,7 @@ func rewriteValuegeneric_OpDiv16u(v *Value) bool {
 	}
 	// match: (Div16u n (Const16 [c]))
 	// cond: isPowerOfTwo(c&0xffff)
-	// result: (Rsh16Ux64 n (Const64 <fe.TypeUInt64()> [log2(c&0xffff)]))
+	// result: (Rsh16Ux64 n (Const64 <types.UInt64> [log2(c&0xffff)]))
 	for {
 		n := v.Args[0]
 		v_1 := v.Args[1]
@@ -3621,14 +3625,14 @@ func rewriteValuegeneric_OpDiv16u(v *Value) bool {
 		}
 		v.reset(OpRsh16Ux64)
 		v.AddArg(n)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = log2(c & 0xffff)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Div16u x (Const16 [c]))
 	// cond: umagicOK(16, c) && config.RegSize == 8
-	// result: (Trunc64to16     (Rsh64Ux64 <fe.TypeUInt64()>       (Mul64 <fe.TypeUInt64()>         (Const64 <fe.TypeUInt64()> [int64(1<<16+umagic(16,c).m)])         (ZeroExt16to64 x))       (Const64 <fe.TypeUInt64()> [16+umagic(16,c).s])))
+	// result: (Trunc64to16     (Rsh64Ux64 <types.UInt64>       (Mul64 <types.UInt64>         (Const64 <types.UInt64> [int64(1<<16+umagic(16,c).m)])         (ZeroExt16to64 x))       (Const64 <types.UInt64> [16+umagic(16,c).s])))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -3640,16 +3644,16 @@ func rewriteValuegeneric_OpDiv16u(v *Value) bool {
 			break
 		}
 		v.reset(OpTrunc64to16)
-		v0 := b.NewValue0(v.Pos, OpRsh64Ux64, fe.TypeUInt64())
-		v1 := b.NewValue0(v.Pos, OpMul64, fe.TypeUInt64())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpRsh64Ux64, types.UInt64)
+		v1 := b.NewValue0(v.Pos, OpMul64, types.UInt64)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = int64(1<<16 + umagic(16, c).m)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to64, types.UInt64)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 16 + umagic(16, c).s
 		v0.AddArg(v4)
 		v.AddArg(v0)
@@ -3657,7 +3661,7 @@ func rewriteValuegeneric_OpDiv16u(v *Value) bool {
 	}
 	// match: (Div16u x (Const16 [c]))
 	// cond: umagicOK(16, c) && config.RegSize == 4 && umagic(16,c).m&1 == 0
-	// result: (Trunc32to16     (Rsh32Ux64 <fe.TypeUInt32()>       (Mul32 <fe.TypeUInt32()>         (Const32 <fe.TypeUInt32()> [int64(1<<15+umagic(16,c).m/2)])         (ZeroExt16to32 x))       (Const64 <fe.TypeUInt64()> [16+umagic(16,c).s-1])))
+	// result: (Trunc32to16     (Rsh32Ux64 <types.UInt32>       (Mul32 <types.UInt32>         (Const32 <types.UInt32> [int64(1<<15+umagic(16,c).m/2)])         (ZeroExt16to32 x))       (Const64 <types.UInt64> [16+umagic(16,c).s-1])))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -3669,16 +3673,16 @@ func rewriteValuegeneric_OpDiv16u(v *Value) bool {
 			break
 		}
 		v.reset(OpTrunc32to16)
-		v0 := b.NewValue0(v.Pos, OpRsh32Ux64, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpMul32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpRsh32Ux64, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpMul32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v2.AuxInt = int64(1<<15 + umagic(16, c).m/2)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 16 + umagic(16, c).s - 1
 		v0.AddArg(v4)
 		v.AddArg(v0)
@@ -3686,7 +3690,7 @@ func rewriteValuegeneric_OpDiv16u(v *Value) bool {
 	}
 	// match: (Div16u x (Const16 [c]))
 	// cond: umagicOK(16, c) && config.RegSize == 4 && c&1 == 0
-	// result: (Trunc32to16     (Rsh32Ux64 <fe.TypeUInt32()>       (Mul32 <fe.TypeUInt32()>         (Const32 <fe.TypeUInt32()> [int64(1<<15+(umagic(16,c).m+1)/2)])         (Rsh32Ux64 <fe.TypeUInt32()> (ZeroExt16to32 x) (Const64 <fe.TypeUInt64()> [1])))       (Const64 <fe.TypeUInt64()> [16+umagic(16,c).s-2])))
+	// result: (Trunc32to16     (Rsh32Ux64 <types.UInt32>       (Mul32 <types.UInt32>         (Const32 <types.UInt32> [int64(1<<15+(umagic(16,c).m+1)/2)])         (Rsh32Ux64 <types.UInt32> (ZeroExt16to32 x) (Const64 <types.UInt64> [1])))       (Const64 <types.UInt64> [16+umagic(16,c).s-2])))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -3698,21 +3702,21 @@ func rewriteValuegeneric_OpDiv16u(v *Value) bool {
 			break
 		}
 		v.reset(OpTrunc32to16)
-		v0 := b.NewValue0(v.Pos, OpRsh32Ux64, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpMul32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpRsh32Ux64, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpMul32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v2.AuxInt = int64(1<<15 + (umagic(16, c).m+1)/2)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpRsh32Ux64, fe.TypeUInt32())
-		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpRsh32Ux64, types.UInt32)
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v4.AddArg(x)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 1
 		v3.AddArg(v5)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v6 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v6.AuxInt = 16 + umagic(16, c).s - 2
 		v0.AddArg(v6)
 		v.AddArg(v0)
@@ -3720,7 +3724,7 @@ func rewriteValuegeneric_OpDiv16u(v *Value) bool {
 	}
 	// match: (Div16u x (Const16 [c]))
 	// cond: umagicOK(16, c) && config.RegSize == 4
-	// result: (Trunc32to16     (Rsh32Ux64 <fe.TypeUInt32()>       (Avg32u         (Lsh32x64 <fe.TypeUInt32()> (ZeroExt16to32 x) (Const64 <fe.TypeUInt64()> [16]))         (Mul32 <fe.TypeUInt32()>           (Const32 <fe.TypeUInt32()> [int64(umagic(16,c).m)])           (ZeroExt16to32 x)))       (Const64 <fe.TypeUInt64()> [16+umagic(16,c).s-1])))
+	// result: (Trunc32to16     (Rsh32Ux64 <types.UInt32>       (Avg32u         (Lsh32x64 <types.UInt32> (ZeroExt16to32 x) (Const64 <types.UInt64> [16]))         (Mul32 <types.UInt32>           (Const32 <types.UInt32> [int64(umagic(16,c).m)])           (ZeroExt16to32 x)))       (Const64 <types.UInt64> [16+umagic(16,c).s-1])))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -3732,26 +3736,26 @@ func rewriteValuegeneric_OpDiv16u(v *Value) bool {
 			break
 		}
 		v.reset(OpTrunc32to16)
-		v0 := b.NewValue0(v.Pos, OpRsh32Ux64, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpAvg32u, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpLsh32x64, fe.TypeUInt32())
-		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpRsh32Ux64, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpAvg32u, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpLsh32x64, types.UInt32)
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 16
 		v2.AddArg(v4)
 		v1.AddArg(v2)
-		v5 := b.NewValue0(v.Pos, OpMul32, fe.TypeUInt32())
-		v6 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpMul32, types.UInt32)
+		v6 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v6.AuxInt = int64(umagic(16, c).m)
 		v5.AddArg(v6)
-		v7 := b.NewValue0(v.Pos, OpZeroExt16to32, fe.TypeUInt32())
+		v7 := b.NewValue0(v.Pos, OpZeroExt16to32, types.UInt32)
 		v7.AddArg(x)
 		v5.AddArg(v7)
 		v1.AddArg(v5)
 		v0.AddArg(v1)
-		v8 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v8 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v8.AuxInt = 16 + umagic(16, c).s - 1
 		v0.AddArg(v8)
 		v.AddArg(v0)
@@ -3764,8 +3768,8 @@ func rewriteValuegeneric_OpDiv32(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div32  (Const32 [c])  (Const32 [d]))
 	// cond: d != 0
 	// result: (Const32 [int64(int32(c)/int32(d))])
@@ -3812,7 +3816,7 @@ func rewriteValuegeneric_OpDiv32(v *Value) bool {
 	}
 	// match: (Div32 <t> x (Const32 [-1<<31]))
 	// cond:
-	// result: (Rsh32Ux64 (And32 <t> x (Neg32 <t> x)) (Const64 <fe.TypeUInt64()> [31]))
+	// result: (Rsh32Ux64 (And32 <t> x (Neg32 <t> x)) (Const64 <types.UInt64> [31]))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -3830,14 +3834,14 @@ func rewriteValuegeneric_OpDiv32(v *Value) bool {
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 31
 		v.AddArg(v2)
 		return true
 	}
 	// match: (Div32 <t> n (Const32 [c]))
 	// cond: isPowerOfTwo(c)
-	// result: (Rsh32x64     (Add32 <t> n (Rsh32Ux64 <t> (Rsh32x64 <t> n (Const64 <fe.TypeUInt64()> [31])) (Const64 <fe.TypeUInt64()> [32-log2(c)])))     (Const64 <fe.TypeUInt64()> [log2(c)]))
+	// result: (Rsh32x64     (Add32 <t> n (Rsh32Ux64 <t> (Rsh32x64 <t> n (Const64 <types.UInt64> [31])) (Const64 <types.UInt64> [32-log2(c)])))     (Const64 <types.UInt64> [log2(c)]))
 	for {
 		t := v.Type
 		n := v.Args[0]
@@ -3855,23 +3859,23 @@ func rewriteValuegeneric_OpDiv32(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpRsh32Ux64, t)
 		v2 := b.NewValue0(v.Pos, OpRsh32x64, t)
 		v2.AddArg(n)
-		v3 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v3.AuxInt = 31
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 32 - log2(c)
 		v1.AddArg(v4)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = log2(c)
 		v.AddArg(v5)
 		return true
 	}
 	// match: (Div32 <t> x (Const32 [c]))
 	// cond: smagicOK(32,c) && config.RegSize == 8
-	// result: (Sub32 <t>     (Rsh64x64 <t>       (Mul64 <fe.TypeUInt64()>         (Const64 <fe.TypeUInt64()> [int64(smagic(32,c).m)])         (SignExt32to64 x))       (Const64 <fe.TypeUInt64()> [32+smagic(32,c).s]))     (Rsh64x64 <t>       (SignExt32to64 x)       (Const64 <fe.TypeUInt64()> [63])))
+	// result: (Sub32 <t>     (Rsh64x64 <t>       (Mul64 <types.UInt64>         (Const64 <types.UInt64> [int64(smagic(32,c).m)])         (SignExt32to64 x))       (Const64 <types.UInt64> [32+smagic(32,c).s]))     (Rsh64x64 <t>       (SignExt32to64 x)       (Const64 <types.UInt64> [63])))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -3886,23 +3890,23 @@ func rewriteValuegeneric_OpDiv32(v *Value) bool {
 		v.reset(OpSub32)
 		v.Type = t
 		v0 := b.NewValue0(v.Pos, OpRsh64x64, t)
-		v1 := b.NewValue0(v.Pos, OpMul64, fe.TypeUInt64())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpMul64, types.UInt64)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = int64(smagic(32, c).m)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v3 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 32 + smagic(32, c).s
 		v0.AddArg(v4)
 		v.AddArg(v0)
 		v5 := b.NewValue0(v.Pos, OpRsh64x64, t)
-		v6 := b.NewValue0(v.Pos, OpSignExt32to64, fe.TypeInt64())
+		v6 := b.NewValue0(v.Pos, OpSignExt32to64, types.Int64)
 		v6.AddArg(x)
 		v5.AddArg(v6)
-		v7 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v7 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v7.AuxInt = 63
 		v5.AddArg(v7)
 		v.AddArg(v5)
@@ -3910,7 +3914,7 @@ func rewriteValuegeneric_OpDiv32(v *Value) bool {
 	}
 	// match: (Div32 <t> x (Const32 [c]))
 	// cond: smagicOK(32,c) && config.RegSize == 4 && smagic(32,c).m&1 == 0
-	// result: (Sub32 <t>     (Rsh32x64 <t>       (Hmul32 <t>         (Const32 <fe.TypeUInt32()> [int64(int32(smagic(32,c).m/2))])         x)       (Const64 <fe.TypeUInt64()> [smagic(32,c).s-1]))     (Rsh32x64 <t>       x       (Const64 <fe.TypeUInt64()> [31])))
+	// result: (Sub32 <t>     (Rsh32x64 <t>       (Hmul32 <t>         (Const32 <types.UInt32> [int64(int32(smagic(32,c).m/2))])         x)       (Const64 <types.UInt64> [smagic(32,c).s-1]))     (Rsh32x64 <t>       x       (Const64 <types.UInt64> [31])))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -3926,18 +3930,18 @@ func rewriteValuegeneric_OpDiv32(v *Value) bool {
 		v.Type = t
 		v0 := b.NewValue0(v.Pos, OpRsh32x64, t)
 		v1 := b.NewValue0(v.Pos, OpHmul32, t)
-		v2 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v2.AuxInt = int64(int32(smagic(32, c).m / 2))
 		v1.AddArg(v2)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v3 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v3.AuxInt = smagic(32, c).s - 1
 		v0.AddArg(v3)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpRsh32x64, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 31
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -3945,7 +3949,7 @@ func rewriteValuegeneric_OpDiv32(v *Value) bool {
 	}
 	// match: (Div32 <t> x (Const32 [c]))
 	// cond: smagicOK(32,c) && config.RegSize == 4 && smagic(32,c).m&1 != 0
-	// result: (Sub32 <t>     (Rsh32x64 <t>       (Add32 <t>         (Hmul32 <t>           (Const32 <fe.TypeUInt32()> [int64(int32(smagic(32,c).m))])           x)         x)       (Const64 <fe.TypeUInt64()> [smagic(32,c).s]))     (Rsh32x64 <t>       x       (Const64 <fe.TypeUInt64()> [31])))
+	// result: (Sub32 <t>     (Rsh32x64 <t>       (Add32 <t>         (Hmul32 <t>           (Const32 <types.UInt32> [int64(int32(smagic(32,c).m))])           x)         x)       (Const64 <types.UInt64> [smagic(32,c).s]))     (Rsh32x64 <t>       x       (Const64 <types.UInt64> [31])))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -3962,20 +3966,20 @@ func rewriteValuegeneric_OpDiv32(v *Value) bool {
 		v0 := b.NewValue0(v.Pos, OpRsh32x64, t)
 		v1 := b.NewValue0(v.Pos, OpAdd32, t)
 		v2 := b.NewValue0(v.Pos, OpHmul32, t)
-		v3 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v3.AuxInt = int64(int32(smagic(32, c).m))
 		v2.AddArg(v3)
 		v2.AddArg(x)
 		v1.AddArg(v2)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = smagic(32, c).s
 		v0.AddArg(v4)
 		v.AddArg(v0)
 		v5 := b.NewValue0(v.Pos, OpRsh32x64, t)
 		v5.AddArg(x)
-		v6 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v6.AuxInt = 31
 		v5.AddArg(v6)
 		v.AddArg(v5)
@@ -4042,8 +4046,8 @@ func rewriteValuegeneric_OpDiv32u(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div32u (Const32 [c])  (Const32 [d]))
 	// cond: d != 0
 	// result: (Const32 [int64(int32(uint32(c)/uint32(d)))])
@@ -4067,7 +4071,7 @@ func rewriteValuegeneric_OpDiv32u(v *Value) bool {
 	}
 	// match: (Div32u n (Const32 [c]))
 	// cond: isPowerOfTwo(c&0xffffffff)
-	// result: (Rsh32Ux64 n (Const64 <fe.TypeUInt64()> [log2(c&0xffffffff)]))
+	// result: (Rsh32Ux64 n (Const64 <types.UInt64> [log2(c&0xffffffff)]))
 	for {
 		n := v.Args[0]
 		v_1 := v.Args[1]
@@ -4080,14 +4084,14 @@ func rewriteValuegeneric_OpDiv32u(v *Value) bool {
 		}
 		v.reset(OpRsh32Ux64)
 		v.AddArg(n)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = log2(c & 0xffffffff)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Div32u x (Const32 [c]))
 	// cond: umagicOK(32, c) && config.RegSize == 4 && umagic(32,c).m&1 == 0
-	// result: (Rsh32Ux64 <fe.TypeUInt32()>     (Hmul32u <fe.TypeUInt32()>       (Const32 <fe.TypeUInt32()> [int64(int32(1<<31+umagic(32,c).m/2))])       x)     (Const64 <fe.TypeUInt64()> [umagic(32,c).s-1]))
+	// result: (Rsh32Ux64 <types.UInt32>     (Hmul32u <types.UInt32>       (Const32 <types.UInt32> [int64(int32(1<<31+umagic(32,c).m/2))])       x)     (Const64 <types.UInt64> [umagic(32,c).s-1]))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -4099,21 +4103,21 @@ func rewriteValuegeneric_OpDiv32u(v *Value) bool {
 			break
 		}
 		v.reset(OpRsh32Ux64)
-		v.Type = fe.TypeUInt32()
-		v0 := b.NewValue0(v.Pos, OpHmul32u, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v.Type = types.UInt32
+		v0 := b.NewValue0(v.Pos, OpHmul32u, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v1.AuxInt = int64(int32(1<<31 + umagic(32, c).m/2))
 		v0.AddArg(v1)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = umagic(32, c).s - 1
 		v.AddArg(v2)
 		return true
 	}
 	// match: (Div32u x (Const32 [c]))
 	// cond: umagicOK(32, c) && config.RegSize == 4 && c&1 == 0
-	// result: (Rsh32Ux64 <fe.TypeUInt32()>     (Hmul32u <fe.TypeUInt32()>       (Const32 <fe.TypeUInt32()> [int64(int32(1<<31+(umagic(32,c).m+1)/2))])       (Rsh32Ux64 <fe.TypeUInt32()> x (Const64 <fe.TypeUInt64()> [1])))     (Const64 <fe.TypeUInt64()> [umagic(32,c).s-2]))
+	// result: (Rsh32Ux64 <types.UInt32>     (Hmul32u <types.UInt32>       (Const32 <types.UInt32> [int64(int32(1<<31+(umagic(32,c).m+1)/2))])       (Rsh32Ux64 <types.UInt32> x (Const64 <types.UInt64> [1])))     (Const64 <types.UInt64> [umagic(32,c).s-2]))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -4125,26 +4129,26 @@ func rewriteValuegeneric_OpDiv32u(v *Value) bool {
 			break
 		}
 		v.reset(OpRsh32Ux64)
-		v.Type = fe.TypeUInt32()
-		v0 := b.NewValue0(v.Pos, OpHmul32u, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v.Type = types.UInt32
+		v0 := b.NewValue0(v.Pos, OpHmul32u, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v1.AuxInt = int64(int32(1<<31 + (umagic(32, c).m+1)/2))
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpRsh32Ux64, fe.TypeUInt32())
+		v2 := b.NewValue0(v.Pos, OpRsh32Ux64, types.UInt32)
 		v2.AddArg(x)
-		v3 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v3.AuxInt = 1
 		v2.AddArg(v3)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = umagic(32, c).s - 2
 		v.AddArg(v4)
 		return true
 	}
 	// match: (Div32u x (Const32 [c]))
 	// cond: umagicOK(32, c) && config.RegSize == 4
-	// result: (Rsh32Ux64 <fe.TypeUInt32()>     (Avg32u       x       (Hmul32u <fe.TypeUInt32()>         (Const32 <fe.TypeUInt32()> [int64(int32(umagic(32,c).m))])         x))     (Const64 <fe.TypeUInt64()> [umagic(32,c).s-1]))
+	// result: (Rsh32Ux64 <types.UInt32>     (Avg32u       x       (Hmul32u <types.UInt32>         (Const32 <types.UInt32> [int64(int32(umagic(32,c).m))])         x))     (Const64 <types.UInt64> [umagic(32,c).s-1]))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -4156,24 +4160,24 @@ func rewriteValuegeneric_OpDiv32u(v *Value) bool {
 			break
 		}
 		v.reset(OpRsh32Ux64)
-		v.Type = fe.TypeUInt32()
-		v0 := b.NewValue0(v.Pos, OpAvg32u, fe.TypeUInt32())
+		v.Type = types.UInt32
+		v0 := b.NewValue0(v.Pos, OpAvg32u, types.UInt32)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpHmul32u, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpHmul32u, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v2.AuxInt = int64(int32(umagic(32, c).m))
 		v1.AddArg(v2)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v3.AuxInt = umagic(32, c).s - 1
 		v.AddArg(v3)
 		return true
 	}
 	// match: (Div32u x (Const32 [c]))
 	// cond: umagicOK(32, c) && config.RegSize == 8 && umagic(32,c).m&1 == 0
-	// result: (Trunc64to32     (Rsh64Ux64 <fe.TypeUInt64()>       (Mul64 <fe.TypeUInt64()>         (Const64 <fe.TypeUInt64()> [int64(1<<31+umagic(32,c).m/2)])         (ZeroExt32to64 x))       (Const64 <fe.TypeUInt64()> [32+umagic(32,c).s-1])))
+	// result: (Trunc64to32     (Rsh64Ux64 <types.UInt64>       (Mul64 <types.UInt64>         (Const64 <types.UInt64> [int64(1<<31+umagic(32,c).m/2)])         (ZeroExt32to64 x))       (Const64 <types.UInt64> [32+umagic(32,c).s-1])))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -4185,16 +4189,16 @@ func rewriteValuegeneric_OpDiv32u(v *Value) bool {
 			break
 		}
 		v.reset(OpTrunc64to32)
-		v0 := b.NewValue0(v.Pos, OpRsh64Ux64, fe.TypeUInt64())
-		v1 := b.NewValue0(v.Pos, OpMul64, fe.TypeUInt64())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpRsh64Ux64, types.UInt64)
+		v1 := b.NewValue0(v.Pos, OpMul64, types.UInt64)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = int64(1<<31 + umagic(32, c).m/2)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 32 + umagic(32, c).s - 1
 		v0.AddArg(v4)
 		v.AddArg(v0)
@@ -4202,7 +4206,7 @@ func rewriteValuegeneric_OpDiv32u(v *Value) bool {
 	}
 	// match: (Div32u x (Const32 [c]))
 	// cond: umagicOK(32, c) && config.RegSize == 8 && c&1 == 0
-	// result: (Trunc64to32     (Rsh64Ux64 <fe.TypeUInt64()>       (Mul64 <fe.TypeUInt64()>         (Const64 <fe.TypeUInt64()> [int64(1<<31+(umagic(32,c).m+1)/2)])         (Rsh64Ux64 <fe.TypeUInt64()> (ZeroExt32to64 x) (Const64 <fe.TypeUInt64()> [1])))       (Const64 <fe.TypeUInt64()> [32+umagic(32,c).s-2])))
+	// result: (Trunc64to32     (Rsh64Ux64 <types.UInt64>       (Mul64 <types.UInt64>         (Const64 <types.UInt64> [int64(1<<31+(umagic(32,c).m+1)/2)])         (Rsh64Ux64 <types.UInt64> (ZeroExt32to64 x) (Const64 <types.UInt64> [1])))       (Const64 <types.UInt64> [32+umagic(32,c).s-2])))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -4214,21 +4218,21 @@ func rewriteValuegeneric_OpDiv32u(v *Value) bool {
 			break
 		}
 		v.reset(OpTrunc64to32)
-		v0 := b.NewValue0(v.Pos, OpRsh64Ux64, fe.TypeUInt64())
-		v1 := b.NewValue0(v.Pos, OpMul64, fe.TypeUInt64())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpRsh64Ux64, types.UInt64)
+		v1 := b.NewValue0(v.Pos, OpMul64, types.UInt64)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = int64(1<<31 + (umagic(32, c).m+1)/2)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpRsh64Ux64, fe.TypeUInt64())
-		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpRsh64Ux64, types.UInt64)
+		v4 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v4.AddArg(x)
 		v3.AddArg(v4)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 1
 		v3.AddArg(v5)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v6 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v6.AuxInt = 32 + umagic(32, c).s - 2
 		v0.AddArg(v6)
 		v.AddArg(v0)
@@ -4236,7 +4240,7 @@ func rewriteValuegeneric_OpDiv32u(v *Value) bool {
 	}
 	// match: (Div32u x (Const32 [c]))
 	// cond: umagicOK(32, c) && config.RegSize == 8
-	// result: (Trunc64to32     (Rsh64Ux64 <fe.TypeUInt64()>       (Avg64u         (Lsh64x64 <fe.TypeUInt64()> (ZeroExt32to64 x) (Const64 <fe.TypeUInt64()> [32]))         (Mul64 <fe.TypeUInt64()>           (Const64 <fe.TypeUInt32()> [int64(umagic(32,c).m)])           (ZeroExt32to64 x)))       (Const64 <fe.TypeUInt64()> [32+umagic(32,c).s-1])))
+	// result: (Trunc64to32     (Rsh64Ux64 <types.UInt64>       (Avg64u         (Lsh64x64 <types.UInt64> (ZeroExt32to64 x) (Const64 <types.UInt64> [32]))         (Mul64 <types.UInt64>           (Const64 <types.UInt32> [int64(umagic(32,c).m)])           (ZeroExt32to64 x)))       (Const64 <types.UInt64> [32+umagic(32,c).s-1])))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -4248,26 +4252,26 @@ func rewriteValuegeneric_OpDiv32u(v *Value) bool {
 			break
 		}
 		v.reset(OpTrunc64to32)
-		v0 := b.NewValue0(v.Pos, OpRsh64Ux64, fe.TypeUInt64())
-		v1 := b.NewValue0(v.Pos, OpAvg64u, fe.TypeUInt64())
-		v2 := b.NewValue0(v.Pos, OpLsh64x64, fe.TypeUInt64())
-		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpRsh64Ux64, types.UInt64)
+		v1 := b.NewValue0(v.Pos, OpAvg64u, types.UInt64)
+		v2 := b.NewValue0(v.Pos, OpLsh64x64, types.UInt64)
+		v3 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v3.AddArg(x)
 		v2.AddArg(v3)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 32
 		v2.AddArg(v4)
 		v1.AddArg(v2)
-		v5 := b.NewValue0(v.Pos, OpMul64, fe.TypeUInt64())
-		v6 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt32())
+		v5 := b.NewValue0(v.Pos, OpMul64, types.UInt64)
+		v6 := b.NewValue0(v.Pos, OpConst64, types.UInt32)
 		v6.AuxInt = int64(umagic(32, c).m)
 		v5.AddArg(v6)
-		v7 := b.NewValue0(v.Pos, OpZeroExt32to64, fe.TypeUInt64())
+		v7 := b.NewValue0(v.Pos, OpZeroExt32to64, types.UInt64)
 		v7.AddArg(x)
 		v5.AddArg(v7)
 		v1.AddArg(v5)
 		v0.AddArg(v1)
-		v8 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v8 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v8.AuxInt = 32 + umagic(32, c).s - 1
 		v0.AddArg(v8)
 		v.AddArg(v0)
@@ -4278,8 +4282,8 @@ func rewriteValuegeneric_OpDiv32u(v *Value) bool {
 func rewriteValuegeneric_OpDiv64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div64  (Const64 [c])  (Const64 [d]))
 	// cond: d != 0
 	// result: (Const64 [c/d])
@@ -4326,7 +4330,7 @@ func rewriteValuegeneric_OpDiv64(v *Value) bool {
 	}
 	// match: (Div64 <t> x (Const64 [-1<<63]))
 	// cond:
-	// result: (Rsh64Ux64 (And64 <t> x (Neg64 <t> x)) (Const64 <fe.TypeUInt64()> [63]))
+	// result: (Rsh64Ux64 (And64 <t> x (Neg64 <t> x)) (Const64 <types.UInt64> [63]))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -4344,14 +4348,14 @@ func rewriteValuegeneric_OpDiv64(v *Value) bool {
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 63
 		v.AddArg(v2)
 		return true
 	}
 	// match: (Div64 <t> n (Const64 [c]))
 	// cond: isPowerOfTwo(c)
-	// result: (Rsh64x64     (Add64 <t> n (Rsh64Ux64 <t> (Rsh64x64 <t> n (Const64 <fe.TypeUInt64()> [63])) (Const64 <fe.TypeUInt64()> [64-log2(c)])))     (Const64 <fe.TypeUInt64()> [log2(c)]))
+	// result: (Rsh64x64     (Add64 <t> n (Rsh64Ux64 <t> (Rsh64x64 <t> n (Const64 <types.UInt64> [63])) (Const64 <types.UInt64> [64-log2(c)])))     (Const64 <types.UInt64> [log2(c)]))
 	for {
 		t := v.Type
 		n := v.Args[0]
@@ -4369,23 +4373,23 @@ func rewriteValuegeneric_OpDiv64(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpRsh64Ux64, t)
 		v2 := b.NewValue0(v.Pos, OpRsh64x64, t)
 		v2.AddArg(n)
-		v3 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v3.AuxInt = 63
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 64 - log2(c)
 		v1.AddArg(v4)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = log2(c)
 		v.AddArg(v5)
 		return true
 	}
 	// match: (Div64 <t> x (Const64 [c]))
 	// cond: smagicOK(64,c) && smagic(64,c).m&1 == 0
-	// result: (Sub64 <t>     (Rsh64x64 <t>       (Hmul64 <t>         (Const64 <fe.TypeUInt64()> [int64(smagic(64,c).m/2)])         x)       (Const64 <fe.TypeUInt64()> [smagic(64,c).s-1]))     (Rsh64x64 <t>       x       (Const64 <fe.TypeUInt64()> [63])))
+	// result: (Sub64 <t>     (Rsh64x64 <t>       (Hmul64 <t>         (Const64 <types.UInt64> [int64(smagic(64,c).m/2)])         x)       (Const64 <types.UInt64> [smagic(64,c).s-1]))     (Rsh64x64 <t>       x       (Const64 <types.UInt64> [63])))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -4401,18 +4405,18 @@ func rewriteValuegeneric_OpDiv64(v *Value) bool {
 		v.Type = t
 		v0 := b.NewValue0(v.Pos, OpRsh64x64, t)
 		v1 := b.NewValue0(v.Pos, OpHmul64, t)
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = int64(smagic(64, c).m / 2)
 		v1.AddArg(v2)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v3 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v3.AuxInt = smagic(64, c).s - 1
 		v0.AddArg(v3)
 		v.AddArg(v0)
 		v4 := b.NewValue0(v.Pos, OpRsh64x64, t)
 		v4.AddArg(x)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = 63
 		v4.AddArg(v5)
 		v.AddArg(v4)
@@ -4420,7 +4424,7 @@ func rewriteValuegeneric_OpDiv64(v *Value) bool {
 	}
 	// match: (Div64 <t> x (Const64 [c]))
 	// cond: smagicOK(64,c) && smagic(64,c).m&1 != 0
-	// result: (Sub64 <t>     (Rsh64x64 <t>       (Add64 <t>         (Hmul64 <t>           (Const64 <fe.TypeUInt64()> [int64(smagic(64,c).m)])           x)         x)       (Const64 <fe.TypeUInt64()> [smagic(64,c).s]))     (Rsh64x64 <t>       x       (Const64 <fe.TypeUInt64()> [63])))
+	// result: (Sub64 <t>     (Rsh64x64 <t>       (Add64 <t>         (Hmul64 <t>           (Const64 <types.UInt64> [int64(smagic(64,c).m)])           x)         x)       (Const64 <types.UInt64> [smagic(64,c).s]))     (Rsh64x64 <t>       x       (Const64 <types.UInt64> [63])))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -4437,20 +4441,20 @@ func rewriteValuegeneric_OpDiv64(v *Value) bool {
 		v0 := b.NewValue0(v.Pos, OpRsh64x64, t)
 		v1 := b.NewValue0(v.Pos, OpAdd64, t)
 		v2 := b.NewValue0(v.Pos, OpHmul64, t)
-		v3 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v3.AuxInt = int64(smagic(64, c).m)
 		v2.AddArg(v3)
 		v2.AddArg(x)
 		v1.AddArg(v2)
 		v1.AddArg(x)
 		v0.AddArg(v1)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = smagic(64, c).s
 		v0.AddArg(v4)
 		v.AddArg(v0)
 		v5 := b.NewValue0(v.Pos, OpRsh64x64, t)
 		v5.AddArg(x)
-		v6 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v6 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v6.AuxInt = 63
 		v5.AddArg(v6)
 		v.AddArg(v5)
@@ -4517,8 +4521,8 @@ func rewriteValuegeneric_OpDiv64u(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div64u (Const64 [c])  (Const64 [d]))
 	// cond: d != 0
 	// result: (Const64 [int64(uint64(c)/uint64(d))])
@@ -4542,7 +4546,7 @@ func rewriteValuegeneric_OpDiv64u(v *Value) bool {
 	}
 	// match: (Div64u n (Const64 [c]))
 	// cond: isPowerOfTwo(c)
-	// result: (Rsh64Ux64 n (Const64 <fe.TypeUInt64()> [log2(c)]))
+	// result: (Rsh64Ux64 n (Const64 <types.UInt64> [log2(c)]))
 	for {
 		n := v.Args[0]
 		v_1 := v.Args[1]
@@ -4555,14 +4559,14 @@ func rewriteValuegeneric_OpDiv64u(v *Value) bool {
 		}
 		v.reset(OpRsh64Ux64)
 		v.AddArg(n)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = log2(c)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Div64u x (Const64 [c]))
 	// cond: umagicOK(64, c) && config.RegSize == 8 && umagic(64,c).m&1 == 0
-	// result: (Rsh64Ux64 <fe.TypeUInt64()>     (Hmul64u <fe.TypeUInt64()>       (Const64 <fe.TypeUInt64()> [int64(1<<63+umagic(64,c).m/2)])       x)     (Const64 <fe.TypeUInt64()> [umagic(64,c).s-1]))
+	// result: (Rsh64Ux64 <types.UInt64>     (Hmul64u <types.UInt64>       (Const64 <types.UInt64> [int64(1<<63+umagic(64,c).m/2)])       x)     (Const64 <types.UInt64> [umagic(64,c).s-1]))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -4574,21 +4578,21 @@ func rewriteValuegeneric_OpDiv64u(v *Value) bool {
 			break
 		}
 		v.reset(OpRsh64Ux64)
-		v.Type = fe.TypeUInt64()
-		v0 := b.NewValue0(v.Pos, OpHmul64u, fe.TypeUInt64())
-		v1 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v.Type = types.UInt64
+		v0 := b.NewValue0(v.Pos, OpHmul64u, types.UInt64)
+		v1 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v1.AuxInt = int64(1<<63 + umagic(64, c).m/2)
 		v0.AddArg(v1)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = umagic(64, c).s - 1
 		v.AddArg(v2)
 		return true
 	}
 	// match: (Div64u x (Const64 [c]))
 	// cond: umagicOK(64, c) && config.RegSize == 8 && c&1 == 0
-	// result: (Rsh64Ux64 <fe.TypeUInt64()>     (Hmul64u <fe.TypeUInt64()>       (Const64 <fe.TypeUInt64()> [int64(1<<63+(umagic(64,c).m+1)/2)])       (Rsh64Ux64 <fe.TypeUInt64()> x (Const64 <fe.TypeUInt64()> [1])))     (Const64 <fe.TypeUInt64()> [umagic(64,c).s-2]))
+	// result: (Rsh64Ux64 <types.UInt64>     (Hmul64u <types.UInt64>       (Const64 <types.UInt64> [int64(1<<63+(umagic(64,c).m+1)/2)])       (Rsh64Ux64 <types.UInt64> x (Const64 <types.UInt64> [1])))     (Const64 <types.UInt64> [umagic(64,c).s-2]))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -4600,26 +4604,26 @@ func rewriteValuegeneric_OpDiv64u(v *Value) bool {
 			break
 		}
 		v.reset(OpRsh64Ux64)
-		v.Type = fe.TypeUInt64()
-		v0 := b.NewValue0(v.Pos, OpHmul64u, fe.TypeUInt64())
-		v1 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v.Type = types.UInt64
+		v0 := b.NewValue0(v.Pos, OpHmul64u, types.UInt64)
+		v1 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v1.AuxInt = int64(1<<63 + (umagic(64, c).m+1)/2)
 		v0.AddArg(v1)
-		v2 := b.NewValue0(v.Pos, OpRsh64Ux64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpRsh64Ux64, types.UInt64)
 		v2.AddArg(x)
-		v3 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v3.AuxInt = 1
 		v2.AddArg(v3)
 		v0.AddArg(v2)
 		v.AddArg(v0)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = umagic(64, c).s - 2
 		v.AddArg(v4)
 		return true
 	}
 	// match: (Div64u x (Const64 [c]))
 	// cond: umagicOK(64, c) && config.RegSize == 8
-	// result: (Rsh64Ux64 <fe.TypeUInt64()>     (Avg64u       x       (Hmul64u <fe.TypeUInt64()>         (Const64 <fe.TypeUInt64()> [int64(umagic(64,c).m)])         x))     (Const64 <fe.TypeUInt64()> [umagic(64,c).s-1]))
+	// result: (Rsh64Ux64 <types.UInt64>     (Avg64u       x       (Hmul64u <types.UInt64>         (Const64 <types.UInt64> [int64(umagic(64,c).m)])         x))     (Const64 <types.UInt64> [umagic(64,c).s-1]))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -4631,17 +4635,17 @@ func rewriteValuegeneric_OpDiv64u(v *Value) bool {
 			break
 		}
 		v.reset(OpRsh64Ux64)
-		v.Type = fe.TypeUInt64()
-		v0 := b.NewValue0(v.Pos, OpAvg64u, fe.TypeUInt64())
+		v.Type = types.UInt64
+		v0 := b.NewValue0(v.Pos, OpAvg64u, types.UInt64)
 		v0.AddArg(x)
-		v1 := b.NewValue0(v.Pos, OpHmul64u, fe.TypeUInt64())
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpHmul64u, types.UInt64)
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = int64(umagic(64, c).m)
 		v1.AddArg(v2)
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v3 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v3.AuxInt = umagic(64, c).s - 1
 		v.AddArg(v3)
 		return true
@@ -4651,8 +4655,8 @@ func rewriteValuegeneric_OpDiv64u(v *Value) bool {
 func rewriteValuegeneric_OpDiv8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8   (Const8  [c])  (Const8  [d]))
 	// cond: d != 0
 	// result: (Const8  [int64(int8(c)/int8(d))])
@@ -4699,7 +4703,7 @@ func rewriteValuegeneric_OpDiv8(v *Value) bool {
 	}
 	// match: (Div8  <t> x (Const8  [-1<<7 ]))
 	// cond:
-	// result: (Rsh8Ux64  (And8  <t> x (Neg8  <t> x)) (Const64 <fe.TypeUInt64()> [7 ]))
+	// result: (Rsh8Ux64  (And8  <t> x (Neg8  <t> x)) (Const64 <types.UInt64> [7 ]))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -4717,14 +4721,14 @@ func rewriteValuegeneric_OpDiv8(v *Value) bool {
 		v1.AddArg(x)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v2 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v2 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v2.AuxInt = 7
 		v.AddArg(v2)
 		return true
 	}
 	// match: (Div8  <t> n (Const8  [c]))
 	// cond: isPowerOfTwo(c)
-	// result: (Rsh8x64     (Add8  <t> n (Rsh8Ux64  <t> (Rsh8x64  <t> n (Const64 <fe.TypeUInt64()> [ 7])) (Const64 <fe.TypeUInt64()> [ 8-log2(c)])))     (Const64 <fe.TypeUInt64()> [log2(c)]))
+	// result: (Rsh8x64     (Add8  <t> n (Rsh8Ux64  <t> (Rsh8x64  <t> n (Const64 <types.UInt64> [ 7])) (Const64 <types.UInt64> [ 8-log2(c)])))     (Const64 <types.UInt64> [log2(c)]))
 	for {
 		t := v.Type
 		n := v.Args[0]
@@ -4742,23 +4746,23 @@ func rewriteValuegeneric_OpDiv8(v *Value) bool {
 		v1 := b.NewValue0(v.Pos, OpRsh8Ux64, t)
 		v2 := b.NewValue0(v.Pos, OpRsh8x64, t)
 		v2.AddArg(n)
-		v3 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v3 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v3.AuxInt = 7
 		v2.AddArg(v3)
 		v1.AddArg(v2)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 8 - log2(c)
 		v1.AddArg(v4)
 		v0.AddArg(v1)
 		v.AddArg(v0)
-		v5 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v5 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v5.AuxInt = log2(c)
 		v.AddArg(v5)
 		return true
 	}
 	// match: (Div8 <t> x (Const8 [c]))
 	// cond: smagicOK(8,c)
-	// result: (Sub8 <t>     (Rsh32x64 <t>       (Mul32 <fe.TypeUInt32()>         (Const32 <fe.TypeUInt32()> [int64(smagic(8,c).m)])         (SignExt8to32 x))       (Const64 <fe.TypeUInt64()> [8+smagic(8,c).s]))     (Rsh32x64 <t>       (SignExt8to32 x)       (Const64 <fe.TypeUInt64()> [31])))
+	// result: (Sub8 <t>     (Rsh32x64 <t>       (Mul32 <types.UInt32>         (Const32 <types.UInt32> [int64(smagic(8,c).m)])         (SignExt8to32 x))       (Const64 <types.UInt64> [8+smagic(8,c).s]))     (Rsh32x64 <t>       (SignExt8to32 x)       (Const64 <types.UInt64> [31])))
 	for {
 		t := v.Type
 		x := v.Args[0]
@@ -4773,23 +4777,23 @@ func rewriteValuegeneric_OpDiv8(v *Value) bool {
 		v.reset(OpSub8)
 		v.Type = t
 		v0 := b.NewValue0(v.Pos, OpRsh32x64, t)
-		v1 := b.NewValue0(v.Pos, OpMul32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v1 := b.NewValue0(v.Pos, OpMul32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v2.AuxInt = int64(smagic(8, c).m)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v3 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 8 + smagic(8, c).s
 		v0.AddArg(v4)
 		v.AddArg(v0)
 		v5 := b.NewValue0(v.Pos, OpRsh32x64, t)
-		v6 := b.NewValue0(v.Pos, OpSignExt8to32, fe.TypeInt32())
+		v6 := b.NewValue0(v.Pos, OpSignExt8to32, types.Int32)
 		v6.AddArg(x)
 		v5.AddArg(v6)
-		v7 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v7 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v7.AuxInt = 31
 		v5.AddArg(v7)
 		v.AddArg(v5)
@@ -4800,8 +4804,8 @@ func rewriteValuegeneric_OpDiv8(v *Value) bool {
 func rewriteValuegeneric_OpDiv8u(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Div8u  (Const8  [c])  (Const8  [d]))
 	// cond: d != 0
 	// result: (Const8  [int64(int8(uint8(c)/uint8(d)))])
@@ -4825,7 +4829,7 @@ func rewriteValuegeneric_OpDiv8u(v *Value) bool {
 	}
 	// match: (Div8u  n (Const8  [c]))
 	// cond: isPowerOfTwo(c&0xff)
-	// result: (Rsh8Ux64 n  (Const64 <fe.TypeUInt64()> [log2(c&0xff)]))
+	// result: (Rsh8Ux64 n  (Const64 <types.UInt64> [log2(c&0xff)]))
 	for {
 		n := v.Args[0]
 		v_1 := v.Args[1]
@@ -4838,14 +4842,14 @@ func rewriteValuegeneric_OpDiv8u(v *Value) bool {
 		}
 		v.reset(OpRsh8Ux64)
 		v.AddArg(n)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = log2(c & 0xff)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Div8u x (Const8 [c]))
 	// cond: umagicOK(8, c)
-	// result: (Trunc32to8     (Rsh32Ux64 <fe.TypeUInt32()>       (Mul32 <fe.TypeUInt32()>         (Const32 <fe.TypeUInt32()> [int64(1<<8+umagic(8,c).m)])         (ZeroExt8to32 x))       (Const64 <fe.TypeUInt64()> [8+umagic(8,c).s])))
+	// result: (Trunc32to8     (Rsh32Ux64 <types.UInt32>       (Mul32 <types.UInt32>         (Const32 <types.UInt32> [int64(1<<8+umagic(8,c).m)])         (ZeroExt8to32 x))       (Const64 <types.UInt64> [8+umagic(8,c).s])))
 	for {
 		x := v.Args[0]
 		v_1 := v.Args[1]
@@ -4857,16 +4861,16 @@ func rewriteValuegeneric_OpDiv8u(v *Value) bool {
 			break
 		}
 		v.reset(OpTrunc32to8)
-		v0 := b.NewValue0(v.Pos, OpRsh32Ux64, fe.TypeUInt32())
-		v1 := b.NewValue0(v.Pos, OpMul32, fe.TypeUInt32())
-		v2 := b.NewValue0(v.Pos, OpConst32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpRsh32Ux64, types.UInt32)
+		v1 := b.NewValue0(v.Pos, OpMul32, types.UInt32)
+		v2 := b.NewValue0(v.Pos, OpConst32, types.UInt32)
 		v2.AuxInt = int64(1<<8 + umagic(8, c).m)
 		v1.AddArg(v2)
-		v3 := b.NewValue0(v.Pos, OpZeroExt8to32, fe.TypeUInt32())
+		v3 := b.NewValue0(v.Pos, OpZeroExt8to32, types.UInt32)
 		v3.AddArg(x)
 		v1.AddArg(v3)
 		v0.AddArg(v1)
-		v4 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v4 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v4.AuxInt = 8 + umagic(8, c).s
 		v0.AddArg(v4)
 		v.AddArg(v0)
@@ -5275,8 +5279,8 @@ func rewriteValuegeneric_OpEqB(v *Value) bool {
 func rewriteValuegeneric_OpEqInter(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (EqInter x y)
 	// cond:
 	// result: (EqPtr  (ITab x) (ITab y))
@@ -5284,10 +5288,10 @@ func rewriteValuegeneric_OpEqInter(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpEqPtr)
-		v0 := b.NewValue0(v.Pos, OpITab, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpITab, types.BytePtr)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpITab, fe.TypeBytePtr())
+		v1 := b.NewValue0(v.Pos, OpITab, types.BytePtr)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -5296,8 +5300,8 @@ func rewriteValuegeneric_OpEqInter(v *Value) bool {
 func rewriteValuegeneric_OpEqPtr(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (EqPtr p (ConstNil))
 	// cond:
 	// result: (Not (IsNonNil p))
@@ -5308,7 +5312,7 @@ func rewriteValuegeneric_OpEqPtr(v *Value) bool {
 			break
 		}
 		v.reset(OpNot)
-		v0 := b.NewValue0(v.Pos, OpIsNonNil, fe.TypeBool())
+		v0 := b.NewValue0(v.Pos, OpIsNonNil, types.Bool)
 		v0.AddArg(p)
 		v.AddArg(v0)
 		return true
@@ -5323,7 +5327,7 @@ func rewriteValuegeneric_OpEqPtr(v *Value) bool {
 		}
 		p := v.Args[1]
 		v.reset(OpNot)
-		v0 := b.NewValue0(v.Pos, OpIsNonNil, fe.TypeBool())
+		v0 := b.NewValue0(v.Pos, OpIsNonNil, types.Bool)
 		v0.AddArg(p)
 		v.AddArg(v0)
 		return true
@@ -5333,8 +5337,8 @@ func rewriteValuegeneric_OpEqPtr(v *Value) bool {
 func rewriteValuegeneric_OpEqSlice(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (EqSlice x y)
 	// cond:
 	// result: (EqPtr  (SlicePtr x) (SlicePtr y))
@@ -5342,10 +5346,10 @@ func rewriteValuegeneric_OpEqSlice(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpEqPtr)
-		v0 := b.NewValue0(v.Pos, OpSlicePtr, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpSlicePtr, types.BytePtr)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSlicePtr, fe.TypeBytePtr())
+		v1 := b.NewValue0(v.Pos, OpSlicePtr, types.BytePtr)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -6971,8 +6975,8 @@ func rewriteValuegeneric_OpLsh16x32(v *Value) bool {
 func rewriteValuegeneric_OpLsh16x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh16x64  (Const16 [c]) (Const64 [d]))
 	// cond:
 	// result: (Const16 [int64(int16(c) << uint64(d))])
@@ -7071,7 +7075,7 @@ func rewriteValuegeneric_OpLsh16x64(v *Value) bool {
 	}
 	// match: (Lsh16x64 (Rsh16Ux64 (Lsh16x64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
 	// cond: uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-	// result: (Lsh16x64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+	// result: (Lsh16x64 x (Const64 <types.UInt64> [c1-c2+c3]))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpRsh16Ux64 {
@@ -7102,7 +7106,7 @@ func rewriteValuegeneric_OpLsh16x64(v *Value) bool {
 		}
 		v.reset(OpLsh16x64)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = c1 - c2 + c3
 		v.AddArg(v0)
 		return true
@@ -7226,8 +7230,8 @@ func rewriteValuegeneric_OpLsh32x32(v *Value) bool {
 func rewriteValuegeneric_OpLsh32x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh32x64  (Const32 [c]) (Const64 [d]))
 	// cond:
 	// result: (Const32 [int64(int32(c) << uint64(d))])
@@ -7326,7 +7330,7 @@ func rewriteValuegeneric_OpLsh32x64(v *Value) bool {
 	}
 	// match: (Lsh32x64 (Rsh32Ux64 (Lsh32x64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
 	// cond: uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-	// result: (Lsh32x64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+	// result: (Lsh32x64 x (Const64 <types.UInt64> [c1-c2+c3]))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpRsh32Ux64 {
@@ -7357,7 +7361,7 @@ func rewriteValuegeneric_OpLsh32x64(v *Value) bool {
 		}
 		v.reset(OpLsh32x64)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = c1 - c2 + c3
 		v.AddArg(v0)
 		return true
@@ -7481,8 +7485,8 @@ func rewriteValuegeneric_OpLsh64x32(v *Value) bool {
 func rewriteValuegeneric_OpLsh64x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh64x64  (Const64 [c]) (Const64 [d]))
 	// cond:
 	// result: (Const64 [c << uint64(d)])
@@ -7581,7 +7585,7 @@ func rewriteValuegeneric_OpLsh64x64(v *Value) bool {
 	}
 	// match: (Lsh64x64 (Rsh64Ux64 (Lsh64x64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
 	// cond: uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-	// result: (Lsh64x64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+	// result: (Lsh64x64 x (Const64 <types.UInt64> [c1-c2+c3]))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpRsh64Ux64 {
@@ -7612,7 +7616,7 @@ func rewriteValuegeneric_OpLsh64x64(v *Value) bool {
 		}
 		v.reset(OpLsh64x64)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = c1 - c2 + c3
 		v.AddArg(v0)
 		return true
@@ -7736,8 +7740,8 @@ func rewriteValuegeneric_OpLsh8x32(v *Value) bool {
 func rewriteValuegeneric_OpLsh8x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Lsh8x64   (Const8  [c]) (Const64 [d]))
 	// cond:
 	// result: (Const8  [int64(int8(c) << uint64(d))])
@@ -7836,7 +7840,7 @@ func rewriteValuegeneric_OpLsh8x64(v *Value) bool {
 	}
 	// match: (Lsh8x64 (Rsh8Ux64 (Lsh8x64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
 	// cond: uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-	// result: (Lsh8x64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+	// result: (Lsh8x64 x (Const64 <types.UInt64> [c1-c2+c3]))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpRsh8Ux64 {
@@ -7867,7 +7871,7 @@ func rewriteValuegeneric_OpLsh8x64(v *Value) bool {
 		}
 		v.reset(OpLsh8x64)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = c1 - c2 + c3
 		v.AddArg(v0)
 		return true
@@ -8527,8 +8531,8 @@ func rewriteValuegeneric_OpMod8u(v *Value) bool {
 func rewriteValuegeneric_OpMul16(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mul16  (Const16 [c])  (Const16 [d]))
 	// cond:
 	// result: (Const16 [int64(int16(c*d))])
@@ -8582,7 +8586,7 @@ func rewriteValuegeneric_OpMul16(v *Value) bool {
 	}
 	// match: (Mul16 <t> n (Const16 [c]))
 	// cond: isPowerOfTwo(c)
-	// result: (Lsh16x64 <t> n (Const64 <fe.TypeUInt64()> [log2(c)]))
+	// result: (Lsh16x64 <t> n (Const64 <types.UInt64> [log2(c)]))
 	for {
 		t := v.Type
 		n := v.Args[0]
@@ -8597,14 +8601,14 @@ func rewriteValuegeneric_OpMul16(v *Value) bool {
 		v.reset(OpLsh16x64)
 		v.Type = t
 		v.AddArg(n)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = log2(c)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Mul16 <t> n (Const16 [c]))
 	// cond: t.IsSigned() && isPowerOfTwo(-c)
-	// result: (Neg16 (Lsh16x64 <t> n (Const64 <fe.TypeUInt64()> [log2(-c)])))
+	// result: (Neg16 (Lsh16x64 <t> n (Const64 <types.UInt64> [log2(-c)])))
 	for {
 		t := v.Type
 		n := v.Args[0]
@@ -8619,7 +8623,7 @@ func rewriteValuegeneric_OpMul16(v *Value) bool {
 		v.reset(OpNeg16)
 		v0 := b.NewValue0(v.Pos, OpLsh16x64, t)
 		v0.AddArg(n)
-		v1 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v1.AuxInt = log2(-c)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -8713,8 +8717,8 @@ func rewriteValuegeneric_OpMul16(v *Value) bool {
 func rewriteValuegeneric_OpMul32(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mul32  (Const32 [c])  (Const32 [d]))
 	// cond:
 	// result: (Const32 [int64(int32(c*d))])
@@ -8768,7 +8772,7 @@ func rewriteValuegeneric_OpMul32(v *Value) bool {
 	}
 	// match: (Mul32 <t> n (Const32 [c]))
 	// cond: isPowerOfTwo(c)
-	// result: (Lsh32x64 <t> n (Const64 <fe.TypeUInt64()> [log2(c)]))
+	// result: (Lsh32x64 <t> n (Const64 <types.UInt64> [log2(c)]))
 	for {
 		t := v.Type
 		n := v.Args[0]
@@ -8783,14 +8787,14 @@ func rewriteValuegeneric_OpMul32(v *Value) bool {
 		v.reset(OpLsh32x64)
 		v.Type = t
 		v.AddArg(n)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = log2(c)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Mul32 <t> n (Const32 [c]))
 	// cond: t.IsSigned() && isPowerOfTwo(-c)
-	// result: (Neg32 (Lsh32x64 <t> n (Const64 <fe.TypeUInt64()> [log2(-c)])))
+	// result: (Neg32 (Lsh32x64 <t> n (Const64 <types.UInt64> [log2(-c)])))
 	for {
 		t := v.Type
 		n := v.Args[0]
@@ -8805,7 +8809,7 @@ func rewriteValuegeneric_OpMul32(v *Value) bool {
 		v.reset(OpNeg32)
 		v0 := b.NewValue0(v.Pos, OpLsh32x64, t)
 		v0.AddArg(n)
-		v1 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v1.AuxInt = log2(-c)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -9024,8 +9028,8 @@ func rewriteValuegeneric_OpMul32F(v *Value) bool {
 func rewriteValuegeneric_OpMul64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mul64  (Const64 [c])  (Const64 [d]))
 	// cond:
 	// result: (Const64 [c*d])
@@ -9079,7 +9083,7 @@ func rewriteValuegeneric_OpMul64(v *Value) bool {
 	}
 	// match: (Mul64 <t> n (Const64 [c]))
 	// cond: isPowerOfTwo(c)
-	// result: (Lsh64x64 <t> n (Const64 <fe.TypeUInt64()> [log2(c)]))
+	// result: (Lsh64x64 <t> n (Const64 <types.UInt64> [log2(c)]))
 	for {
 		t := v.Type
 		n := v.Args[0]
@@ -9094,14 +9098,14 @@ func rewriteValuegeneric_OpMul64(v *Value) bool {
 		v.reset(OpLsh64x64)
 		v.Type = t
 		v.AddArg(n)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = log2(c)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Mul64 <t> n (Const64 [c]))
 	// cond: t.IsSigned() && isPowerOfTwo(-c)
-	// result: (Neg64 (Lsh64x64 <t> n (Const64 <fe.TypeUInt64()> [log2(-c)])))
+	// result: (Neg64 (Lsh64x64 <t> n (Const64 <types.UInt64> [log2(-c)])))
 	for {
 		t := v.Type
 		n := v.Args[0]
@@ -9116,7 +9120,7 @@ func rewriteValuegeneric_OpMul64(v *Value) bool {
 		v.reset(OpNeg64)
 		v0 := b.NewValue0(v.Pos, OpLsh64x64, t)
 		v0.AddArg(n)
-		v1 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v1.AuxInt = log2(-c)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -9335,8 +9339,8 @@ func rewriteValuegeneric_OpMul64F(v *Value) bool {
 func rewriteValuegeneric_OpMul8(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Mul8   (Const8 [c])   (Const8 [d]))
 	// cond:
 	// result: (Const8  [int64(int8(c*d))])
@@ -9390,7 +9394,7 @@ func rewriteValuegeneric_OpMul8(v *Value) bool {
 	}
 	// match: (Mul8  <t> n (Const8  [c]))
 	// cond: isPowerOfTwo(c)
-	// result: (Lsh8x64  <t> n (Const64 <fe.TypeUInt64()> [log2(c)]))
+	// result: (Lsh8x64  <t> n (Const64 <types.UInt64> [log2(c)]))
 	for {
 		t := v.Type
 		n := v.Args[0]
@@ -9405,14 +9409,14 @@ func rewriteValuegeneric_OpMul8(v *Value) bool {
 		v.reset(OpLsh8x64)
 		v.Type = t
 		v.AddArg(n)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = log2(c)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Mul8  <t> n (Const8  [c]))
 	// cond: t.IsSigned() && isPowerOfTwo(-c)
-	// result: (Neg8  (Lsh8x64  <t> n (Const64 <fe.TypeUInt64()> [log2(-c)])))
+	// result: (Neg8  (Lsh8x64  <t> n (Const64 <types.UInt64> [log2(-c)])))
 	for {
 		t := v.Type
 		n := v.Args[0]
@@ -9427,7 +9431,7 @@ func rewriteValuegeneric_OpMul8(v *Value) bool {
 		v.reset(OpNeg8)
 		v0 := b.NewValue0(v.Pos, OpLsh8x64, t)
 		v0.AddArg(n)
-		v1 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v1 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v1.AuxInt = log2(-c)
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -10081,8 +10085,8 @@ func rewriteValuegeneric_OpNeqB(v *Value) bool {
 func rewriteValuegeneric_OpNeqInter(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (NeqInter x y)
 	// cond:
 	// result: (NeqPtr (ITab x) (ITab y))
@@ -10090,10 +10094,10 @@ func rewriteValuegeneric_OpNeqInter(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpNeqPtr)
-		v0 := b.NewValue0(v.Pos, OpITab, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpITab, types.BytePtr)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpITab, fe.TypeBytePtr())
+		v1 := b.NewValue0(v.Pos, OpITab, types.BytePtr)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -10131,8 +10135,8 @@ func rewriteValuegeneric_OpNeqPtr(v *Value) bool {
 func rewriteValuegeneric_OpNeqSlice(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (NeqSlice x y)
 	// cond:
 	// result: (NeqPtr (SlicePtr x) (SlicePtr y))
@@ -10140,10 +10144,10 @@ func rewriteValuegeneric_OpNeqSlice(v *Value) bool {
 		x := v.Args[0]
 		y := v.Args[1]
 		v.reset(OpNeqPtr)
-		v0 := b.NewValue0(v.Pos, OpSlicePtr, fe.TypeBytePtr())
+		v0 := b.NewValue0(v.Pos, OpSlicePtr, types.BytePtr)
 		v0.AddArg(x)
 		v.AddArg(v0)
-		v1 := b.NewValue0(v.Pos, OpSlicePtr, fe.TypeBytePtr())
+		v1 := b.NewValue0(v.Pos, OpSlicePtr, types.BytePtr)
 		v1.AddArg(y)
 		v.AddArg(v1)
 		return true
@@ -11937,11 +11941,11 @@ func rewriteValuegeneric_OpPtrIndex(v *Value) bool {
 	_ = b
 	config := b.Func.Config
 	_ = config
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (PtrIndex <t> ptr idx)
 	// cond: config.PtrSize == 4
-	// result: (AddPtr ptr (Mul32 <fe.TypeInt()> idx (Const32 <fe.TypeInt()> [t.ElemType().Size()])))
+	// result: (AddPtr ptr (Mul32 <types.Int> idx (Const32 <types.Int> [t.ElemType().Size()])))
 	for {
 		t := v.Type
 		ptr := v.Args[0]
@@ -11951,9 +11955,9 @@ func rewriteValuegeneric_OpPtrIndex(v *Value) bool {
 		}
 		v.reset(OpAddPtr)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMul32, fe.TypeInt())
+		v0 := b.NewValue0(v.Pos, OpMul32, types.Int)
 		v0.AddArg(idx)
-		v1 := b.NewValue0(v.Pos, OpConst32, fe.TypeInt())
+		v1 := b.NewValue0(v.Pos, OpConst32, types.Int)
 		v1.AuxInt = t.ElemType().Size()
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -11961,7 +11965,7 @@ func rewriteValuegeneric_OpPtrIndex(v *Value) bool {
 	}
 	// match: (PtrIndex <t> ptr idx)
 	// cond: config.PtrSize == 8
-	// result: (AddPtr ptr (Mul64 <fe.TypeInt()> idx (Const64 <fe.TypeInt()> [t.ElemType().Size()])))
+	// result: (AddPtr ptr (Mul64 <types.Int> idx (Const64 <types.Int> [t.ElemType().Size()])))
 	for {
 		t := v.Type
 		ptr := v.Args[0]
@@ -11971,9 +11975,9 @@ func rewriteValuegeneric_OpPtrIndex(v *Value) bool {
 		}
 		v.reset(OpAddPtr)
 		v.AddArg(ptr)
-		v0 := b.NewValue0(v.Pos, OpMul64, fe.TypeInt())
+		v0 := b.NewValue0(v.Pos, OpMul64, types.Int)
 		v0.AddArg(idx)
-		v1 := b.NewValue0(v.Pos, OpConst64, fe.TypeInt())
+		v1 := b.NewValue0(v.Pos, OpConst64, types.Int)
 		v1.AuxInt = t.ElemType().Size()
 		v0.AddArg(v1)
 		v.AddArg(v0)
@@ -12092,8 +12096,8 @@ func rewriteValuegeneric_OpRsh16Ux32(v *Value) bool {
 func rewriteValuegeneric_OpRsh16Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16Ux64 (Const16 [c]) (Const64 [d]))
 	// cond:
 	// result: (Const16 [int64(int16(uint16(c) >> uint64(d)))])
@@ -12192,7 +12196,7 @@ func rewriteValuegeneric_OpRsh16Ux64(v *Value) bool {
 	}
 	// match: (Rsh16Ux64 (Lsh16x64 (Rsh16Ux64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
 	// cond: uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-	// result: (Rsh16Ux64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+	// result: (Rsh16Ux64 x (Const64 <types.UInt64> [c1-c2+c3]))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh16x64 {
@@ -12223,14 +12227,14 @@ func rewriteValuegeneric_OpRsh16Ux64(v *Value) bool {
 		}
 		v.reset(OpRsh16Ux64)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = c1 - c2 + c3
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh16Ux64 (Lsh16x64 x (Const64  [8])) (Const64  [8]))
 	// cond:
-	// result: (ZeroExt8to16  (Trunc16to8  <fe.TypeUInt8()>  x))
+	// result: (ZeroExt8to16  (Trunc16to8  <types.UInt8>  x))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh16x64 {
@@ -12252,7 +12256,7 @@ func rewriteValuegeneric_OpRsh16Ux64(v *Value) bool {
 			break
 		}
 		v.reset(OpZeroExt8to16)
-		v0 := b.NewValue0(v.Pos, OpTrunc16to8, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpTrunc16to8, types.UInt8)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -12376,8 +12380,8 @@ func rewriteValuegeneric_OpRsh16x32(v *Value) bool {
 func rewriteValuegeneric_OpRsh16x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh16x64  (Const16 [c]) (Const64 [d]))
 	// cond:
 	// result: (Const16 [int64(int16(c) >> uint64(d))])
@@ -12460,7 +12464,7 @@ func rewriteValuegeneric_OpRsh16x64(v *Value) bool {
 	}
 	// match: (Rsh16x64 (Lsh16x64 x (Const64  [8])) (Const64  [8]))
 	// cond:
-	// result: (SignExt8to16  (Trunc16to8  <fe.TypeInt8()>  x))
+	// result: (SignExt8to16  (Trunc16to8  <types.Int8>  x))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh16x64 {
@@ -12482,7 +12486,7 @@ func rewriteValuegeneric_OpRsh16x64(v *Value) bool {
 			break
 		}
 		v.reset(OpSignExt8to16)
-		v0 := b.NewValue0(v.Pos, OpTrunc16to8, fe.TypeInt8())
+		v0 := b.NewValue0(v.Pos, OpTrunc16to8, types.Int8)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -12606,8 +12610,8 @@ func rewriteValuegeneric_OpRsh32Ux32(v *Value) bool {
 func rewriteValuegeneric_OpRsh32Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32Ux64 (Const32 [c]) (Const64 [d]))
 	// cond:
 	// result: (Const32 [int64(int32(uint32(c) >> uint64(d)))])
@@ -12706,7 +12710,7 @@ func rewriteValuegeneric_OpRsh32Ux64(v *Value) bool {
 	}
 	// match: (Rsh32Ux64 (Lsh32x64 (Rsh32Ux64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
 	// cond: uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-	// result: (Rsh32Ux64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+	// result: (Rsh32Ux64 x (Const64 <types.UInt64> [c1-c2+c3]))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh32x64 {
@@ -12737,14 +12741,14 @@ func rewriteValuegeneric_OpRsh32Ux64(v *Value) bool {
 		}
 		v.reset(OpRsh32Ux64)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = c1 - c2 + c3
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh32Ux64 (Lsh32x64 x (Const64 [24])) (Const64 [24]))
 	// cond:
-	// result: (ZeroExt8to32  (Trunc32to8  <fe.TypeUInt8()>  x))
+	// result: (ZeroExt8to32  (Trunc32to8  <types.UInt8>  x))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh32x64 {
@@ -12766,14 +12770,14 @@ func rewriteValuegeneric_OpRsh32Ux64(v *Value) bool {
 			break
 		}
 		v.reset(OpZeroExt8to32)
-		v0 := b.NewValue0(v.Pos, OpTrunc32to8, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpTrunc32to8, types.UInt8)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh32Ux64 (Lsh32x64 x (Const64 [16])) (Const64 [16]))
 	// cond:
-	// result: (ZeroExt16to32 (Trunc32to16 <fe.TypeUInt16()> x))
+	// result: (ZeroExt16to32 (Trunc32to16 <types.UInt16> x))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh32x64 {
@@ -12795,7 +12799,7 @@ func rewriteValuegeneric_OpRsh32Ux64(v *Value) bool {
 			break
 		}
 		v.reset(OpZeroExt16to32)
-		v0 := b.NewValue0(v.Pos, OpTrunc32to16, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpTrunc32to16, types.UInt16)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -12919,8 +12923,8 @@ func rewriteValuegeneric_OpRsh32x32(v *Value) bool {
 func rewriteValuegeneric_OpRsh32x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh32x64  (Const32 [c]) (Const64 [d]))
 	// cond:
 	// result: (Const32 [int64(int32(c) >> uint64(d))])
@@ -13003,7 +13007,7 @@ func rewriteValuegeneric_OpRsh32x64(v *Value) bool {
 	}
 	// match: (Rsh32x64 (Lsh32x64 x (Const64 [24])) (Const64 [24]))
 	// cond:
-	// result: (SignExt8to32  (Trunc32to8  <fe.TypeInt8()>  x))
+	// result: (SignExt8to32  (Trunc32to8  <types.Int8>  x))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh32x64 {
@@ -13025,14 +13029,14 @@ func rewriteValuegeneric_OpRsh32x64(v *Value) bool {
 			break
 		}
 		v.reset(OpSignExt8to32)
-		v0 := b.NewValue0(v.Pos, OpTrunc32to8, fe.TypeInt8())
+		v0 := b.NewValue0(v.Pos, OpTrunc32to8, types.Int8)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh32x64 (Lsh32x64 x (Const64 [16])) (Const64 [16]))
 	// cond:
-	// result: (SignExt16to32 (Trunc32to16 <fe.TypeInt16()> x))
+	// result: (SignExt16to32 (Trunc32to16 <types.Int16> x))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh32x64 {
@@ -13054,7 +13058,7 @@ func rewriteValuegeneric_OpRsh32x64(v *Value) bool {
 			break
 		}
 		v.reset(OpSignExt16to32)
-		v0 := b.NewValue0(v.Pos, OpTrunc32to16, fe.TypeInt16())
+		v0 := b.NewValue0(v.Pos, OpTrunc32to16, types.Int16)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -13178,8 +13182,8 @@ func rewriteValuegeneric_OpRsh64Ux32(v *Value) bool {
 func rewriteValuegeneric_OpRsh64Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64Ux64 (Const64 [c]) (Const64 [d]))
 	// cond:
 	// result: (Const64 [int64(uint64(c) >> uint64(d))])
@@ -13278,7 +13282,7 @@ func rewriteValuegeneric_OpRsh64Ux64(v *Value) bool {
 	}
 	// match: (Rsh64Ux64 (Lsh64x64 (Rsh64Ux64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
 	// cond: uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-	// result: (Rsh64Ux64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+	// result: (Rsh64Ux64 x (Const64 <types.UInt64> [c1-c2+c3]))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh64x64 {
@@ -13309,14 +13313,14 @@ func rewriteValuegeneric_OpRsh64Ux64(v *Value) bool {
 		}
 		v.reset(OpRsh64Ux64)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = c1 - c2 + c3
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh64Ux64 (Lsh64x64 x (Const64 [56])) (Const64 [56]))
 	// cond:
-	// result: (ZeroExt8to64  (Trunc64to8  <fe.TypeUInt8()>  x))
+	// result: (ZeroExt8to64  (Trunc64to8  <types.UInt8>  x))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh64x64 {
@@ -13338,14 +13342,14 @@ func rewriteValuegeneric_OpRsh64Ux64(v *Value) bool {
 			break
 		}
 		v.reset(OpZeroExt8to64)
-		v0 := b.NewValue0(v.Pos, OpTrunc64to8, fe.TypeUInt8())
+		v0 := b.NewValue0(v.Pos, OpTrunc64to8, types.UInt8)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh64Ux64 (Lsh64x64 x (Const64 [48])) (Const64 [48]))
 	// cond:
-	// result: (ZeroExt16to64 (Trunc64to16 <fe.TypeUInt16()> x))
+	// result: (ZeroExt16to64 (Trunc64to16 <types.UInt16> x))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh64x64 {
@@ -13367,14 +13371,14 @@ func rewriteValuegeneric_OpRsh64Ux64(v *Value) bool {
 			break
 		}
 		v.reset(OpZeroExt16to64)
-		v0 := b.NewValue0(v.Pos, OpTrunc64to16, fe.TypeUInt16())
+		v0 := b.NewValue0(v.Pos, OpTrunc64to16, types.UInt16)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh64Ux64 (Lsh64x64 x (Const64 [32])) (Const64 [32]))
 	// cond:
-	// result: (ZeroExt32to64 (Trunc64to32 <fe.TypeUInt32()> x))
+	// result: (ZeroExt32to64 (Trunc64to32 <types.UInt32> x))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh64x64 {
@@ -13396,7 +13400,7 @@ func rewriteValuegeneric_OpRsh64Ux64(v *Value) bool {
 			break
 		}
 		v.reset(OpZeroExt32to64)
-		v0 := b.NewValue0(v.Pos, OpTrunc64to32, fe.TypeUInt32())
+		v0 := b.NewValue0(v.Pos, OpTrunc64to32, types.UInt32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -13520,8 +13524,8 @@ func rewriteValuegeneric_OpRsh64x32(v *Value) bool {
 func rewriteValuegeneric_OpRsh64x64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh64x64  (Const64 [c]) (Const64 [d]))
 	// cond:
 	// result: (Const64 [c >> uint64(d)])
@@ -13604,7 +13608,7 @@ func rewriteValuegeneric_OpRsh64x64(v *Value) bool {
 	}
 	// match: (Rsh64x64 (Lsh64x64 x (Const64 [56])) (Const64 [56]))
 	// cond:
-	// result: (SignExt8to64  (Trunc64to8  <fe.TypeInt8()>  x))
+	// result: (SignExt8to64  (Trunc64to8  <types.Int8>  x))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh64x64 {
@@ -13626,14 +13630,14 @@ func rewriteValuegeneric_OpRsh64x64(v *Value) bool {
 			break
 		}
 		v.reset(OpSignExt8to64)
-		v0 := b.NewValue0(v.Pos, OpTrunc64to8, fe.TypeInt8())
+		v0 := b.NewValue0(v.Pos, OpTrunc64to8, types.Int8)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh64x64 (Lsh64x64 x (Const64 [48])) (Const64 [48]))
 	// cond:
-	// result: (SignExt16to64 (Trunc64to16 <fe.TypeInt16()> x))
+	// result: (SignExt16to64 (Trunc64to16 <types.Int16> x))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh64x64 {
@@ -13655,14 +13659,14 @@ func rewriteValuegeneric_OpRsh64x64(v *Value) bool {
 			break
 		}
 		v.reset(OpSignExt16to64)
-		v0 := b.NewValue0(v.Pos, OpTrunc64to16, fe.TypeInt16())
+		v0 := b.NewValue0(v.Pos, OpTrunc64to16, types.Int16)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
 	}
 	// match: (Rsh64x64 (Lsh64x64 x (Const64 [32])) (Const64 [32]))
 	// cond:
-	// result: (SignExt32to64 (Trunc64to32 <fe.TypeInt32()> x))
+	// result: (SignExt32to64 (Trunc64to32 <types.Int32> x))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh64x64 {
@@ -13684,7 +13688,7 @@ func rewriteValuegeneric_OpRsh64x64(v *Value) bool {
 			break
 		}
 		v.reset(OpSignExt32to64)
-		v0 := b.NewValue0(v.Pos, OpTrunc64to32, fe.TypeInt32())
+		v0 := b.NewValue0(v.Pos, OpTrunc64to32, types.Int32)
 		v0.AddArg(x)
 		v.AddArg(v0)
 		return true
@@ -13808,8 +13812,8 @@ func rewriteValuegeneric_OpRsh8Ux32(v *Value) bool {
 func rewriteValuegeneric_OpRsh8Ux64(v *Value) bool {
 	b := v.Block
 	_ = b
-	fe := b.Func.fe
-	_ = fe
+	types := &b.Func.Config.Types
+	_ = types
 	// match: (Rsh8Ux64  (Const8  [c]) (Const64 [d]))
 	// cond:
 	// result: (Const8  [int64(int8(uint8(c) >> uint64(d)))])
@@ -13908,7 +13912,7 @@ func rewriteValuegeneric_OpRsh8Ux64(v *Value) bool {
 	}
 	// match: (Rsh8Ux64 (Lsh8x64 (Rsh8Ux64 x (Const64 [c1])) (Const64 [c2])) (Const64 [c3]))
 	// cond: uint64(c1) >= uint64(c2) && uint64(c3) >= uint64(c2) && !uaddOvf(c1-c2, c3)
-	// result: (Rsh8Ux64 x (Const64 <fe.TypeUInt64()> [c1-c2+c3]))
+	// result: (Rsh8Ux64 x (Const64 <types.UInt64> [c1-c2+c3]))
 	for {
 		v_0 := v.Args[0]
 		if v_0.Op != OpLsh8x64 {
@@ -13939,7 +13943,7 @@ func rewriteValuegeneric_OpRsh8Ux64(v *Value) bool {
 		}
 		v.reset(OpRsh8Ux64)
 		v.AddArg(x)
-		v0 := b.NewValue0(v.Pos, OpConst64, fe.TypeUInt64())
+		v0 := b.NewValue0(v.Pos, OpConst64, types.UInt64)
 		v0.AuxInt = c1 - c2 + c3
 		v.AddArg(v0)
 		return true
@@ -17789,6 +17793,8 @@ func rewriteBlockgeneric(b *Block) bool {
 	_ = config
 	fe := b.Func.fe
 	_ = fe
+	types := &config.Types
+	_ = types
 	switch b.Kind {
 	case BlockIf:
 		// match: (If (Not cond) yes no)
diff --git a/src/cmd/compile/internal/ssa/shortcircuit.go b/src/cmd/compile/internal/ssa/shortcircuit.go
index d5dfdefbb8..54e186980d 100644
--- a/src/cmd/compile/internal/ssa/shortcircuit.go
+++ b/src/cmd/compile/internal/ssa/shortcircuit.go
@@ -17,8 +17,8 @@ func shortcircuit(f *Func) {
 	//    x = phi(a, ...)
 	//
 	// We can replace the "a" in the phi with the constant true.
-	ct := f.ConstBool(f.Entry.Pos, f.fe.TypeBool(), true)
-	cf := f.ConstBool(f.Entry.Pos, f.fe.TypeBool(), false)
+	ct := f.ConstBool(f.Entry.Pos, f.Config.Types.Bool, true)
+	cf := f.ConstBool(f.Entry.Pos, f.Config.Types.Bool, false)
 	for _, b := range f.Blocks {
 		for _, v := range b.Values {
 			if v.Op != OpPhi {
diff --git a/src/cmd/compile/internal/ssa/writebarrier.go b/src/cmd/compile/internal/ssa/writebarrier.go
index 43349bfaf5..850a4b886d 100644
--- a/src/cmd/compile/internal/ssa/writebarrier.go
+++ b/src/cmd/compile/internal/ssa/writebarrier.go
@@ -88,17 +88,17 @@ func writebarrier(f *Func) {
 				}
 			}
 			if sb == nil {
-				sb = f.Entry.NewValue0(initpos, OpSB, f.fe.TypeUintptr())
+				sb = f.Entry.NewValue0(initpos, OpSB, f.Config.Types.Uintptr)
 			}
 			if sp == nil {
-				sp = f.Entry.NewValue0(initpos, OpSP, f.fe.TypeUintptr())
+				sp = f.Entry.NewValue0(initpos, OpSP, f.Config.Types.Uintptr)
 			}
-			wbsym := &ExternSymbol{Typ: f.fe.TypeBool(), Sym: f.fe.Syslook("writeBarrier")}
-			wbaddr = f.Entry.NewValue1A(initpos, OpAddr, f.fe.TypeUInt32().PtrTo(), wbsym, sb)
+			wbsym := &ExternSymbol{Typ: f.Config.Types.Bool, Sym: f.fe.Syslook("writeBarrier")}
+			wbaddr = f.Entry.NewValue1A(initpos, OpAddr, f.Config.Types.UInt32.PtrTo(), wbsym, sb)
 			writebarrierptr = f.fe.Syslook("writebarrierptr")
 			typedmemmove = f.fe.Syslook("typedmemmove")
 			typedmemclr = f.fe.Syslook("typedmemclr")
-			const0 = f.ConstInt32(initpos, f.fe.TypeUInt32(), 0)
+			const0 = f.ConstInt32(initpos, f.Config.Types.UInt32, 0)
 
 			// allocate auxiliary data structures for computing store order
 			sset = f.newSparseSet(f.NumValues())
@@ -155,8 +155,9 @@ func writebarrier(f *Func) {
 
 		// set up control flow for write barrier test
 		// load word, test word, avoiding partial register write from load byte.
-		flag := b.NewValue2(pos, OpLoad, f.fe.TypeUInt32(), wbaddr, mem)
-		flag = b.NewValue2(pos, OpNeq32, f.fe.TypeBool(), flag, const0)
+		types := &f.Config.Types
+		flag := b.NewValue2(pos, OpLoad, types.UInt32, wbaddr, mem)
+		flag = b.NewValue2(pos, OpNeq32, types.Bool, flag, const0)
 		b.Kind = BlockIf
 		b.SetControl(flag)
 		b.Likely = BranchUnlikely
@@ -175,7 +176,7 @@ func writebarrier(f *Func) {
 			ptr := w.Args[0]
 			var typ interface{}
 			if w.Op != OpStoreWB {
-				typ = &ExternSymbol{Typ: f.fe.TypeUintptr(), Sym: w.Aux.(Type).Symbol()}
+				typ = &ExternSymbol{Typ: types.Uintptr, Sym: w.Aux.(Type).Symbol()}
 			}
 			pos = w.Pos
 
@@ -280,7 +281,7 @@ func wbcall(pos src.XPos, b *Block, fn *obj.LSym, typ interface{}, ptr, val, mem
 	off := config.ctxt.FixedFrameSize()
 
 	if typ != nil { // for typedmemmove
-		taddr := b.NewValue1A(pos, OpAddr, b.Func.fe.TypeUintptr(), typ, sb)
+		taddr := b.NewValue1A(pos, OpAddr, b.Func.Config.Types.Uintptr, typ, sb)
 		off = round(off, taddr.Type.Alignment())
 		arg := b.NewValue1I(pos, OpOffPtr, taddr.Type.PtrTo(), off, sp)
 		mem = b.NewValue3A(pos, OpStore, TypeMem, ptr.Type, arg, taddr, mem)
