commit 397bdb216f705a3e3221c98904237820d4950042
Author: Russ Cox <rsc@golang.org>
Date:   Sat Aug 30 00:56:52 2014 -0400

    runtime: increase nosplit area to 192
    
    In CL 131450043, which raised it to 160,
    I'd raise it to 192 if necessary.
    Apparently it is necessary on windows/amd64.
    
    One note for those concerned about the growth:
    in the old segmented stack world, we wasted this much
    space at the bottom of every stack segment.
    In the new contiguous stack world, each goroutine has
    only one stack segment, so we only waste this much space
    once per goroutine. So even raising the limit further might
    still be a net savings.
    
    Fixes windows/amd64 build.
    
    TBR=r
    CC=golang-codereviews
    https://golang.org/cl/132480043
---
 src/pkg/runtime/stack.go      | 2 +-
 src/pkg/runtime/stack.h       | 4 ++--
 src/pkg/runtime/stack_test.go | 2 +-
 test/nosplit.go               | 4 ++--
 4 files changed, 6 insertions(+), 6 deletions(-)

diff --git a/src/pkg/runtime/stack.go b/src/pkg/runtime/stack.go
index ae7e96a005..ea27c1fb70 100644
--- a/src/pkg/runtime/stack.go
+++ b/src/pkg/runtime/stack.go
@@ -86,7 +86,7 @@ const (
 	// After a stack split check the SP is allowed to be this
 	// many bytes below the stack guard.  This saves an instruction
 	// in the checking sequence for tiny frames.
-	stackSmall = 96
+	stackSmall = 64
 
 	// The maximum number of bytes that a chain of NOSPLIT
 	// functions can use.
diff --git a/src/pkg/runtime/stack.h b/src/pkg/runtime/stack.h
index b2de78d898..dc9da74887 100644
--- a/src/pkg/runtime/stack.h
+++ b/src/pkg/runtime/stack.h
@@ -47,7 +47,7 @@ above checks (without allocating a full frame), which might trigger
 a call to morestack.  This sequence needs to fit in the bottom
 section of the stack.  On amd64, morestack's frame is 40 bytes, and
 deferproc's frame is 56 bytes.  That fits well within the
-StackGuard - StackSmall = 128 bytes at the bottom.  
+StackGuard - StackSmall bytes at the bottom.  
 The linkers explore all possible call traces involving non-splitting
 functions to make sure that this limit cannot be violated.
  */
@@ -94,7 +94,7 @@ enum {
 	// After a stack split check the SP is allowed to be this
 	// many bytes below the stack guard.  This saves an instruction
 	// in the checking sequence for tiny frames.
-	StackSmall = 96,
+	StackSmall = 64,
 
 	// The maximum number of bytes that a chain of NOSPLIT
 	// functions can use.
diff --git a/src/pkg/runtime/stack_test.go b/src/pkg/runtime/stack_test.go
index 956c282136..2877074f76 100644
--- a/src/pkg/runtime/stack_test.go
+++ b/src/pkg/runtime/stack_test.go
@@ -15,7 +15,7 @@ import (
 // See stack.h.
 const (
 	StackGuard = 256
-	StackSmall = 96
+	StackSmall = 64
 	StackLimit = StackGuard - StackSmall
 )
 
diff --git a/test/nosplit.go b/test/nosplit.go
index 39bb3fcb47..3854f24203 100644
--- a/test/nosplit.go
+++ b/test/nosplit.go
@@ -256,11 +256,11 @@ TestCases:
 				name := m[1]
 				size, _ := strconv.Atoi(m[2])
 
-				// CL 131450043 raised the limit from 128 to 160.
+				// The limit was originally 128 but is now 192.
 				// Instead of rewriting the test cases above, adjust
 				// the first stack frame to use up the extra 32 bytes.
 				if i == 0 {
-					size += 32
+					size += 192 - 128
 				}
 
 				if goarch == "amd64" && size%8 == 4 {
