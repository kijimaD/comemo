commit 1ce6fd03b8a72fd8346fb23a975124edf977d25e
Author: Daniel Martí <mvdan@mvdan.cc>
Date:   Sat Jan 16 18:03:31 2021 +0000

    cmd/gofmt: format files in parallel
    
    gofmt is pretty heavily CPU-bound, since parsing and formatting 1MiB
    of Go code takes much longer than reading that amount of bytes from
    disk. However, parsing and manipulating a large Go source file is very
    difficult to parallelize, so we continue to process each file in its
    own goroutine.
    
    A Go module may contain a large number of Go source files, so we need
    to bound the amount of work in flight. However, because the
    distribution of sizes for Go source files varies widely — from tiny
    doc.go files containing a single package comment all the way up to
    massive API wrappers generated by automated tools — the amount of
    time, work, and memory overhead needed to process each file also
    varies. To account for this variability, we limit the in-flight work
    by bytes of input rather than by number of files. That allows us to
    make progress on many small files while we wait for work on a handful
    of large files to complete.
    
    The gofmt tool has a well-defined output format on stdout, which was
    previously deterministic. We keep it deterministic by printing the
    results of each file in order, using a lazily-synchronized io.Writer
    (loosly inspired by Haskell's IO monad). After a file has been
    formatted in memory, we keep it in memory (again, limited by the
    corresponding number of input bytes) until the output for all previous
    files has been flushed. This adds a bit of latency compared to
    emitting the output in nondeterministic order, but a little extra
    latency seems worth the cost to preserve output stability.
    
    This change is based on Daniel Martí's work in CL 284139, but using a
    weighted semaphore and ephemeral goroutines instead of a worker pool
    and batches. Benchmark results are similar, and I find the concurrency
    in this approach a bit easier to reason about.
    
    In the batching-based approach, the batch size allows us to "look
    ahead" to find large files and start processing them early. To keep
    the CPUs saturated and prevent stragglers, we would need to tune the
    batch size to be about the same as the largest input files. If the
    batch size is set too high, a large batch of small files could turn
    into a straggler, but if the batch size is set too low, the largest
    files in the data set won't be started early enough and we'll end up
    with a large-file straggler.
    
    One possible alternative would be to sort by file size instead of
    batching: identify all of the files to be processed, sort from largest
    to smallest, and then process the largest files first so that the
    "tail" of processing covers the smallest files. However, that approach
    would still fail to saturate available CPU when disk latency is high,
    would require buffering an arbitrary amount of metadata in order to
    sort by size, and (perhaps most importantly!) would not allow the
    `gofmt` binary to preserve the same (deterministic) output order that
    it has today.
    
    In contrast, with a semaphore we can produce the same deterministic
    output as ever using only one tuning parameter: the memory footprint,
    expressed as a rough lower bound on the amount of RAM available per
    thread. While we're below the memory limit, we can run arbitrarily
    many disk operations arbitrarily far ahead, and process the results of
    those operations whenever they become avaliable. Then it's up to the
    kernel (not us) to schedule the disk operations for throughput and
    latency, and it's up to the runtime (not us) to schedule the
    goroutines so that they complete quickly.
    
    In practice, even a modest assumption of a few megabytes per thread
    seems to provide a nice speedup, and it should scale reasonably even
    to machines with vastly different ratios of CPU to disk. (In practice,
    I expect that most 'gofmt' invocations will work with files on at most
    one physical disk, so the CPU:disk ratio should vary more-or-less
    directly with the thread count, whereas the CPU:memory ratio is
    more-or-less independent of thread count.)
    
    name \ time/op         baseline.txt  284139.txt    simplified.txt
    GofmtGorootCmd           11.9s ± 2%     2.7s ± 3%       2.8s ± 5%
    
    name \ user-time/op    baseline.txt  284139.txt    simplified.txt
    GofmtGorootCmd           13.5s ± 2%    14.4s ± 1%      14.7s ± 1%
    
    name \ sys-time/op     baseline.txt  284139.txt    simplified.txt
    GofmtGorootCmd           465ms ± 8%    229ms ±28%      232ms ±31%
    
    name \ peak-RSS-bytes  baseline.txt  284139.txt    simplified.txt
    GofmtGorootCmd          77.7MB ± 4%  162.2MB ±10%    192.9MB ±15%
    
    For #43566
    
    Change-Id: I4ba251eb4d188a3bd1901039086be57f0b341910
    Reviewed-on: https://go-review.googlesource.com/c/go/+/317975
    Trust: Bryan C. Mills <bcmills@google.com>
    Trust: Daniel Martí <mvdan@mvdan.cc>
    Run-TryBot: Bryan C. Mills <bcmills@google.com>
    TryBot-Result: Go Bot <gobot@golang.org>
    Reviewed-by: Daniel Martí <mvdan@mvdan.cc>

 doc/go1.18.html                                    |  11 +-
 src/cmd/go.mod                                     |   1 +
 src/cmd/go.sum                                     |   2 +
 src/cmd/gofmt/gofmt.go                             | 306 +++++++++++++++++----
 src/cmd/gofmt/gofmt_test.go                        |  24 +-
 src/cmd/gofmt/rewrite.go                           |   6 +-
 src/cmd/vendor/golang.org/x/sync/AUTHORS           |   3 +
 src/cmd/vendor/golang.org/x/sync/CONTRIBUTORS      |   3 +
 src/cmd/vendor/golang.org/x/sync/LICENSE           |  27 ++
 src/cmd/vendor/golang.org/x/sync/PATENTS           |  22 ++
 .../golang.org/x/sync/semaphore/semaphore.go       | 136 +++++++++
 src/cmd/vendor/modules.txt                         |   3 +
 12 files changed, 481 insertions(+), 63 deletions(-)
