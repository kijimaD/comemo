commit 259b7edf5c3a9b898d9b250c44d4f2c94bc90ca5
Author: Keith Randall <khr@golang.org>
Date:   Mon Mar 21 16:18:45 2016 -0700

    cmd/compile: allow naming of subexpressions
    
    Allow names to be used for subexpressions of match rules.
    For example:
    
    (OpA x:(OpB y)) -> ..use x here to refer to the OpB value..
    
    This gets rid of the .Args[0].Args[0]... way of naming we
    used to use.
    
    While we're here, give all subexpression matches names instead
    of recomputing them with .Args[i] sequences each time they
    are referenced.  Makes the generated rule code a bit smaller.
    
    Change-Id: Ie42139f6f208933b75bd2ae8bd34e95419bc0e4e
    Reviewed-on: https://go-review.googlesource.com/20997
    Run-TryBot: Todd Neal <todd@tneal.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Todd Neal <todd@tneal.org>
---
 src/cmd/compile/internal/ssa/gen/AMD64.rules   |   24 +-
 src/cmd/compile/internal/ssa/gen/generic.rules |    8 +-
 src/cmd/compile/internal/ssa/gen/rulegen.go    |   66 +-
 src/cmd/compile/internal/ssa/rewriteAMD64.go   | 2954 ++++++++++++++----------
 src/cmd/compile/internal/ssa/rewritedec.go     |   25 +-
 src/cmd/compile/internal/ssa/rewritegeneric.go | 2935 +++++++++++++----------
 6 files changed, 3521 insertions(+), 2491 deletions(-)

diff --git a/src/cmd/compile/internal/ssa/gen/AMD64.rules b/src/cmd/compile/internal/ssa/gen/AMD64.rules
index eba7c34d68..1aa6821460 100644
--- a/src/cmd/compile/internal/ssa/gen/AMD64.rules
+++ b/src/cmd/compile/internal/ssa/gen/AMD64.rules
@@ -618,12 +618,12 @@
 // Make sure we don't combine these ops if the load has another use.
 // This prevents a single load from being split into multiple loads
 // which then might return different values.  See test/atomicload.go.
-(MOVBQSX (MOVBload [off] {sym} ptr mem)) && v.Args[0].Uses == 1 -> @v.Args[0].Block (MOVBQSXload <v.Type> [off] {sym} ptr mem)
-(MOVBQZX (MOVBload [off] {sym} ptr mem)) && v.Args[0].Uses == 1 -> @v.Args[0].Block (MOVBQZXload <v.Type> [off] {sym} ptr mem)
-(MOVWQSX (MOVWload [off] {sym} ptr mem)) && v.Args[0].Uses == 1 -> @v.Args[0].Block (MOVWQSXload <v.Type> [off] {sym} ptr mem)
-(MOVWQZX (MOVWload [off] {sym} ptr mem)) && v.Args[0].Uses == 1 -> @v.Args[0].Block (MOVWQZXload <v.Type> [off] {sym} ptr mem)
-(MOVLQSX (MOVLload [off] {sym} ptr mem)) && v.Args[0].Uses == 1 -> @v.Args[0].Block (MOVLQSXload <v.Type> [off] {sym} ptr mem)
-(MOVLQZX (MOVLload [off] {sym} ptr mem)) && v.Args[0].Uses == 1 -> @v.Args[0].Block (MOVLQZXload <v.Type> [off] {sym} ptr mem)
+(MOVBQSX x:(MOVBload [off] {sym} ptr mem)) && x.Uses == 1 -> @x.Block (MOVBQSXload <v.Type> [off] {sym} ptr mem)
+(MOVBQZX x:(MOVBload [off] {sym} ptr mem)) && x.Uses == 1 -> @x.Block (MOVBQZXload <v.Type> [off] {sym} ptr mem)
+(MOVWQSX x:(MOVWload [off] {sym} ptr mem)) && x.Uses == 1 -> @x.Block (MOVWQSXload <v.Type> [off] {sym} ptr mem)
+(MOVWQZX x:(MOVWload [off] {sym} ptr mem)) && x.Uses == 1 -> @x.Block (MOVWQZXload <v.Type> [off] {sym} ptr mem)
+(MOVLQSX x:(MOVLload [off] {sym} ptr mem)) && x.Uses == 1 -> @x.Block (MOVLQSXload <v.Type> [off] {sym} ptr mem)
+(MOVLQZX x:(MOVLload [off] {sym} ptr mem)) && x.Uses == 1 -> @x.Block (MOVLQZXload <v.Type> [off] {sym} ptr mem)
 
 // replace load from same location as preceding store with copy
 (MOVBload [off] {sym} ptr (MOVBstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) -> x
@@ -1197,21 +1197,21 @@
 // Combining byte loads into larger (unaligned) loads.
 // There are many ways these combinations could occur.  This is
 // designed to match the way encoding/binary.LittleEndian does it.
-(ORW                (MOVBQZXload [i]   {s} p mem)
-    (SHLWconst [8]  (MOVBQZXload [i+1] {s} p mem))) -> @v.Args[0].Block (MOVWload [i] {s} p mem)
+(ORW              x:(MOVBQZXload [i]   {s} p mem)
+    (SHLWconst [8]  (MOVBQZXload [i+1] {s} p mem))) -> @x.Block (MOVWload [i] {s} p mem)
 
 (ORL (ORL (ORL
-                    (MOVBQZXload [i]   {s} p mem)
+                  x:(MOVBQZXload [i]   {s} p mem)
     (SHLLconst [8]  (MOVBQZXload [i+1] {s} p mem)))
     (SHLLconst [16] (MOVBQZXload [i+2] {s} p mem)))
-    (SHLLconst [24] (MOVBQZXload [i+3] {s} p mem))) -> @v.Args[0].Args[0].Args[0].Block (MOVLload [i] {s} p mem)
+    (SHLLconst [24] (MOVBQZXload [i+3] {s} p mem))) -> @x.Block (MOVLload [i] {s} p mem)
 
 (ORQ (ORQ (ORQ (ORQ (ORQ (ORQ (ORQ
-                    (MOVBQZXload [i]   {s} p mem)
+                  x:(MOVBQZXload [i]   {s} p mem)
     (SHLQconst [8]  (MOVBQZXload [i+1] {s} p mem)))
     (SHLQconst [16] (MOVBQZXload [i+2] {s} p mem)))
     (SHLQconst [24] (MOVBQZXload [i+3] {s} p mem)))
     (SHLQconst [32] (MOVBQZXload [i+4] {s} p mem)))
     (SHLQconst [40] (MOVBQZXload [i+5] {s} p mem)))
     (SHLQconst [48] (MOVBQZXload [i+6] {s} p mem)))
-    (SHLQconst [56] (MOVBQZXload [i+7] {s} p mem))) -> @v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Block (MOVQload [i] {s} p mem)
+    (SHLQconst [56] (MOVBQZXload [i+7] {s} p mem))) -> @x.Block (MOVQload [i] {s} p mem)
diff --git a/src/cmd/compile/internal/ssa/gen/generic.rules b/src/cmd/compile/internal/ssa/gen/generic.rules
index 99fc75df6a..059914d979 100644
--- a/src/cmd/compile/internal/ssa/gen/generic.rules
+++ b/src/cmd/compile/internal/ssa/gen/generic.rules
@@ -21,7 +21,7 @@
 
 // Simplify nil checks.
 // These are inserted by for _, e := range a {}
-(NilCheck (Phi x (Add64 (Const64 [c]) y)) mem) && c > 0 && v.Args[0] == y -> (NilCheck x mem)
+(NilCheck z:(Phi x (Add64 (Const64 [c]) y)) mem) && c > 0 && z == y -> (NilCheck x mem)
 
 // constant folding
 (Trunc16to8 (Const16 [c])) -> (Const8 [int64(int8(c))])
@@ -456,7 +456,7 @@
 
 // indexing operations
 // Note: bounds check has already been done
-(ArrayIndex <t> [0] (Load ptr mem)) -> @v.Args[0].Block (Load <t> ptr mem)
+(ArrayIndex <t> [0] x:(Load ptr mem)) -> @x.Block (Load <t> ptr mem)
 (PtrIndex <t> ptr idx) && config.PtrSize == 4 -> (AddPtr ptr (Mul32 <config.fe.TypeInt()> idx (Const32 <config.fe.TypeInt()> [t.ElemType().Size()])))
 (PtrIndex <t> ptr idx) && config.PtrSize == 8 -> (AddPtr ptr (Mul64 <config.fe.TypeInt()> idx (Const64 <config.fe.TypeInt()> [t.ElemType().Size()])))
 
@@ -493,8 +493,8 @@
     (Load <t.FieldType(2)> (OffPtr <t.FieldType(2).PtrTo()> [t.FieldOff(2)] ptr) mem)
     (Load <t.FieldType(3)> (OffPtr <t.FieldType(3).PtrTo()> [t.FieldOff(3)] ptr) mem))
 
-(StructSelect [i] (Load <t> ptr mem)) && !config.fe.CanSSA(t) ->
-  @v.Args[0].Block (Load <v.Type> (OffPtr <v.Type.PtrTo()> [t.FieldOff(int(i))] ptr) mem)
+(StructSelect [i] x:(Load <t> ptr mem)) && !config.fe.CanSSA(t) ->
+  @x.Block (Load <v.Type> (OffPtr <v.Type.PtrTo()> [t.FieldOff(int(i))] ptr) mem)
 
 (Store _ (StructMake0) mem) -> mem
 (Store dst (StructMake1 <t> f0) mem) ->
diff --git a/src/cmd/compile/internal/ssa/gen/rulegen.go b/src/cmd/compile/internal/ssa/gen/rulegen.go
index 68e2dbf1a5..d01fb03745 100644
--- a/src/cmd/compile/internal/ssa/gen/rulegen.go
+++ b/src/cmd/compile/internal/ssa/gen/rulegen.go
@@ -214,7 +214,11 @@ func genRules(arch arch) {
 			// check match of control value
 			if s[1] != "nil" {
 				fmt.Fprintf(w, "v := b.Control\n")
-				genMatch0(w, arch, s[1], "v", map[string]string{}, false)
+				if strings.Contains(s[1], "(") {
+					genMatch0(w, arch, s[1], "v", map[string]struct{}{}, false)
+				} else {
+					fmt.Fprintf(w, "%s := b.Control\n", s[1])
+				}
 			}
 
 			// assign successor names
@@ -310,26 +314,12 @@ func genRules(arch arch) {
 }
 
 func genMatch(w io.Writer, arch arch, match string) {
-	genMatch0(w, arch, match, "v", map[string]string{}, true)
+	genMatch0(w, arch, match, "v", map[string]struct{}{}, true)
 }
 
-func genMatch0(w io.Writer, arch arch, match, v string, m map[string]string, top bool) {
-	if match[0] != '(' {
-		if _, ok := m[match]; ok {
-			// variable already has a definition. Check whether
-			// the old definition and the new definition match.
-			// For example, (add x x).  Equality is just pointer equality
-			// on Values (so cse is important to do before lowering).
-			fmt.Fprintf(w, "if %s != %s {\nbreak\n}\n", v, match)
-			return
-		}
-		// remember that this variable references the given value
-		if match == "_" {
-			return
-		}
-		m[match] = v
-		fmt.Fprintf(w, "%s := %s\n", match, v)
-		return
+func genMatch0(w io.Writer, arch arch, match, v string, m map[string]struct{}, top bool) {
+	if match[0] != '(' || match[len(match)-1] != ')' {
+		panic("non-compound expr in genMatch0: " + match)
 	}
 
 	// split body up into regions. Split by spaces/tabs, except those
@@ -356,7 +346,7 @@ func genMatch0(w io.Writer, arch arch, match, v string, m map[string]string, top
 					// must match previous variable
 					fmt.Fprintf(w, "if %s.Type != %s {\nbreak\n}\n", v, t)
 				} else {
-					m[t] = v + ".Type"
+					m[t] = struct{}{}
 					fmt.Fprintf(w, "%s := %s.Type\n", t, v)
 				}
 			}
@@ -371,7 +361,7 @@ func genMatch0(w io.Writer, arch arch, match, v string, m map[string]string, top
 				if _, ok := m[x]; ok {
 					fmt.Fprintf(w, "if %s.AuxInt != %s {\nbreak\n}\n", v, x)
 				} else {
-					m[x] = v + ".AuxInt"
+					m[x] = struct{}{}
 					fmt.Fprintf(w, "%s := %s.AuxInt\n", x, v)
 				}
 			}
@@ -386,13 +376,41 @@ func genMatch0(w io.Writer, arch arch, match, v string, m map[string]string, top
 				if _, ok := m[x]; ok {
 					fmt.Fprintf(w, "if %s.Aux != %s {\nbreak\n}\n", v, x)
 				} else {
-					m[x] = v + ".Aux"
+					m[x] = struct{}{}
 					fmt.Fprintf(w, "%s := %s.Aux\n", x, v)
 				}
 			}
+		} else if a == "_" {
+			argnum++
+		} else if !strings.Contains(a, "(") {
+			// leaf variable
+			if _, ok := m[a]; ok {
+				// variable already has a definition. Check whether
+				// the old definition and the new definition match.
+				// For example, (add x x).  Equality is just pointer equality
+				// on Values (so cse is important to do before lowering).
+				fmt.Fprintf(w, "if %s != %s.Args[%d] {\nbreak\n}\n", a, v, argnum)
+			} else {
+				// remember that this variable references the given value
+				m[a] = struct{}{}
+				fmt.Fprintf(w, "%s := %s.Args[%d]\n", a, v, argnum)
+			}
+			argnum++
 		} else {
-			// variable or sexpr
-			genMatch0(w, arch, a, fmt.Sprintf("%s.Args[%d]", v, argnum), m, false)
+			// compound sexpr
+			var argname string
+			colon := strings.Index(a, ":")
+			openparen := strings.Index(a, "(")
+			if colon >= 0 && openparen >= 0 && colon < openparen {
+				// rule-specified name
+				argname = a[:colon]
+				a = a[colon+1:]
+			} else {
+				// autogenerated name
+				argname = fmt.Sprintf("%s_%d", v, argnum)
+			}
+			fmt.Fprintf(w, "%s := %s.Args[%d]\n", argname, v, argnum)
+			genMatch0(w, arch, a, argname, m, false)
 			argnum++
 		}
 	}
diff --git a/src/cmd/compile/internal/ssa/rewriteAMD64.go b/src/cmd/compile/internal/ssa/rewriteAMD64.go
index e5a959d82c..9de795e2d4 100644
--- a/src/cmd/compile/internal/ssa/rewriteAMD64.go
+++ b/src/cmd/compile/internal/ssa/rewriteAMD64.go
@@ -769,10 +769,11 @@ func rewriteValueAMD64_OpAMD64ADDB(v *Value, config *Config) bool {
 	// result: (ADDBconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64ADDBconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -782,10 +783,11 @@ func rewriteValueAMD64_OpAMD64ADDB(v *Value, config *Config) bool {
 	// cond:
 	// result: (ADDBconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64ADDBconst)
 		v.AuxInt = c
@@ -797,10 +799,11 @@ func rewriteValueAMD64_OpAMD64ADDB(v *Value, config *Config) bool {
 	// result: (SUBB x y)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64NEGB {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64NEGB {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64SUBB)
 		v.AddArg(x)
 		v.AddArg(y)
@@ -830,10 +833,11 @@ func rewriteValueAMD64_OpAMD64ADDBconst(v *Value, config *Config) bool {
 	// result: (MOVBconst [c+d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVBconst)
 		v.AuxInt = c + d
 		return true
@@ -843,11 +847,12 @@ func rewriteValueAMD64_OpAMD64ADDBconst(v *Value, config *Config) bool {
 	// result: (ADDBconst [c+d] x)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64ADDBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDBconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		v.reset(OpAMD64ADDBconst)
 		v.AuxInt = c + d
 		v.AddArg(x)
@@ -863,10 +868,11 @@ func rewriteValueAMD64_OpAMD64ADDL(v *Value, config *Config) bool {
 	// result: (ADDLconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64ADDLconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -876,10 +882,11 @@ func rewriteValueAMD64_OpAMD64ADDL(v *Value, config *Config) bool {
 	// cond:
 	// result: (ADDLconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64ADDLconst)
 		v.AuxInt = c
@@ -891,10 +898,11 @@ func rewriteValueAMD64_OpAMD64ADDL(v *Value, config *Config) bool {
 	// result: (SUBL x y)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64NEGL {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64NEGL {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64SUBL)
 		v.AddArg(x)
 		v.AddArg(y)
@@ -924,10 +932,11 @@ func rewriteValueAMD64_OpAMD64ADDLconst(v *Value, config *Config) bool {
 	// result: (MOVLconst [c+d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVLconst)
 		v.AuxInt = c + d
 		return true
@@ -937,11 +946,12 @@ func rewriteValueAMD64_OpAMD64ADDLconst(v *Value, config *Config) bool {
 	// result: (ADDLconst [c+d] x)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64ADDLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDLconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		v.reset(OpAMD64ADDLconst)
 		v.AuxInt = c + d
 		v.AddArg(x)
@@ -957,10 +967,11 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// result: (ADDQconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(is32Bit(c)) {
 			break
 		}
@@ -973,10 +984,11 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// cond: is32Bit(c)
 	// result: (ADDQconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		if !(is32Bit(c)) {
 			break
@@ -991,13 +1003,14 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// result: (LEAQ8 x y)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64SHLQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[1].AuxInt != 3 {
+		if v_1.AuxInt != 3 {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64LEAQ8)
 		v.AddArg(x)
 		v.AddArg(y)
@@ -1008,13 +1021,14 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// result: (LEAQ4 x y)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64SHLQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[1].AuxInt != 2 {
+		if v_1.AuxInt != 2 {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64LEAQ4)
 		v.AddArg(x)
 		v.AddArg(y)
@@ -1025,13 +1039,14 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// result: (LEAQ2 x y)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64SHLQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[1].AuxInt != 1 {
+		if v_1.AuxInt != 1 {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64LEAQ2)
 		v.AddArg(x)
 		v.AddArg(y)
@@ -1042,11 +1057,12 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// result: (LEAQ2 x y)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQ {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQ {
 			break
 		}
-		y := v.Args[1].Args[0]
-		if v.Args[1].Args[1] != y {
+		y := v_1.Args[0]
+		if y != v_1.Args[1] {
 			break
 		}
 		v.reset(OpAMD64LEAQ2)
@@ -1059,13 +1075,14 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// result: (LEAQ2 y x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQ {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQ {
 			break
 		}
-		if v.Args[1].Args[0] != x {
+		if x != v_1.Args[0] {
 			break
 		}
-		y := v.Args[1].Args[1]
+		y := v_1.Args[1]
 		v.reset(OpAMD64LEAQ2)
 		v.AddArg(y)
 		v.AddArg(x)
@@ -1076,11 +1093,12 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// result: (LEAQ2 y x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQ {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQ {
 			break
 		}
-		y := v.Args[1].Args[0]
-		if v.Args[1].Args[1] != x {
+		y := v_1.Args[0]
+		if x != v_1.Args[1] {
 			break
 		}
 		v.reset(OpAMD64LEAQ2)
@@ -1092,11 +1110,12 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// cond:
 	// result: (LEAQ1 [c] x y)
 	for {
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		x := v_0.Args[0]
 		y := v.Args[1]
 		v.reset(OpAMD64LEAQ1)
 		v.AuxInt = c
@@ -1109,11 +1128,12 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// result: (LEAQ1 [c] x y)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
-		y := v.Args[1].Args[0]
+		c := v_1.AuxInt
+		y := v_1.Args[0]
 		v.reset(OpAMD64LEAQ1)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -1125,12 +1145,13 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// result: (LEAQ1 [c] {s} x y)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64LEAQ {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64LEAQ {
 			break
 		}
-		c := v.Args[1].AuxInt
-		s := v.Args[1].Aux
-		y := v.Args[1].Args[0]
+		c := v_1.AuxInt
+		s := v_1.Aux
+		y := v_1.Args[0]
 		if !(x.Op != OpSB && y.Op != OpSB) {
 			break
 		}
@@ -1145,12 +1166,13 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// cond: x.Op != OpSB && y.Op != OpSB
 	// result: (LEAQ1 [c] {s} x y)
 	for {
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		c := v.Args[0].AuxInt
-		s := v.Args[0].Aux
-		x := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		s := v_0.Aux
+		x := v_0.Args[0]
 		y := v.Args[1]
 		if !(x.Op != OpSB && y.Op != OpSB) {
 			break
@@ -1167,10 +1189,11 @@ func rewriteValueAMD64_OpAMD64ADDQ(v *Value, config *Config) bool {
 	// result: (SUBQ x y)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64NEGQ {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64NEGQ {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64SUBQ)
 		v.AddArg(x)
 		v.AddArg(y)
@@ -1186,11 +1209,12 @@ func rewriteValueAMD64_OpAMD64ADDQconst(v *Value, config *Config) bool {
 	// result: (LEAQ1 [c] x y)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64ADDQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQ {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		v.reset(OpAMD64LEAQ1)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -1202,12 +1226,13 @@ func rewriteValueAMD64_OpAMD64ADDQconst(v *Value, config *Config) bool {
 	// result: (LEAQ [c+d] {s} x)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		d := v.Args[0].AuxInt
-		s := v.Args[0].Aux
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		s := v_0.Aux
+		x := v_0.Args[0]
 		if !(is32Bit(c + d)) {
 			break
 		}
@@ -1222,13 +1247,14 @@ func rewriteValueAMD64_OpAMD64ADDQconst(v *Value, config *Config) bool {
 	// result: (LEAQ1 [c+d] {s} x y)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64LEAQ1 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ1 {
 			break
 		}
-		d := v.Args[0].AuxInt
-		s := v.Args[0].Aux
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		d := v_0.AuxInt
+		s := v_0.Aux
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if !(is32Bit(c + d)) {
 			break
 		}
@@ -1244,13 +1270,14 @@ func rewriteValueAMD64_OpAMD64ADDQconst(v *Value, config *Config) bool {
 	// result: (LEAQ2 [c+d] {s} x y)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64LEAQ2 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ2 {
 			break
 		}
-		d := v.Args[0].AuxInt
-		s := v.Args[0].Aux
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		d := v_0.AuxInt
+		s := v_0.Aux
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if !(is32Bit(c + d)) {
 			break
 		}
@@ -1266,13 +1293,14 @@ func rewriteValueAMD64_OpAMD64ADDQconst(v *Value, config *Config) bool {
 	// result: (LEAQ4 [c+d] {s} x y)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64LEAQ4 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ4 {
 			break
 		}
-		d := v.Args[0].AuxInt
-		s := v.Args[0].Aux
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		d := v_0.AuxInt
+		s := v_0.Aux
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if !(is32Bit(c + d)) {
 			break
 		}
@@ -1288,13 +1316,14 @@ func rewriteValueAMD64_OpAMD64ADDQconst(v *Value, config *Config) bool {
 	// result: (LEAQ8 [c+d] {s} x y)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64LEAQ8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ8 {
 			break
 		}
-		d := v.Args[0].AuxInt
-		s := v.Args[0].Aux
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		d := v_0.AuxInt
+		s := v_0.Aux
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if !(is32Bit(c + d)) {
 			break
 		}
@@ -1323,10 +1352,11 @@ func rewriteValueAMD64_OpAMD64ADDQconst(v *Value, config *Config) bool {
 	// result: (MOVQconst [c+d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVQconst)
 		v.AuxInt = c + d
 		return true
@@ -1336,11 +1366,12 @@ func rewriteValueAMD64_OpAMD64ADDQconst(v *Value, config *Config) bool {
 	// result: (ADDQconst [c+d] x)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		if !(is32Bit(c + d)) {
 			break
 		}
@@ -1359,10 +1390,11 @@ func rewriteValueAMD64_OpAMD64ADDW(v *Value, config *Config) bool {
 	// result: (ADDWconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64ADDWconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -1372,10 +1404,11 @@ func rewriteValueAMD64_OpAMD64ADDW(v *Value, config *Config) bool {
 	// cond:
 	// result: (ADDWconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64ADDWconst)
 		v.AuxInt = c
@@ -1387,10 +1420,11 @@ func rewriteValueAMD64_OpAMD64ADDW(v *Value, config *Config) bool {
 	// result: (SUBW x y)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64NEGW {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64NEGW {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64SUBW)
 		v.AddArg(x)
 		v.AddArg(y)
@@ -1420,10 +1454,11 @@ func rewriteValueAMD64_OpAMD64ADDWconst(v *Value, config *Config) bool {
 	// result: (MOVWconst [c+d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVWconst)
 		v.AuxInt = c + d
 		return true
@@ -1433,11 +1468,12 @@ func rewriteValueAMD64_OpAMD64ADDWconst(v *Value, config *Config) bool {
 	// result: (ADDWconst [c+d] x)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64ADDWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDWconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		v.reset(OpAMD64ADDWconst)
 		v.AuxInt = c + d
 		v.AddArg(x)
@@ -1453,10 +1489,11 @@ func rewriteValueAMD64_OpAMD64ANDB(v *Value, config *Config) bool {
 	// result: (ANDBconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64ANDBconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -1466,10 +1503,11 @@ func rewriteValueAMD64_OpAMD64ANDB(v *Value, config *Config) bool {
 	// cond:
 	// result: (ANDBconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64ANDBconst)
 		v.AuxInt = c
@@ -1481,10 +1519,11 @@ func rewriteValueAMD64_OpAMD64ANDB(v *Value, config *Config) bool {
 	// result: (ANDBconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64ANDBconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -1494,10 +1533,11 @@ func rewriteValueAMD64_OpAMD64ANDB(v *Value, config *Config) bool {
 	// cond:
 	// result: (ANDBconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64ANDBconst)
 		v.AuxInt = c
@@ -1509,7 +1549,7 @@ func rewriteValueAMD64_OpAMD64ANDB(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -1553,10 +1593,11 @@ func rewriteValueAMD64_OpAMD64ANDBconst(v *Value, config *Config) bool {
 	// result: (MOVBconst [c&d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVBconst)
 		v.AuxInt = c & d
 		return true
@@ -1571,10 +1612,11 @@ func rewriteValueAMD64_OpAMD64ANDL(v *Value, config *Config) bool {
 	// result: (ANDLconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64ANDLconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -1584,10 +1626,11 @@ func rewriteValueAMD64_OpAMD64ANDL(v *Value, config *Config) bool {
 	// cond:
 	// result: (ANDLconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64ANDLconst)
 		v.AuxInt = c
@@ -1599,7 +1642,7 @@ func rewriteValueAMD64_OpAMD64ANDL(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -1643,10 +1686,11 @@ func rewriteValueAMD64_OpAMD64ANDLconst(v *Value, config *Config) bool {
 	// result: (MOVLconst [c&d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVLconst)
 		v.AuxInt = c & d
 		return true
@@ -1661,10 +1705,11 @@ func rewriteValueAMD64_OpAMD64ANDQ(v *Value, config *Config) bool {
 	// result: (ANDQconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(is32Bit(c)) {
 			break
 		}
@@ -1677,10 +1722,11 @@ func rewriteValueAMD64_OpAMD64ANDQ(v *Value, config *Config) bool {
 	// cond: is32Bit(c)
 	// result: (ANDQconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		if !(is32Bit(c)) {
 			break
@@ -1695,7 +1741,7 @@ func rewriteValueAMD64_OpAMD64ANDQ(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -1737,10 +1783,11 @@ func rewriteValueAMD64_OpAMD64ANDQconst(v *Value, config *Config) bool {
 	// result: (MOVQconst [c&d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVQconst)
 		v.AuxInt = c & d
 		return true
@@ -1755,10 +1802,11 @@ func rewriteValueAMD64_OpAMD64ANDW(v *Value, config *Config) bool {
 	// result: (ANDWconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64ANDWconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -1768,10 +1816,11 @@ func rewriteValueAMD64_OpAMD64ANDW(v *Value, config *Config) bool {
 	// cond:
 	// result: (ANDWconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64ANDWconst)
 		v.AuxInt = c
@@ -1783,10 +1832,11 @@ func rewriteValueAMD64_OpAMD64ANDW(v *Value, config *Config) bool {
 	// result: (ANDWconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64ANDWconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -1796,10 +1846,11 @@ func rewriteValueAMD64_OpAMD64ANDW(v *Value, config *Config) bool {
 	// cond:
 	// result: (ANDWconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64ANDWconst)
 		v.AuxInt = c
@@ -1811,7 +1862,7 @@ func rewriteValueAMD64_OpAMD64ANDW(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -1855,10 +1906,11 @@ func rewriteValueAMD64_OpAMD64ANDWconst(v *Value, config *Config) bool {
 	// result: (MOVWconst [c&d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVWconst)
 		v.AuxInt = c & d
 		return true
@@ -2081,10 +2133,11 @@ func rewriteValueAMD64_OpAMD64CMPB(v *Value, config *Config) bool {
 	// result: (CMPBconst x [c])
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64CMPBconst)
 		v.AddArg(x)
 		v.AuxInt = c
@@ -2094,10 +2147,11 @@ func rewriteValueAMD64_OpAMD64CMPB(v *Value, config *Config) bool {
 	// cond:
 	// result: (InvertFlags (CMPBconst x [c]))
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64InvertFlags)
 		v0 := b.NewValue0(v.Line, OpAMD64CMPBconst, TypeFlags)
@@ -2115,10 +2169,11 @@ func rewriteValueAMD64_OpAMD64CMPBconst(v *Value, config *Config) bool {
 	// cond: int8(x)==int8(y)
 	// result: (FlagEQ)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int8(x) == int8(y)) {
 			break
@@ -2130,10 +2185,11 @@ func rewriteValueAMD64_OpAMD64CMPBconst(v *Value, config *Config) bool {
 	// cond: int8(x)<int8(y) && uint8(x)<uint8(y)
 	// result: (FlagLT_ULT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int8(x) < int8(y) && uint8(x) < uint8(y)) {
 			break
@@ -2145,10 +2201,11 @@ func rewriteValueAMD64_OpAMD64CMPBconst(v *Value, config *Config) bool {
 	// cond: int8(x)<int8(y) && uint8(x)>uint8(y)
 	// result: (FlagLT_UGT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int8(x) < int8(y) && uint8(x) > uint8(y)) {
 			break
@@ -2160,10 +2217,11 @@ func rewriteValueAMD64_OpAMD64CMPBconst(v *Value, config *Config) bool {
 	// cond: int8(x)>int8(y) && uint8(x)<uint8(y)
 	// result: (FlagGT_ULT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int8(x) > int8(y) && uint8(x) < uint8(y)) {
 			break
@@ -2175,10 +2233,11 @@ func rewriteValueAMD64_OpAMD64CMPBconst(v *Value, config *Config) bool {
 	// cond: int8(x)>int8(y) && uint8(x)>uint8(y)
 	// result: (FlagGT_UGT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int8(x) > int8(y) && uint8(x) > uint8(y)) {
 			break
@@ -2190,10 +2249,11 @@ func rewriteValueAMD64_OpAMD64CMPBconst(v *Value, config *Config) bool {
 	// cond: int8(m)+1==int8(n) && isPowerOfTwo(int64(int8(n)))
 	// result: (FlagLT_ULT)
 	for {
-		if v.Args[0].Op != OpAMD64ANDBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDBconst {
 			break
 		}
-		m := v.Args[0].AuxInt
+		m := v_0.AuxInt
 		n := v.AuxInt
 		if !(int8(m)+1 == int8(n) && isPowerOfTwo(int64(int8(n)))) {
 			break
@@ -2205,11 +2265,12 @@ func rewriteValueAMD64_OpAMD64CMPBconst(v *Value, config *Config) bool {
 	// cond:
 	// result: (TESTB x y)
 	for {
-		if v.Args[0].Op != OpAMD64ANDB {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDB {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if v.AuxInt != 0 {
 			break
 		}
@@ -2222,11 +2283,12 @@ func rewriteValueAMD64_OpAMD64CMPBconst(v *Value, config *Config) bool {
 	// cond:
 	// result: (TESTBconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64ANDBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDBconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		x := v_0.Args[0]
 		if v.AuxInt != 0 {
 			break
 		}
@@ -2245,10 +2307,11 @@ func rewriteValueAMD64_OpAMD64CMPL(v *Value, config *Config) bool {
 	// result: (CMPLconst x [c])
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64CMPLconst)
 		v.AddArg(x)
 		v.AuxInt = c
@@ -2258,10 +2321,11 @@ func rewriteValueAMD64_OpAMD64CMPL(v *Value, config *Config) bool {
 	// cond:
 	// result: (InvertFlags (CMPLconst x [c]))
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64InvertFlags)
 		v0 := b.NewValue0(v.Line, OpAMD64CMPLconst, TypeFlags)
@@ -2279,10 +2343,11 @@ func rewriteValueAMD64_OpAMD64CMPLconst(v *Value, config *Config) bool {
 	// cond: int32(x)==int32(y)
 	// result: (FlagEQ)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int32(x) == int32(y)) {
 			break
@@ -2294,10 +2359,11 @@ func rewriteValueAMD64_OpAMD64CMPLconst(v *Value, config *Config) bool {
 	// cond: int32(x)<int32(y) && uint32(x)<uint32(y)
 	// result: (FlagLT_ULT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int32(x) < int32(y) && uint32(x) < uint32(y)) {
 			break
@@ -2309,10 +2375,11 @@ func rewriteValueAMD64_OpAMD64CMPLconst(v *Value, config *Config) bool {
 	// cond: int32(x)<int32(y) && uint32(x)>uint32(y)
 	// result: (FlagLT_UGT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int32(x) < int32(y) && uint32(x) > uint32(y)) {
 			break
@@ -2324,10 +2391,11 @@ func rewriteValueAMD64_OpAMD64CMPLconst(v *Value, config *Config) bool {
 	// cond: int32(x)>int32(y) && uint32(x)<uint32(y)
 	// result: (FlagGT_ULT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int32(x) > int32(y) && uint32(x) < uint32(y)) {
 			break
@@ -2339,10 +2407,11 @@ func rewriteValueAMD64_OpAMD64CMPLconst(v *Value, config *Config) bool {
 	// cond: int32(x)>int32(y) && uint32(x)>uint32(y)
 	// result: (FlagGT_UGT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int32(x) > int32(y) && uint32(x) > uint32(y)) {
 			break
@@ -2354,10 +2423,11 @@ func rewriteValueAMD64_OpAMD64CMPLconst(v *Value, config *Config) bool {
 	// cond: int32(m)+1==int32(n) && isPowerOfTwo(int64(int32(n)))
 	// result: (FlagLT_ULT)
 	for {
-		if v.Args[0].Op != OpAMD64ANDLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDLconst {
 			break
 		}
-		m := v.Args[0].AuxInt
+		m := v_0.AuxInt
 		n := v.AuxInt
 		if !(int32(m)+1 == int32(n) && isPowerOfTwo(int64(int32(n)))) {
 			break
@@ -2369,11 +2439,12 @@ func rewriteValueAMD64_OpAMD64CMPLconst(v *Value, config *Config) bool {
 	// cond:
 	// result: (TESTL x y)
 	for {
-		if v.Args[0].Op != OpAMD64ANDL {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDL {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if v.AuxInt != 0 {
 			break
 		}
@@ -2386,11 +2457,12 @@ func rewriteValueAMD64_OpAMD64CMPLconst(v *Value, config *Config) bool {
 	// cond:
 	// result: (TESTLconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64ANDLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		x := v_0.Args[0]
 		if v.AuxInt != 0 {
 			break
 		}
@@ -2409,10 +2481,11 @@ func rewriteValueAMD64_OpAMD64CMPQ(v *Value, config *Config) bool {
 	// result: (CMPQconst x [c])
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(is32Bit(c)) {
 			break
 		}
@@ -2425,10 +2498,11 @@ func rewriteValueAMD64_OpAMD64CMPQ(v *Value, config *Config) bool {
 	// cond: is32Bit(c)
 	// result: (InvertFlags (CMPQconst x [c]))
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		if !(is32Bit(c)) {
 			break
@@ -2449,10 +2523,11 @@ func rewriteValueAMD64_OpAMD64CMPQconst(v *Value, config *Config) bool {
 	// cond: x==y
 	// result: (FlagEQ)
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(x == y) {
 			break
@@ -2464,10 +2539,11 @@ func rewriteValueAMD64_OpAMD64CMPQconst(v *Value, config *Config) bool {
 	// cond: x<y && uint64(x)<uint64(y)
 	// result: (FlagLT_ULT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(x < y && uint64(x) < uint64(y)) {
 			break
@@ -2479,10 +2555,11 @@ func rewriteValueAMD64_OpAMD64CMPQconst(v *Value, config *Config) bool {
 	// cond: x<y && uint64(x)>uint64(y)
 	// result: (FlagLT_UGT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(x < y && uint64(x) > uint64(y)) {
 			break
@@ -2494,10 +2571,11 @@ func rewriteValueAMD64_OpAMD64CMPQconst(v *Value, config *Config) bool {
 	// cond: x>y && uint64(x)<uint64(y)
 	// result: (FlagGT_ULT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(x > y && uint64(x) < uint64(y)) {
 			break
@@ -2509,10 +2587,11 @@ func rewriteValueAMD64_OpAMD64CMPQconst(v *Value, config *Config) bool {
 	// cond: x>y && uint64(x)>uint64(y)
 	// result: (FlagGT_UGT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(x > y && uint64(x) > uint64(y)) {
 			break
@@ -2524,10 +2603,11 @@ func rewriteValueAMD64_OpAMD64CMPQconst(v *Value, config *Config) bool {
 	// cond: m+1==n && isPowerOfTwo(n)
 	// result: (FlagLT_ULT)
 	for {
-		if v.Args[0].Op != OpAMD64ANDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDQconst {
 			break
 		}
-		m := v.Args[0].AuxInt
+		m := v_0.AuxInt
 		n := v.AuxInt
 		if !(m+1 == n && isPowerOfTwo(n)) {
 			break
@@ -2539,11 +2619,12 @@ func rewriteValueAMD64_OpAMD64CMPQconst(v *Value, config *Config) bool {
 	// cond:
 	// result: (TESTQ x y)
 	for {
-		if v.Args[0].Op != OpAMD64ANDQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDQ {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if v.AuxInt != 0 {
 			break
 		}
@@ -2556,11 +2637,12 @@ func rewriteValueAMD64_OpAMD64CMPQconst(v *Value, config *Config) bool {
 	// cond:
 	// result: (TESTQconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64ANDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		x := v_0.Args[0]
 		if v.AuxInt != 0 {
 			break
 		}
@@ -2579,10 +2661,11 @@ func rewriteValueAMD64_OpAMD64CMPW(v *Value, config *Config) bool {
 	// result: (CMPWconst x [c])
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64CMPWconst)
 		v.AddArg(x)
 		v.AuxInt = c
@@ -2592,10 +2675,11 @@ func rewriteValueAMD64_OpAMD64CMPW(v *Value, config *Config) bool {
 	// cond:
 	// result: (InvertFlags (CMPWconst x [c]))
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64InvertFlags)
 		v0 := b.NewValue0(v.Line, OpAMD64CMPWconst, TypeFlags)
@@ -2613,10 +2697,11 @@ func rewriteValueAMD64_OpAMD64CMPWconst(v *Value, config *Config) bool {
 	// cond: int16(x)==int16(y)
 	// result: (FlagEQ)
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int16(x) == int16(y)) {
 			break
@@ -2628,10 +2713,11 @@ func rewriteValueAMD64_OpAMD64CMPWconst(v *Value, config *Config) bool {
 	// cond: int16(x)<int16(y) && uint16(x)<uint16(y)
 	// result: (FlagLT_ULT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int16(x) < int16(y) && uint16(x) < uint16(y)) {
 			break
@@ -2643,10 +2729,11 @@ func rewriteValueAMD64_OpAMD64CMPWconst(v *Value, config *Config) bool {
 	// cond: int16(x)<int16(y) && uint16(x)>uint16(y)
 	// result: (FlagLT_UGT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int16(x) < int16(y) && uint16(x) > uint16(y)) {
 			break
@@ -2658,10 +2745,11 @@ func rewriteValueAMD64_OpAMD64CMPWconst(v *Value, config *Config) bool {
 	// cond: int16(x)>int16(y) && uint16(x)<uint16(y)
 	// result: (FlagGT_ULT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int16(x) > int16(y) && uint16(x) < uint16(y)) {
 			break
@@ -2673,10 +2761,11 @@ func rewriteValueAMD64_OpAMD64CMPWconst(v *Value, config *Config) bool {
 	// cond: int16(x)>int16(y) && uint16(x)>uint16(y)
 	// result: (FlagGT_UGT)
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		x := v.Args[0].AuxInt
+		x := v_0.AuxInt
 		y := v.AuxInt
 		if !(int16(x) > int16(y) && uint16(x) > uint16(y)) {
 			break
@@ -2688,10 +2777,11 @@ func rewriteValueAMD64_OpAMD64CMPWconst(v *Value, config *Config) bool {
 	// cond: int16(m)+1==int16(n) && isPowerOfTwo(int64(int16(n)))
 	// result: (FlagLT_ULT)
 	for {
-		if v.Args[0].Op != OpAMD64ANDWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDWconst {
 			break
 		}
-		m := v.Args[0].AuxInt
+		m := v_0.AuxInt
 		n := v.AuxInt
 		if !(int16(m)+1 == int16(n) && isPowerOfTwo(int64(int16(n)))) {
 			break
@@ -2703,11 +2793,12 @@ func rewriteValueAMD64_OpAMD64CMPWconst(v *Value, config *Config) bool {
 	// cond:
 	// result: (TESTW x y)
 	for {
-		if v.Args[0].Op != OpAMD64ANDW {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDW {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if v.AuxInt != 0 {
 			break
 		}
@@ -2720,11 +2811,12 @@ func rewriteValueAMD64_OpAMD64CMPWconst(v *Value, config *Config) bool {
 	// cond:
 	// result: (TESTWconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64ANDWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDWconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		x := v_0.Args[0]
 		if v.AuxInt != 0 {
 			break
 		}
@@ -3927,11 +4019,12 @@ func rewriteValueAMD64_OpITab(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVQload ptr mem)
 	for {
-		if v.Args[0].Op != OpLoad {
+		v_0 := v.Args[0]
+		if v_0.Op != OpLoad {
 			break
 		}
-		ptr := v.Args[0].Args[0]
-		mem := v.Args[0].Args[1]
+		ptr := v_0.Args[0]
+		mem := v_0.Args[1]
 		v.reset(OpAMD64MOVQload)
 		v.AddArg(ptr)
 		v.AddArg(mem)
@@ -4019,11 +4112,12 @@ func rewriteValueAMD64_OpAMD64LEAQ(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		if !(is32Bit(c + d)) {
 			break
 		}
@@ -4039,11 +4133,12 @@ func rewriteValueAMD64_OpAMD64LEAQ(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQ {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if !(x.Op != OpSB && y.Op != OpSB) {
 			break
 		}
@@ -4060,12 +4155,13 @@ func rewriteValueAMD64_OpAMD64LEAQ(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		x := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		x := v_0.Args[0]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
 		}
@@ -4081,13 +4177,14 @@ func rewriteValueAMD64_OpAMD64LEAQ(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ1 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ1 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
 		}
@@ -4104,13 +4201,14 @@ func rewriteValueAMD64_OpAMD64LEAQ(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ2 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ2 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
 		}
@@ -4127,13 +4225,14 @@ func rewriteValueAMD64_OpAMD64LEAQ(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ4 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ4 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
 		}
@@ -4150,13 +4249,14 @@ func rewriteValueAMD64_OpAMD64LEAQ(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ8 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
 		}
@@ -4178,11 +4278,12 @@ func rewriteValueAMD64_OpAMD64LEAQ1(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		y := v.Args[1]
 		if !(is32Bit(c+d) && x.Op != OpSB) {
 			break
@@ -4201,11 +4302,12 @@ func rewriteValueAMD64_OpAMD64LEAQ1(v *Value, config *Config) bool {
 		c := v.AuxInt
 		s := v.Aux
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		y := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		y := v_1.Args[0]
 		if !(is32Bit(c+d) && y.Op != OpSB) {
 			break
 		}
@@ -4223,13 +4325,14 @@ func rewriteValueAMD64_OpAMD64LEAQ1(v *Value, config *Config) bool {
 		c := v.AuxInt
 		s := v.Aux
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64SHLQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[1].AuxInt != 1 {
+		if v_1.AuxInt != 1 {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64LEAQ2)
 		v.AuxInt = c
 		v.Aux = s
@@ -4243,13 +4346,14 @@ func rewriteValueAMD64_OpAMD64LEAQ1(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64SHLQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[0].AuxInt != 1 {
+		if v_0.AuxInt != 1 {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		y := v.Args[1]
 		v.reset(OpAMD64LEAQ2)
 		v.AuxInt = c
@@ -4265,13 +4369,14 @@ func rewriteValueAMD64_OpAMD64LEAQ1(v *Value, config *Config) bool {
 		c := v.AuxInt
 		s := v.Aux
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64SHLQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[1].AuxInt != 2 {
+		if v_1.AuxInt != 2 {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64LEAQ4)
 		v.AuxInt = c
 		v.Aux = s
@@ -4285,13 +4390,14 @@ func rewriteValueAMD64_OpAMD64LEAQ1(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64SHLQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[0].AuxInt != 2 {
+		if v_0.AuxInt != 2 {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		y := v.Args[1]
 		v.reset(OpAMD64LEAQ4)
 		v.AuxInt = c
@@ -4307,13 +4413,14 @@ func rewriteValueAMD64_OpAMD64LEAQ1(v *Value, config *Config) bool {
 		c := v.AuxInt
 		s := v.Aux
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64SHLQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[1].AuxInt != 3 {
+		if v_1.AuxInt != 3 {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64LEAQ8)
 		v.AuxInt = c
 		v.Aux = s
@@ -4327,13 +4434,14 @@ func rewriteValueAMD64_OpAMD64LEAQ1(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64SHLQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[0].AuxInt != 3 {
+		if v_0.AuxInt != 3 {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		y := v.Args[1]
 		v.reset(OpAMD64LEAQ8)
 		v.AuxInt = c
@@ -4348,12 +4456,13 @@ func rewriteValueAMD64_OpAMD64LEAQ1(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		x := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		x := v_0.Args[0]
 		y := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2) && x.Op != OpSB) {
 			break
@@ -4372,12 +4481,13 @@ func rewriteValueAMD64_OpAMD64LEAQ1(v *Value, config *Config) bool {
 		off1 := v.AuxInt
 		sym1 := v.Aux
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64LEAQ {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[1].AuxInt
-		sym2 := v.Args[1].Aux
-		y := v.Args[1].Args[0]
+		off2 := v_1.AuxInt
+		sym2 := v_1.Aux
+		y := v_1.Args[0]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2) && y.Op != OpSB) {
 			break
 		}
@@ -4399,11 +4509,12 @@ func rewriteValueAMD64_OpAMD64LEAQ2(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		y := v.Args[1]
 		if !(is32Bit(c+d) && x.Op != OpSB) {
 			break
@@ -4422,11 +4533,12 @@ func rewriteValueAMD64_OpAMD64LEAQ2(v *Value, config *Config) bool {
 		c := v.AuxInt
 		s := v.Aux
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		y := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		y := v_1.Args[0]
 		if !(is32Bit(c+2*d) && y.Op != OpSB) {
 			break
 		}
@@ -4444,13 +4556,14 @@ func rewriteValueAMD64_OpAMD64LEAQ2(v *Value, config *Config) bool {
 		c := v.AuxInt
 		s := v.Aux
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64SHLQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[1].AuxInt != 1 {
+		if v_1.AuxInt != 1 {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64LEAQ4)
 		v.AuxInt = c
 		v.Aux = s
@@ -4465,13 +4578,14 @@ func rewriteValueAMD64_OpAMD64LEAQ2(v *Value, config *Config) bool {
 		c := v.AuxInt
 		s := v.Aux
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64SHLQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[1].AuxInt != 2 {
+		if v_1.AuxInt != 2 {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64LEAQ8)
 		v.AuxInt = c
 		v.Aux = s
@@ -4485,12 +4599,13 @@ func rewriteValueAMD64_OpAMD64LEAQ2(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		x := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		x := v_0.Args[0]
 		y := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2) && x.Op != OpSB) {
 			break
@@ -4513,11 +4628,12 @@ func rewriteValueAMD64_OpAMD64LEAQ4(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		y := v.Args[1]
 		if !(is32Bit(c+d) && x.Op != OpSB) {
 			break
@@ -4536,11 +4652,12 @@ func rewriteValueAMD64_OpAMD64LEAQ4(v *Value, config *Config) bool {
 		c := v.AuxInt
 		s := v.Aux
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		y := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		y := v_1.Args[0]
 		if !(is32Bit(c+4*d) && y.Op != OpSB) {
 			break
 		}
@@ -4558,13 +4675,14 @@ func rewriteValueAMD64_OpAMD64LEAQ4(v *Value, config *Config) bool {
 		c := v.AuxInt
 		s := v.Aux
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64SHLQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[1].AuxInt != 1 {
+		if v_1.AuxInt != 1 {
 			break
 		}
-		y := v.Args[1].Args[0]
+		y := v_1.Args[0]
 		v.reset(OpAMD64LEAQ8)
 		v.AuxInt = c
 		v.Aux = s
@@ -4578,12 +4696,13 @@ func rewriteValueAMD64_OpAMD64LEAQ4(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		x := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		x := v_0.Args[0]
 		y := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2) && x.Op != OpSB) {
 			break
@@ -4606,11 +4725,12 @@ func rewriteValueAMD64_OpAMD64LEAQ8(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		y := v.Args[1]
 		if !(is32Bit(c+d) && x.Op != OpSB) {
 			break
@@ -4629,11 +4749,12 @@ func rewriteValueAMD64_OpAMD64LEAQ8(v *Value, config *Config) bool {
 		c := v.AuxInt
 		s := v.Aux
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		y := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		y := v_1.Args[0]
 		if !(is32Bit(c+8*d) && y.Op != OpSB) {
 			break
 		}
@@ -4650,12 +4771,13 @@ func rewriteValueAMD64_OpAMD64LEAQ8(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		x := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		x := v_0.Args[0]
 		y := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2) && x.Op != OpSB) {
 			break
@@ -5599,21 +5721,22 @@ func rewriteValueAMD64_OpLsh8x8(v *Value, config *Config) bool {
 func rewriteValueAMD64_OpAMD64MOVBQSX(v *Value, config *Config) bool {
 	b := v.Block
 	_ = b
-	// match: (MOVBQSX (MOVBload [off] {sym} ptr mem))
-	// cond: v.Args[0].Uses == 1
-	// result: @v.Args[0].Block (MOVBQSXload <v.Type> [off] {sym} ptr mem)
+	// match: (MOVBQSX x:(MOVBload [off] {sym} ptr mem))
+	// cond: x.Uses == 1
+	// result: @x.Block (MOVBQSXload <v.Type> [off] {sym} ptr mem)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBload {
+		x := v.Args[0]
+		if x.Op != OpAMD64MOVBload {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		mem := v.Args[0].Args[1]
-		if !(v.Args[0].Uses == 1) {
+		off := x.AuxInt
+		sym := x.Aux
+		ptr := x.Args[0]
+		mem := x.Args[1]
+		if !(x.Uses == 1) {
 			break
 		}
-		b = v.Args[0].Block
+		b = x.Block
 		v0 := b.NewValue0(v.Line, OpAMD64MOVBQSXload, v.Type)
 		v.reset(OpCopy)
 		v.AddArg(v0)
@@ -5627,11 +5750,12 @@ func rewriteValueAMD64_OpAMD64MOVBQSX(v *Value, config *Config) bool {
 	// cond: c & 0x80 == 0
 	// result: (ANDQconst [c & 0x7f] x)
 	for {
-		if v.Args[0].Op != OpAMD64ANDBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDBconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		x := v_0.Args[0]
 		if !(c&0x80 == 0) {
 			break
 		}
@@ -5651,12 +5775,13 @@ func rewriteValueAMD64_OpAMD64MOVBQSXload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -5673,21 +5798,22 @@ func rewriteValueAMD64_OpAMD64MOVBQSXload(v *Value, config *Config) bool {
 func rewriteValueAMD64_OpAMD64MOVBQZX(v *Value, config *Config) bool {
 	b := v.Block
 	_ = b
-	// match: (MOVBQZX (MOVBload [off] {sym} ptr mem))
-	// cond: v.Args[0].Uses == 1
-	// result: @v.Args[0].Block (MOVBQZXload <v.Type> [off] {sym} ptr mem)
+	// match: (MOVBQZX x:(MOVBload [off] {sym} ptr mem))
+	// cond: x.Uses == 1
+	// result: @x.Block (MOVBQZXload <v.Type> [off] {sym} ptr mem)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBload {
+		x := v.Args[0]
+		if x.Op != OpAMD64MOVBload {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		mem := v.Args[0].Args[1]
-		if !(v.Args[0].Uses == 1) {
+		off := x.AuxInt
+		sym := x.Aux
+		ptr := x.Args[0]
+		mem := x.Args[1]
+		if !(x.Uses == 1) {
 			break
 		}
-		b = v.Args[0].Block
+		b = x.Block
 		v0 := b.NewValue0(v.Line, OpAMD64MOVBQZXload, v.Type)
 		v.reset(OpCopy)
 		v.AddArg(v0)
@@ -5701,11 +5827,12 @@ func rewriteValueAMD64_OpAMD64MOVBQZX(v *Value, config *Config) bool {
 	// cond:
 	// result: (ANDQconst [c & 0xff] x)
 	for {
-		if v.Args[0].Op != OpAMD64ANDBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDBconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		x := v_0.Args[0]
 		v.reset(OpAMD64ANDQconst)
 		v.AuxInt = c & 0xff
 		v.AddArg(x)
@@ -5722,12 +5849,13 @@ func rewriteValueAMD64_OpAMD64MOVBQZXload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -5751,13 +5879,14 @@ func rewriteValueAMD64_OpAMD64MOVBload(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBstore {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBstore {
 			break
 		}
-		off2 := v.Args[1].AuxInt
-		sym2 := v.Args[1].Aux
-		ptr2 := v.Args[1].Args[0]
-		x := v.Args[1].Args[1]
+		off2 := v_1.AuxInt
+		sym2 := v_1.Aux
+		ptr2 := v_1.Args[0]
+		x := v_1.Args[1]
 		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
 			break
 		}
@@ -5772,11 +5901,12 @@ func rewriteValueAMD64_OpAMD64MOVBload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1 + off2)) {
 			break
@@ -5794,12 +5924,13 @@ func rewriteValueAMD64_OpAMD64MOVBload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -5817,13 +5948,14 @@ func rewriteValueAMD64_OpAMD64MOVBload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ1 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ1 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -5842,11 +5974,12 @@ func rewriteValueAMD64_OpAMD64MOVBload(v *Value, config *Config) bool {
 	for {
 		off := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQ {
 			break
 		}
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		mem := v.Args[1]
 		if !(ptr.Op != OpSB) {
 			break
@@ -5870,11 +6003,12 @@ func rewriteValueAMD64_OpAMD64MOVBloadidx1(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVBloadidx1)
@@ -5892,11 +6026,12 @@ func rewriteValueAMD64_OpAMD64MOVBloadidx1(v *Value, config *Config) bool {
 		c := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		idx := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVBloadidx1)
 		v.AuxInt = c + d
@@ -5918,10 +6053,11 @@ func rewriteValueAMD64_OpAMD64MOVBstore(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBQSX {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBQSX {
 			break
 		}
-		x := v.Args[1].Args[0]
+		x := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVBstore)
 		v.AuxInt = off
@@ -5938,10 +6074,11 @@ func rewriteValueAMD64_OpAMD64MOVBstore(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBQZX {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBQZX {
 			break
 		}
-		x := v.Args[1].Args[0]
+		x := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVBstore)
 		v.AuxInt = off
@@ -5957,11 +6094,12 @@ func rewriteValueAMD64_OpAMD64MOVBstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1 + off2)) {
@@ -5982,10 +6120,11 @@ func rewriteValueAMD64_OpAMD64MOVBstore(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		mem := v.Args[2]
 		if !(validOff(off)) {
 			break
@@ -6003,12 +6142,13 @@ func rewriteValueAMD64_OpAMD64MOVBstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -6028,13 +6168,14 @@ func rewriteValueAMD64_OpAMD64MOVBstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ1 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ1 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -6055,11 +6196,12 @@ func rewriteValueAMD64_OpAMD64MOVBstore(v *Value, config *Config) bool {
 	for {
 		off := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQ {
 			break
 		}
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(ptr.Op != OpSB) {
@@ -6085,11 +6227,12 @@ func rewriteValueAMD64_OpAMD64MOVBstoreconst(v *Value, config *Config) bool {
 	for {
 		sc := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off := v_0.AuxInt
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(ValAndOff(sc).canAdd(off)) {
 			break
@@ -6107,12 +6250,13 @@ func rewriteValueAMD64_OpAMD64MOVBstoreconst(v *Value, config *Config) bool {
 	for {
 		sc := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
+		off := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd(off)) {
 			break
@@ -6130,13 +6274,14 @@ func rewriteValueAMD64_OpAMD64MOVBstoreconst(v *Value, config *Config) bool {
 	for {
 		x := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ1 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ1 {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		mem := v.Args[1]
 		if !(canMergeSym(sym1, sym2)) {
 			break
@@ -6155,11 +6300,12 @@ func rewriteValueAMD64_OpAMD64MOVBstoreconst(v *Value, config *Config) bool {
 	for {
 		x := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQ {
 			break
 		}
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		mem := v.Args[1]
 		v.reset(OpAMD64MOVBstoreconstidx1)
 		v.AuxInt = x
@@ -6180,11 +6326,12 @@ func rewriteValueAMD64_OpAMD64MOVBstoreconstidx1(v *Value, config *Config) bool
 	for {
 		x := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVBstoreconstidx1)
@@ -6202,11 +6349,12 @@ func rewriteValueAMD64_OpAMD64MOVBstoreconstidx1(v *Value, config *Config) bool
 		x := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVBstoreconstidx1)
 		v.AuxInt = ValAndOff(x).add(c)
@@ -6227,11 +6375,12 @@ func rewriteValueAMD64_OpAMD64MOVBstoreidx1(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		val := v.Args[2]
 		mem := v.Args[3]
@@ -6251,11 +6400,12 @@ func rewriteValueAMD64_OpAMD64MOVBstoreidx1(v *Value, config *Config) bool {
 		c := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		idx := v_1.Args[0]
 		val := v.Args[2]
 		mem := v.Args[3]
 		v.reset(OpAMD64MOVBstoreidx1)
@@ -6272,21 +6422,22 @@ func rewriteValueAMD64_OpAMD64MOVBstoreidx1(v *Value, config *Config) bool {
 func rewriteValueAMD64_OpAMD64MOVLQSX(v *Value, config *Config) bool {
 	b := v.Block
 	_ = b
-	// match: (MOVLQSX (MOVLload [off] {sym} ptr mem))
-	// cond: v.Args[0].Uses == 1
-	// result: @v.Args[0].Block (MOVLQSXload <v.Type> [off] {sym} ptr mem)
+	// match: (MOVLQSX x:(MOVLload [off] {sym} ptr mem))
+	// cond: x.Uses == 1
+	// result: @x.Block (MOVLQSXload <v.Type> [off] {sym} ptr mem)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLload {
+		x := v.Args[0]
+		if x.Op != OpAMD64MOVLload {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		mem := v.Args[0].Args[1]
-		if !(v.Args[0].Uses == 1) {
+		off := x.AuxInt
+		sym := x.Aux
+		ptr := x.Args[0]
+		mem := x.Args[1]
+		if !(x.Uses == 1) {
 			break
 		}
-		b = v.Args[0].Block
+		b = x.Block
 		v0 := b.NewValue0(v.Line, OpAMD64MOVLQSXload, v.Type)
 		v.reset(OpCopy)
 		v.AddArg(v0)
@@ -6300,11 +6451,12 @@ func rewriteValueAMD64_OpAMD64MOVLQSX(v *Value, config *Config) bool {
 	// cond: c & 0x80000000 == 0
 	// result: (ANDQconst [c & 0x7fffffff] x)
 	for {
-		if v.Args[0].Op != OpAMD64ANDLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		x := v_0.Args[0]
 		if !(c&0x80000000 == 0) {
 			break
 		}
@@ -6324,12 +6476,13 @@ func rewriteValueAMD64_OpAMD64MOVLQSXload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -6346,21 +6499,22 @@ func rewriteValueAMD64_OpAMD64MOVLQSXload(v *Value, config *Config) bool {
 func rewriteValueAMD64_OpAMD64MOVLQZX(v *Value, config *Config) bool {
 	b := v.Block
 	_ = b
-	// match: (MOVLQZX (MOVLload [off] {sym} ptr mem))
-	// cond: v.Args[0].Uses == 1
-	// result: @v.Args[0].Block (MOVLQZXload <v.Type> [off] {sym} ptr mem)
+	// match: (MOVLQZX x:(MOVLload [off] {sym} ptr mem))
+	// cond: x.Uses == 1
+	// result: @x.Block (MOVLQZXload <v.Type> [off] {sym} ptr mem)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLload {
+		x := v.Args[0]
+		if x.Op != OpAMD64MOVLload {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		mem := v.Args[0].Args[1]
-		if !(v.Args[0].Uses == 1) {
+		off := x.AuxInt
+		sym := x.Aux
+		ptr := x.Args[0]
+		mem := x.Args[1]
+		if !(x.Uses == 1) {
 			break
 		}
-		b = v.Args[0].Block
+		b = x.Block
 		v0 := b.NewValue0(v.Line, OpAMD64MOVLQZXload, v.Type)
 		v.reset(OpCopy)
 		v.AddArg(v0)
@@ -6374,11 +6528,12 @@ func rewriteValueAMD64_OpAMD64MOVLQZX(v *Value, config *Config) bool {
 	// cond: c & 0x80000000 == 0
 	// result: (ANDQconst [c & 0x7fffffff] x)
 	for {
-		if v.Args[0].Op != OpAMD64ANDLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		x := v_0.Args[0]
 		if !(c&0x80000000 == 0) {
 			break
 		}
@@ -6398,12 +6553,13 @@ func rewriteValueAMD64_OpAMD64MOVLQZXload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -6427,13 +6583,14 @@ func rewriteValueAMD64_OpAMD64MOVLload(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLstore {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLstore {
 			break
 		}
-		off2 := v.Args[1].AuxInt
-		sym2 := v.Args[1].Aux
-		ptr2 := v.Args[1].Args[0]
-		x := v.Args[1].Args[1]
+		off2 := v_1.AuxInt
+		sym2 := v_1.Aux
+		ptr2 := v_1.Args[0]
+		x := v_1.Args[1]
 		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
 			break
 		}
@@ -6448,11 +6605,12 @@ func rewriteValueAMD64_OpAMD64MOVLload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1 + off2)) {
 			break
@@ -6470,12 +6628,13 @@ func rewriteValueAMD64_OpAMD64MOVLload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -6493,13 +6652,14 @@ func rewriteValueAMD64_OpAMD64MOVLload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ4 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ4 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -6523,11 +6683,12 @@ func rewriteValueAMD64_OpAMD64MOVLloadidx4(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVLloadidx4)
@@ -6545,11 +6706,12 @@ func rewriteValueAMD64_OpAMD64MOVLloadidx4(v *Value, config *Config) bool {
 		c := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		idx := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVLloadidx4)
 		v.AuxInt = c + 4*d
@@ -6571,10 +6733,11 @@ func rewriteValueAMD64_OpAMD64MOVLstore(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLQSX {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLQSX {
 			break
 		}
-		x := v.Args[1].Args[0]
+		x := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVLstore)
 		v.AuxInt = off
@@ -6591,10 +6754,11 @@ func rewriteValueAMD64_OpAMD64MOVLstore(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLQZX {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLQZX {
 			break
 		}
-		x := v.Args[1].Args[0]
+		x := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVLstore)
 		v.AuxInt = off
@@ -6610,11 +6774,12 @@ func rewriteValueAMD64_OpAMD64MOVLstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1 + off2)) {
@@ -6635,10 +6800,11 @@ func rewriteValueAMD64_OpAMD64MOVLstore(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		mem := v.Args[2]
 		if !(validOff(off)) {
 			break
@@ -6656,12 +6822,13 @@ func rewriteValueAMD64_OpAMD64MOVLstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -6681,13 +6848,14 @@ func rewriteValueAMD64_OpAMD64MOVLstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ4 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ4 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -6713,11 +6881,12 @@ func rewriteValueAMD64_OpAMD64MOVLstoreconst(v *Value, config *Config) bool {
 	for {
 		sc := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off := v_0.AuxInt
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(ValAndOff(sc).canAdd(off)) {
 			break
@@ -6735,12 +6904,13 @@ func rewriteValueAMD64_OpAMD64MOVLstoreconst(v *Value, config *Config) bool {
 	for {
 		sc := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
+		off := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd(off)) {
 			break
@@ -6758,13 +6928,14 @@ func rewriteValueAMD64_OpAMD64MOVLstoreconst(v *Value, config *Config) bool {
 	for {
 		x := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ4 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ4 {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		mem := v.Args[1]
 		if !(canMergeSym(sym1, sym2)) {
 			break
@@ -6788,11 +6959,12 @@ func rewriteValueAMD64_OpAMD64MOVLstoreconstidx4(v *Value, config *Config) bool
 	for {
 		x := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVLstoreconstidx4)
@@ -6810,11 +6982,12 @@ func rewriteValueAMD64_OpAMD64MOVLstoreconstidx4(v *Value, config *Config) bool
 		x := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVLstoreconstidx4)
 		v.AuxInt = ValAndOff(x).add(4 * c)
@@ -6835,11 +7008,12 @@ func rewriteValueAMD64_OpAMD64MOVLstoreidx4(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		val := v.Args[2]
 		mem := v.Args[3]
@@ -6859,11 +7033,12 @@ func rewriteValueAMD64_OpAMD64MOVLstoreidx4(v *Value, config *Config) bool {
 		c := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		idx := v_1.Args[0]
 		val := v.Args[2]
 		mem := v.Args[3]
 		v.reset(OpAMD64MOVLstoreidx4)
@@ -6886,11 +7061,12 @@ func rewriteValueAMD64_OpAMD64MOVOload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1 + off2)) {
 			break
@@ -6908,12 +7084,13 @@ func rewriteValueAMD64_OpAMD64MOVOload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -6936,11 +7113,12 @@ func rewriteValueAMD64_OpAMD64MOVOstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1 + off2)) {
@@ -6960,12 +7138,13 @@ func rewriteValueAMD64_OpAMD64MOVOstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -6991,13 +7170,14 @@ func rewriteValueAMD64_OpAMD64MOVQload(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQstore {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQstore {
 			break
 		}
-		off2 := v.Args[1].AuxInt
-		sym2 := v.Args[1].Aux
-		ptr2 := v.Args[1].Args[0]
-		x := v.Args[1].Args[1]
+		off2 := v_1.AuxInt
+		sym2 := v_1.Aux
+		ptr2 := v_1.Args[0]
+		x := v_1.Args[1]
 		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
 			break
 		}
@@ -7012,11 +7192,12 @@ func rewriteValueAMD64_OpAMD64MOVQload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1 + off2)) {
 			break
@@ -7034,12 +7215,13 @@ func rewriteValueAMD64_OpAMD64MOVQload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -7057,13 +7239,14 @@ func rewriteValueAMD64_OpAMD64MOVQload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ8 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -7087,11 +7270,12 @@ func rewriteValueAMD64_OpAMD64MOVQloadidx8(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVQloadidx8)
@@ -7109,11 +7293,12 @@ func rewriteValueAMD64_OpAMD64MOVQloadidx8(v *Value, config *Config) bool {
 		c := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		idx := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVQloadidx8)
 		v.AuxInt = c + 8*d
@@ -7134,11 +7319,12 @@ func rewriteValueAMD64_OpAMD64MOVQstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1 + off2)) {
@@ -7159,10 +7345,11 @@ func rewriteValueAMD64_OpAMD64MOVQstore(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		mem := v.Args[2]
 		if !(validValAndOff(c, off)) {
 			break
@@ -7180,12 +7367,13 @@ func rewriteValueAMD64_OpAMD64MOVQstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -7205,13 +7393,14 @@ func rewriteValueAMD64_OpAMD64MOVQstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ8 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -7237,11 +7426,12 @@ func rewriteValueAMD64_OpAMD64MOVQstoreconst(v *Value, config *Config) bool {
 	for {
 		sc := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off := v_0.AuxInt
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(ValAndOff(sc).canAdd(off)) {
 			break
@@ -7259,12 +7449,13 @@ func rewriteValueAMD64_OpAMD64MOVQstoreconst(v *Value, config *Config) bool {
 	for {
 		sc := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
+		off := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd(off)) {
 			break
@@ -7282,13 +7473,14 @@ func rewriteValueAMD64_OpAMD64MOVQstoreconst(v *Value, config *Config) bool {
 	for {
 		x := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ8 {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		mem := v.Args[1]
 		if !(canMergeSym(sym1, sym2)) {
 			break
@@ -7312,11 +7504,12 @@ func rewriteValueAMD64_OpAMD64MOVQstoreconstidx8(v *Value, config *Config) bool
 	for {
 		x := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVQstoreconstidx8)
@@ -7334,11 +7527,12 @@ func rewriteValueAMD64_OpAMD64MOVQstoreconstidx8(v *Value, config *Config) bool
 		x := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVQstoreconstidx8)
 		v.AuxInt = ValAndOff(x).add(8 * c)
@@ -7359,11 +7553,12 @@ func rewriteValueAMD64_OpAMD64MOVQstoreidx8(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		val := v.Args[2]
 		mem := v.Args[3]
@@ -7383,11 +7578,12 @@ func rewriteValueAMD64_OpAMD64MOVQstoreidx8(v *Value, config *Config) bool {
 		c := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		idx := v_1.Args[0]
 		val := v.Args[2]
 		mem := v.Args[3]
 		v.reset(OpAMD64MOVQstoreidx8)
@@ -7410,11 +7606,12 @@ func rewriteValueAMD64_OpAMD64MOVSDload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1 + off2)) {
 			break
@@ -7432,12 +7629,13 @@ func rewriteValueAMD64_OpAMD64MOVSDload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -7455,13 +7653,14 @@ func rewriteValueAMD64_OpAMD64MOVSDload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ8 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -7485,11 +7684,12 @@ func rewriteValueAMD64_OpAMD64MOVSDloadidx8(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVSDloadidx8)
@@ -7507,11 +7707,12 @@ func rewriteValueAMD64_OpAMD64MOVSDloadidx8(v *Value, config *Config) bool {
 		c := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		idx := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVSDloadidx8)
 		v.AuxInt = c + 8*d
@@ -7532,11 +7733,12 @@ func rewriteValueAMD64_OpAMD64MOVSDstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1 + off2)) {
@@ -7556,12 +7758,13 @@ func rewriteValueAMD64_OpAMD64MOVSDstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -7581,13 +7784,14 @@ func rewriteValueAMD64_OpAMD64MOVSDstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ8 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -7613,11 +7817,12 @@ func rewriteValueAMD64_OpAMD64MOVSDstoreidx8(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		val := v.Args[2]
 		mem := v.Args[3]
@@ -7637,11 +7842,12 @@ func rewriteValueAMD64_OpAMD64MOVSDstoreidx8(v *Value, config *Config) bool {
 		c := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		idx := v_1.Args[0]
 		val := v.Args[2]
 		mem := v.Args[3]
 		v.reset(OpAMD64MOVSDstoreidx8)
@@ -7664,11 +7870,12 @@ func rewriteValueAMD64_OpAMD64MOVSSload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1 + off2)) {
 			break
@@ -7686,12 +7893,13 @@ func rewriteValueAMD64_OpAMD64MOVSSload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -7709,13 +7917,14 @@ func rewriteValueAMD64_OpAMD64MOVSSload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ4 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ4 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -7739,11 +7948,12 @@ func rewriteValueAMD64_OpAMD64MOVSSloadidx4(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVSSloadidx4)
@@ -7761,11 +7971,12 @@ func rewriteValueAMD64_OpAMD64MOVSSloadidx4(v *Value, config *Config) bool {
 		c := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		idx := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVSSloadidx4)
 		v.AuxInt = c + 4*d
@@ -7786,11 +7997,12 @@ func rewriteValueAMD64_OpAMD64MOVSSstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1 + off2)) {
@@ -7810,12 +8022,13 @@ func rewriteValueAMD64_OpAMD64MOVSSstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -7835,13 +8048,14 @@ func rewriteValueAMD64_OpAMD64MOVSSstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ4 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ4 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -7867,11 +8081,12 @@ func rewriteValueAMD64_OpAMD64MOVSSstoreidx4(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		val := v.Args[2]
 		mem := v.Args[3]
@@ -7891,11 +8106,12 @@ func rewriteValueAMD64_OpAMD64MOVSSstoreidx4(v *Value, config *Config) bool {
 		c := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		idx := v_1.Args[0]
 		val := v.Args[2]
 		mem := v.Args[3]
 		v.reset(OpAMD64MOVSSstoreidx4)
@@ -7912,21 +8128,22 @@ func rewriteValueAMD64_OpAMD64MOVSSstoreidx4(v *Value, config *Config) bool {
 func rewriteValueAMD64_OpAMD64MOVWQSX(v *Value, config *Config) bool {
 	b := v.Block
 	_ = b
-	// match: (MOVWQSX (MOVWload [off] {sym} ptr mem))
-	// cond: v.Args[0].Uses == 1
-	// result: @v.Args[0].Block (MOVWQSXload <v.Type> [off] {sym} ptr mem)
+	// match: (MOVWQSX x:(MOVWload [off] {sym} ptr mem))
+	// cond: x.Uses == 1
+	// result: @x.Block (MOVWQSXload <v.Type> [off] {sym} ptr mem)
 	for {
-		if v.Args[0].Op != OpAMD64MOVWload {
+		x := v.Args[0]
+		if x.Op != OpAMD64MOVWload {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		mem := v.Args[0].Args[1]
-		if !(v.Args[0].Uses == 1) {
+		off := x.AuxInt
+		sym := x.Aux
+		ptr := x.Args[0]
+		mem := x.Args[1]
+		if !(x.Uses == 1) {
 			break
 		}
-		b = v.Args[0].Block
+		b = x.Block
 		v0 := b.NewValue0(v.Line, OpAMD64MOVWQSXload, v.Type)
 		v.reset(OpCopy)
 		v.AddArg(v0)
@@ -7940,11 +8157,12 @@ func rewriteValueAMD64_OpAMD64MOVWQSX(v *Value, config *Config) bool {
 	// cond: c & 0x8000 == 0
 	// result: (ANDQconst [c & 0x7fff] x)
 	for {
-		if v.Args[0].Op != OpAMD64ANDWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDWconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		x := v_0.Args[0]
 		if !(c&0x8000 == 0) {
 			break
 		}
@@ -7964,12 +8182,13 @@ func rewriteValueAMD64_OpAMD64MOVWQSXload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -7986,21 +8205,22 @@ func rewriteValueAMD64_OpAMD64MOVWQSXload(v *Value, config *Config) bool {
 func rewriteValueAMD64_OpAMD64MOVWQZX(v *Value, config *Config) bool {
 	b := v.Block
 	_ = b
-	// match: (MOVWQZX (MOVWload [off] {sym} ptr mem))
-	// cond: v.Args[0].Uses == 1
-	// result: @v.Args[0].Block (MOVWQZXload <v.Type> [off] {sym} ptr mem)
+	// match: (MOVWQZX x:(MOVWload [off] {sym} ptr mem))
+	// cond: x.Uses == 1
+	// result: @x.Block (MOVWQZXload <v.Type> [off] {sym} ptr mem)
 	for {
-		if v.Args[0].Op != OpAMD64MOVWload {
+		x := v.Args[0]
+		if x.Op != OpAMD64MOVWload {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		mem := v.Args[0].Args[1]
-		if !(v.Args[0].Uses == 1) {
+		off := x.AuxInt
+		sym := x.Aux
+		ptr := x.Args[0]
+		mem := x.Args[1]
+		if !(x.Uses == 1) {
 			break
 		}
-		b = v.Args[0].Block
+		b = x.Block
 		v0 := b.NewValue0(v.Line, OpAMD64MOVWQZXload, v.Type)
 		v.reset(OpCopy)
 		v.AddArg(v0)
@@ -8014,11 +8234,12 @@ func rewriteValueAMD64_OpAMD64MOVWQZX(v *Value, config *Config) bool {
 	// cond:
 	// result: (ANDQconst [c & 0xffff] x)
 	for {
-		if v.Args[0].Op != OpAMD64ANDWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ANDWconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		x := v_0.Args[0]
 		v.reset(OpAMD64ANDQconst)
 		v.AuxInt = c & 0xffff
 		v.AddArg(x)
@@ -8035,12 +8256,13 @@ func rewriteValueAMD64_OpAMD64MOVWQZXload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -8064,13 +8286,14 @@ func rewriteValueAMD64_OpAMD64MOVWload(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWstore {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWstore {
 			break
 		}
-		off2 := v.Args[1].AuxInt
-		sym2 := v.Args[1].Aux
-		ptr2 := v.Args[1].Args[0]
-		x := v.Args[1].Args[1]
+		off2 := v_1.AuxInt
+		sym2 := v_1.Aux
+		ptr2 := v_1.Args[0]
+		x := v_1.Args[1]
 		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
 			break
 		}
@@ -8085,11 +8308,12 @@ func rewriteValueAMD64_OpAMD64MOVWload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1 + off2)) {
 			break
@@ -8107,12 +8331,13 @@ func rewriteValueAMD64_OpAMD64MOVWload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -8130,13 +8355,14 @@ func rewriteValueAMD64_OpAMD64MOVWload(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ2 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ2 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		mem := v.Args[1]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
 			break
@@ -8160,11 +8386,12 @@ func rewriteValueAMD64_OpAMD64MOVWloadidx2(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVWloadidx2)
@@ -8182,11 +8409,12 @@ func rewriteValueAMD64_OpAMD64MOVWloadidx2(v *Value, config *Config) bool {
 		c := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		idx := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVWloadidx2)
 		v.AuxInt = c + 2*d
@@ -8208,10 +8436,11 @@ func rewriteValueAMD64_OpAMD64MOVWstore(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWQSX {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWQSX {
 			break
 		}
-		x := v.Args[1].Args[0]
+		x := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVWstore)
 		v.AuxInt = off
@@ -8228,10 +8457,11 @@ func rewriteValueAMD64_OpAMD64MOVWstore(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWQZX {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWQZX {
 			break
 		}
-		x := v.Args[1].Args[0]
+		x := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVWstore)
 		v.AuxInt = off
@@ -8247,11 +8477,12 @@ func rewriteValueAMD64_OpAMD64MOVWstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1 + off2)) {
@@ -8272,10 +8503,11 @@ func rewriteValueAMD64_OpAMD64MOVWstore(v *Value, config *Config) bool {
 		off := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		mem := v.Args[2]
 		if !(validOff(off)) {
 			break
@@ -8293,12 +8525,13 @@ func rewriteValueAMD64_OpAMD64MOVWstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		base := v.Args[0].Args[0]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		base := v_0.Args[0]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -8318,13 +8551,14 @@ func rewriteValueAMD64_OpAMD64MOVWstore(v *Value, config *Config) bool {
 	for {
 		off1 := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ2 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ2 {
 			break
 		}
-		off2 := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		val := v.Args[1]
 		mem := v.Args[2]
 		if !(is32Bit(off1+off2) && canMergeSym(sym1, sym2)) {
@@ -8350,11 +8584,12 @@ func rewriteValueAMD64_OpAMD64MOVWstoreconst(v *Value, config *Config) bool {
 	for {
 		sc := v.AuxInt
 		s := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		off := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		off := v_0.AuxInt
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(ValAndOff(sc).canAdd(off)) {
 			break
@@ -8372,12 +8607,13 @@ func rewriteValueAMD64_OpAMD64MOVWstoreconst(v *Value, config *Config) bool {
 	for {
 		sc := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
+		off := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
 		mem := v.Args[1]
 		if !(canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd(off)) {
 			break
@@ -8395,13 +8631,14 @@ func rewriteValueAMD64_OpAMD64MOVWstoreconst(v *Value, config *Config) bool {
 	for {
 		x := v.AuxInt
 		sym1 := v.Aux
-		if v.Args[0].Op != OpAMD64LEAQ2 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64LEAQ2 {
 			break
 		}
-		off := v.Args[0].AuxInt
-		sym2 := v.Args[0].Aux
-		ptr := v.Args[0].Args[0]
-		idx := v.Args[0].Args[1]
+		off := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		idx := v_0.Args[1]
 		mem := v.Args[1]
 		if !(canMergeSym(sym1, sym2)) {
 			break
@@ -8425,11 +8662,12 @@ func rewriteValueAMD64_OpAMD64MOVWstoreconstidx2(v *Value, config *Config) bool
 	for {
 		x := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		c := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVWstoreconstidx2)
@@ -8447,11 +8685,12 @@ func rewriteValueAMD64_OpAMD64MOVWstoreconstidx2(v *Value, config *Config) bool
 		x := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpAMD64MOVWstoreconstidx2)
 		v.AuxInt = ValAndOff(x).add(2 * c)
@@ -8472,11 +8711,12 @@ func rewriteValueAMD64_OpAMD64MOVWstoreidx2(v *Value, config *Config) bool {
 	for {
 		c := v.AuxInt
 		sym := v.Aux
-		if v.Args[0].Op != OpAMD64ADDQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		ptr := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		ptr := v_0.Args[0]
 		idx := v.Args[1]
 		val := v.Args[2]
 		mem := v.Args[3]
@@ -8496,11 +8736,12 @@ func rewriteValueAMD64_OpAMD64MOVWstoreidx2(v *Value, config *Config) bool {
 		c := v.AuxInt
 		sym := v.Aux
 		ptr := v.Args[0]
-		if v.Args[1].Op != OpAMD64ADDQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64ADDQconst {
 			break
 		}
-		d := v.Args[1].AuxInt
-		idx := v.Args[1].Args[0]
+		d := v_1.AuxInt
+		idx := v_1.Args[0]
 		val := v.Args[2]
 		mem := v.Args[3]
 		v.reset(OpAMD64MOVWstoreidx2)
@@ -8522,10 +8763,11 @@ func rewriteValueAMD64_OpAMD64MULB(v *Value, config *Config) bool {
 	// result: (MULBconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64MULBconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -8535,10 +8777,11 @@ func rewriteValueAMD64_OpAMD64MULB(v *Value, config *Config) bool {
 	// cond:
 	// result: (MULBconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64MULBconst)
 		v.AuxInt = c
@@ -8555,10 +8798,11 @@ func rewriteValueAMD64_OpAMD64MULBconst(v *Value, config *Config) bool {
 	// result: (MOVBconst [c*d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVBconst)
 		v.AuxInt = c * d
 		return true
@@ -8573,10 +8817,11 @@ func rewriteValueAMD64_OpAMD64MULL(v *Value, config *Config) bool {
 	// result: (MULLconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64MULLconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -8586,10 +8831,11 @@ func rewriteValueAMD64_OpAMD64MULL(v *Value, config *Config) bool {
 	// cond:
 	// result: (MULLconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64MULLconst)
 		v.AuxInt = c
@@ -8606,10 +8852,11 @@ func rewriteValueAMD64_OpAMD64MULLconst(v *Value, config *Config) bool {
 	// result: (MOVLconst [c*d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVLconst)
 		v.AuxInt = c * d
 		return true
@@ -8624,10 +8871,11 @@ func rewriteValueAMD64_OpAMD64MULQ(v *Value, config *Config) bool {
 	// result: (MULQconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(is32Bit(c)) {
 			break
 		}
@@ -8640,10 +8888,11 @@ func rewriteValueAMD64_OpAMD64MULQ(v *Value, config *Config) bool {
 	// cond: is32Bit(c)
 	// result: (MULQconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		if !(is32Bit(c)) {
 			break
@@ -8768,10 +9017,11 @@ func rewriteValueAMD64_OpAMD64MULQconst(v *Value, config *Config) bool {
 	// result: (MOVQconst [c*d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVQconst)
 		v.AuxInt = c * d
 		return true
@@ -8786,10 +9036,11 @@ func rewriteValueAMD64_OpAMD64MULW(v *Value, config *Config) bool {
 	// result: (MULWconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64MULWconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -8799,10 +9050,11 @@ func rewriteValueAMD64_OpAMD64MULW(v *Value, config *Config) bool {
 	// cond:
 	// result: (MULWconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64MULWconst)
 		v.AuxInt = c
@@ -8819,10 +9071,11 @@ func rewriteValueAMD64_OpAMD64MULWconst(v *Value, config *Config) bool {
 	// result: (MOVWconst [c*d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVWconst)
 		v.AuxInt = c * d
 		return true
@@ -9422,10 +9675,11 @@ func rewriteValueAMD64_OpAMD64NEGB(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [-c])
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpAMD64MOVBconst)
 		v.AuxInt = -c
 		return true
@@ -9439,10 +9693,11 @@ func rewriteValueAMD64_OpAMD64NEGL(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVLconst [-c])
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpAMD64MOVLconst)
 		v.AuxInt = -c
 		return true
@@ -9456,10 +9711,11 @@ func rewriteValueAMD64_OpAMD64NEGQ(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVQconst [-c])
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpAMD64MOVQconst)
 		v.AuxInt = -c
 		return true
@@ -9473,10 +9729,11 @@ func rewriteValueAMD64_OpAMD64NEGW(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVWconst [-c])
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpAMD64MOVWconst)
 		v.AuxInt = -c
 		return true
@@ -9490,10 +9747,11 @@ func rewriteValueAMD64_OpAMD64NOTB(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [^c])
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpAMD64MOVBconst)
 		v.AuxInt = ^c
 		return true
@@ -9507,10 +9765,11 @@ func rewriteValueAMD64_OpAMD64NOTL(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVLconst [^c])
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpAMD64MOVLconst)
 		v.AuxInt = ^c
 		return true
@@ -9524,10 +9783,11 @@ func rewriteValueAMD64_OpAMD64NOTQ(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVQconst [^c])
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpAMD64MOVQconst)
 		v.AuxInt = ^c
 		return true
@@ -9541,10 +9801,11 @@ func rewriteValueAMD64_OpAMD64NOTW(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVWconst [^c])
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpAMD64MOVWconst)
 		v.AuxInt = ^c
 		return true
@@ -9806,10 +10067,11 @@ func rewriteValueAMD64_OpAMD64ORB(v *Value, config *Config) bool {
 	// result: (ORBconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64ORBconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -9819,10 +10081,11 @@ func rewriteValueAMD64_OpAMD64ORB(v *Value, config *Config) bool {
 	// cond:
 	// result: (ORBconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64ORBconst)
 		v.AuxInt = c
@@ -9834,7 +10097,7 @@ func rewriteValueAMD64_OpAMD64ORB(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -9878,10 +10141,11 @@ func rewriteValueAMD64_OpAMD64ORBconst(v *Value, config *Config) bool {
 	// result: (MOVBconst [c|d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVBconst)
 		v.AuxInt = c | d
 		return true
@@ -9896,10 +10160,11 @@ func rewriteValueAMD64_OpAMD64ORL(v *Value, config *Config) bool {
 	// result: (ORLconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64ORLconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -9909,10 +10174,11 @@ func rewriteValueAMD64_OpAMD64ORL(v *Value, config *Config) bool {
 	// cond:
 	// result: (ORLconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64ORLconst)
 		v.AuxInt = c
@@ -9924,7 +10190,7 @@ func rewriteValueAMD64_OpAMD64ORL(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -9932,87 +10198,96 @@ func rewriteValueAMD64_OpAMD64ORL(v *Value, config *Config) bool {
 		v.AddArg(x)
 		return true
 	}
-	// match: (ORL (ORL (ORL                     (MOVBQZXload [i]   {s} p mem)     (SHLLconst [8]  (MOVBQZXload [i+1] {s} p mem)))     (SHLLconst [16] (MOVBQZXload [i+2] {s} p mem)))     (SHLLconst [24] (MOVBQZXload [i+3] {s} p mem)))
+	// match: (ORL (ORL (ORL                   x:(MOVBQZXload [i]   {s} p mem)     (SHLLconst [8]  (MOVBQZXload [i+1] {s} p mem)))     (SHLLconst [16] (MOVBQZXload [i+2] {s} p mem)))     (SHLLconst [24] (MOVBQZXload [i+3] {s} p mem)))
 	// cond:
-	// result: @v.Args[0].Args[0].Args[0].Block (MOVLload [i] {s} p mem)
+	// result: @x.Block (MOVLload [i] {s} p mem)
 	for {
-		if v.Args[0].Op != OpAMD64ORL {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ORL {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpAMD64ORL {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpAMD64ORL {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Op != OpAMD64MOVBQZXload {
+		x := v_0_0.Args[0]
+		if x.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		i := v.Args[0].Args[0].Args[0].AuxInt
-		s := v.Args[0].Args[0].Args[0].Aux
-		p := v.Args[0].Args[0].Args[0].Args[0]
-		mem := v.Args[0].Args[0].Args[0].Args[1]
-		if v.Args[0].Args[0].Args[1].Op != OpAMD64SHLLconst {
+		i := x.AuxInt
+		s := x.Aux
+		p := x.Args[0]
+		mem := x.Args[1]
+		v_0_0_1 := v_0_0.Args[1]
+		if v_0_0_1.Op != OpAMD64SHLLconst {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].AuxInt != 8 {
+		if v_0_0_1.AuxInt != 8 {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].Args[0].Op != OpAMD64MOVBQZXload {
+		v_0_0_1_0 := v_0_0_1.Args[0]
+		if v_0_0_1_0.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].Args[0].AuxInt != i+1 {
+		if v_0_0_1_0.AuxInt != i+1 {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].Args[0].Aux != s {
+		if v_0_0_1_0.Aux != s {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].Args[0].Args[0] != p {
+		if p != v_0_0_1_0.Args[0] {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].Args[0].Args[1] != mem {
+		if mem != v_0_0_1_0.Args[1] {
 			break
 		}
-		if v.Args[0].Args[1].Op != OpAMD64SHLLconst {
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpAMD64SHLLconst {
 			break
 		}
-		if v.Args[0].Args[1].AuxInt != 16 {
+		if v_0_1.AuxInt != 16 {
 			break
 		}
-		if v.Args[0].Args[1].Args[0].Op != OpAMD64MOVBQZXload {
+		v_0_1_0 := v_0_1.Args[0]
+		if v_0_1_0.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		if v.Args[0].Args[1].Args[0].AuxInt != i+2 {
+		if v_0_1_0.AuxInt != i+2 {
 			break
 		}
-		if v.Args[0].Args[1].Args[0].Aux != s {
+		if v_0_1_0.Aux != s {
 			break
 		}
-		if v.Args[0].Args[1].Args[0].Args[0] != p {
+		if p != v_0_1_0.Args[0] {
 			break
 		}
-		if v.Args[0].Args[1].Args[0].Args[1] != mem {
+		if mem != v_0_1_0.Args[1] {
 			break
 		}
-		if v.Args[1].Op != OpAMD64SHLLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64SHLLconst {
 			break
 		}
-		if v.Args[1].AuxInt != 24 {
+		if v_1.AuxInt != 24 {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpAMD64MOVBQZXload {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		if v.Args[1].Args[0].AuxInt != i+3 {
+		if v_1_0.AuxInt != i+3 {
 			break
 		}
-		if v.Args[1].Args[0].Aux != s {
+		if v_1_0.Aux != s {
 			break
 		}
-		if v.Args[1].Args[0].Args[0] != p {
+		if p != v_1_0.Args[0] {
 			break
 		}
-		if v.Args[1].Args[0].Args[1] != mem {
+		if mem != v_1_0.Args[1] {
 			break
 		}
-		b = v.Args[0].Args[0].Args[0].Block
+		b = x.Block
 		v0 := b.NewValue0(v.Line, OpAMD64MOVLload, config.fe.TypeUInt32())
 		v.reset(OpCopy)
 		v.AddArg(v0)
@@ -10058,10 +10333,11 @@ func rewriteValueAMD64_OpAMD64ORLconst(v *Value, config *Config) bool {
 	// result: (MOVLconst [c|d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVLconst)
 		v.AuxInt = c | d
 		return true
@@ -10076,10 +10352,11 @@ func rewriteValueAMD64_OpAMD64ORQ(v *Value, config *Config) bool {
 	// result: (ORQconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(is32Bit(c)) {
 			break
 		}
@@ -10092,10 +10369,11 @@ func rewriteValueAMD64_OpAMD64ORQ(v *Value, config *Config) bool {
 	// cond: is32Bit(c)
 	// result: (ORQconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		if !(is32Bit(c)) {
 			break
@@ -10110,7 +10388,7 @@ func rewriteValueAMD64_OpAMD64ORQ(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -10118,183 +10396,204 @@ func rewriteValueAMD64_OpAMD64ORQ(v *Value, config *Config) bool {
 		v.AddArg(x)
 		return true
 	}
-	// match: (ORQ (ORQ (ORQ (ORQ (ORQ (ORQ (ORQ                     (MOVBQZXload [i]   {s} p mem)     (SHLQconst [8]  (MOVBQZXload [i+1] {s} p mem)))     (SHLQconst [16] (MOVBQZXload [i+2] {s} p mem)))     (SHLQconst [24] (MOVBQZXload [i+3] {s} p mem)))     (SHLQconst [32] (MOVBQZXload [i+4] {s} p mem)))     (SHLQconst [40] (MOVBQZXload [i+5] {s} p mem)))     (SHLQconst [48] (MOVBQZXload [i+6] {s} p mem)))     (SHLQconst [56] (MOVBQZXload [i+7] {s} p mem)))
+	// match: (ORQ (ORQ (ORQ (ORQ (ORQ (ORQ (ORQ                   x:(MOVBQZXload [i]   {s} p mem)     (SHLQconst [8]  (MOVBQZXload [i+1] {s} p mem)))     (SHLQconst [16] (MOVBQZXload [i+2] {s} p mem)))     (SHLQconst [24] (MOVBQZXload [i+3] {s} p mem)))     (SHLQconst [32] (MOVBQZXload [i+4] {s} p mem)))     (SHLQconst [40] (MOVBQZXload [i+5] {s} p mem)))     (SHLQconst [48] (MOVBQZXload [i+6] {s} p mem)))     (SHLQconst [56] (MOVBQZXload [i+7] {s} p mem)))
 	// cond:
-	// result: @v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Block (MOVQload [i] {s} p mem)
+	// result: @x.Block (MOVQload [i] {s} p mem)
 	for {
-		if v.Args[0].Op != OpAMD64ORQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64ORQ {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpAMD64ORQ {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpAMD64ORQ {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Op != OpAMD64ORQ {
+		v_0_0_0 := v_0_0.Args[0]
+		if v_0_0_0.Op != OpAMD64ORQ {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Op != OpAMD64ORQ {
+		v_0_0_0_0 := v_0_0_0.Args[0]
+		if v_0_0_0_0.Op != OpAMD64ORQ {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Op != OpAMD64ORQ {
+		v_0_0_0_0_0 := v_0_0_0_0.Args[0]
+		if v_0_0_0_0_0.Op != OpAMD64ORQ {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Op != OpAMD64ORQ {
+		v_0_0_0_0_0_0 := v_0_0_0_0_0.Args[0]
+		if v_0_0_0_0_0_0.Op != OpAMD64ORQ {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Op != OpAMD64MOVBQZXload {
+		x := v_0_0_0_0_0_0.Args[0]
+		if x.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		i := v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].AuxInt
-		s := v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Aux
-		p := v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[0]
-		mem := v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[1]
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].Op != OpAMD64SHLQconst {
+		i := x.AuxInt
+		s := x.Aux
+		p := x.Args[0]
+		mem := x.Args[1]
+		v_0_0_0_0_0_0_1 := v_0_0_0_0_0_0.Args[1]
+		if v_0_0_0_0_0_0_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].AuxInt != 8 {
+		if v_0_0_0_0_0_0_1.AuxInt != 8 {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].Op != OpAMD64MOVBQZXload {
+		v_0_0_0_0_0_0_1_0 := v_0_0_0_0_0_0_1.Args[0]
+		if v_0_0_0_0_0_0_1_0.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].AuxInt != i+1 {
+		if v_0_0_0_0_0_0_1_0.AuxInt != i+1 {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].Aux != s {
+		if v_0_0_0_0_0_0_1_0.Aux != s {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].Args[0] != p {
+		if p != v_0_0_0_0_0_0_1_0.Args[0] {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].Args[1] != mem {
+		if mem != v_0_0_0_0_0_0_1_0.Args[1] {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].Op != OpAMD64SHLQconst {
+		v_0_0_0_0_0_1 := v_0_0_0_0_0.Args[1]
+		if v_0_0_0_0_0_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].AuxInt != 16 {
+		if v_0_0_0_0_0_1.AuxInt != 16 {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].Op != OpAMD64MOVBQZXload {
+		v_0_0_0_0_0_1_0 := v_0_0_0_0_0_1.Args[0]
+		if v_0_0_0_0_0_1_0.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].AuxInt != i+2 {
+		if v_0_0_0_0_0_1_0.AuxInt != i+2 {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].Aux != s {
+		if v_0_0_0_0_0_1_0.Aux != s {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].Args[0] != p {
+		if p != v_0_0_0_0_0_1_0.Args[0] {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].Args[1] != mem {
+		if mem != v_0_0_0_0_0_1_0.Args[1] {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[1].Op != OpAMD64SHLQconst {
+		v_0_0_0_0_1 := v_0_0_0_0.Args[1]
+		if v_0_0_0_0_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[1].AuxInt != 24 {
+		if v_0_0_0_0_1.AuxInt != 24 {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].Op != OpAMD64MOVBQZXload {
+		v_0_0_0_0_1_0 := v_0_0_0_0_1.Args[0]
+		if v_0_0_0_0_1_0.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].AuxInt != i+3 {
+		if v_0_0_0_0_1_0.AuxInt != i+3 {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].Aux != s {
+		if v_0_0_0_0_1_0.Aux != s {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].Args[0] != p {
+		if p != v_0_0_0_0_1_0.Args[0] {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[0].Args[1].Args[0].Args[1] != mem {
+		if mem != v_0_0_0_0_1_0.Args[1] {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[1].Op != OpAMD64SHLQconst {
+		v_0_0_0_1 := v_0_0_0.Args[1]
+		if v_0_0_0_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[1].AuxInt != 32 {
+		if v_0_0_0_1.AuxInt != 32 {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[1].Args[0].Op != OpAMD64MOVBQZXload {
+		v_0_0_0_1_0 := v_0_0_0_1.Args[0]
+		if v_0_0_0_1_0.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[1].Args[0].AuxInt != i+4 {
+		if v_0_0_0_1_0.AuxInt != i+4 {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[1].Args[0].Aux != s {
+		if v_0_0_0_1_0.Aux != s {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[1].Args[0].Args[0] != p {
+		if p != v_0_0_0_1_0.Args[0] {
 			break
 		}
-		if v.Args[0].Args[0].Args[0].Args[1].Args[0].Args[1] != mem {
+		if mem != v_0_0_0_1_0.Args[1] {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].Op != OpAMD64SHLQconst {
+		v_0_0_1 := v_0_0.Args[1]
+		if v_0_0_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].AuxInt != 40 {
+		if v_0_0_1.AuxInt != 40 {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].Args[0].Op != OpAMD64MOVBQZXload {
+		v_0_0_1_0 := v_0_0_1.Args[0]
+		if v_0_0_1_0.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].Args[0].AuxInt != i+5 {
+		if v_0_0_1_0.AuxInt != i+5 {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].Args[0].Aux != s {
+		if v_0_0_1_0.Aux != s {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].Args[0].Args[0] != p {
+		if p != v_0_0_1_0.Args[0] {
 			break
 		}
-		if v.Args[0].Args[0].Args[1].Args[0].Args[1] != mem {
+		if mem != v_0_0_1_0.Args[1] {
 			break
 		}
-		if v.Args[0].Args[1].Op != OpAMD64SHLQconst {
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[0].Args[1].AuxInt != 48 {
+		if v_0_1.AuxInt != 48 {
 			break
 		}
-		if v.Args[0].Args[1].Args[0].Op != OpAMD64MOVBQZXload {
+		v_0_1_0 := v_0_1.Args[0]
+		if v_0_1_0.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		if v.Args[0].Args[1].Args[0].AuxInt != i+6 {
+		if v_0_1_0.AuxInt != i+6 {
 			break
 		}
-		if v.Args[0].Args[1].Args[0].Aux != s {
+		if v_0_1_0.Aux != s {
 			break
 		}
-		if v.Args[0].Args[1].Args[0].Args[0] != p {
+		if p != v_0_1_0.Args[0] {
 			break
 		}
-		if v.Args[0].Args[1].Args[0].Args[1] != mem {
+		if mem != v_0_1_0.Args[1] {
 			break
 		}
-		if v.Args[1].Op != OpAMD64SHLQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64SHLQconst {
 			break
 		}
-		if v.Args[1].AuxInt != 56 {
+		if v_1.AuxInt != 56 {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpAMD64MOVBQZXload {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		if v.Args[1].Args[0].AuxInt != i+7 {
+		if v_1_0.AuxInt != i+7 {
 			break
 		}
-		if v.Args[1].Args[0].Aux != s {
+		if v_1_0.Aux != s {
 			break
 		}
-		if v.Args[1].Args[0].Args[0] != p {
+		if p != v_1_0.Args[0] {
 			break
 		}
-		if v.Args[1].Args[0].Args[1] != mem {
+		if mem != v_1_0.Args[1] {
 			break
 		}
-		b = v.Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Args[0].Block
+		b = x.Block
 		v0 := b.NewValue0(v.Line, OpAMD64MOVQload, config.fe.TypeUInt64())
 		v.reset(OpCopy)
 		v.AddArg(v0)
@@ -10338,10 +10637,11 @@ func rewriteValueAMD64_OpAMD64ORQconst(v *Value, config *Config) bool {
 	// result: (MOVQconst [c|d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVQconst)
 		v.AuxInt = c | d
 		return true
@@ -10356,10 +10656,11 @@ func rewriteValueAMD64_OpAMD64ORW(v *Value, config *Config) bool {
 	// result: (ORWconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64ORWconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -10369,10 +10670,11 @@ func rewriteValueAMD64_OpAMD64ORW(v *Value, config *Config) bool {
 	// cond:
 	// result: (ORWconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64ORWconst)
 		v.AuxInt = c
@@ -10384,7 +10686,7 @@ func rewriteValueAMD64_OpAMD64ORW(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -10392,39 +10694,42 @@ func rewriteValueAMD64_OpAMD64ORW(v *Value, config *Config) bool {
 		v.AddArg(x)
 		return true
 	}
-	// match: (ORW                (MOVBQZXload [i]   {s} p mem)     (SHLWconst [8]  (MOVBQZXload [i+1] {s} p mem)))
+	// match: (ORW              x:(MOVBQZXload [i]   {s} p mem)     (SHLWconst [8]  (MOVBQZXload [i+1] {s} p mem)))
 	// cond:
-	// result: @v.Args[0].Block (MOVWload [i] {s} p mem)
+	// result: @x.Block (MOVWload [i] {s} p mem)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBQZXload {
+		x := v.Args[0]
+		if x.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		i := v.Args[0].AuxInt
-		s := v.Args[0].Aux
-		p := v.Args[0].Args[0]
-		mem := v.Args[0].Args[1]
-		if v.Args[1].Op != OpAMD64SHLWconst {
+		i := x.AuxInt
+		s := x.Aux
+		p := x.Args[0]
+		mem := x.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64SHLWconst {
 			break
 		}
-		if v.Args[1].AuxInt != 8 {
+		if v_1.AuxInt != 8 {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpAMD64MOVBQZXload {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpAMD64MOVBQZXload {
 			break
 		}
-		if v.Args[1].Args[0].AuxInt != i+1 {
+		if v_1_0.AuxInt != i+1 {
 			break
 		}
-		if v.Args[1].Args[0].Aux != s {
+		if v_1_0.Aux != s {
 			break
 		}
-		if v.Args[1].Args[0].Args[0] != p {
+		if p != v_1_0.Args[0] {
 			break
 		}
-		if v.Args[1].Args[0].Args[1] != mem {
+		if mem != v_1_0.Args[1] {
 			break
 		}
-		b = v.Args[0].Block
+		b = x.Block
 		v0 := b.NewValue0(v.Line, OpAMD64MOVWload, config.fe.TypeUInt16())
 		v.reset(OpCopy)
 		v.AddArg(v0)
@@ -10470,10 +10775,11 @@ func rewriteValueAMD64_OpAMD64ORWconst(v *Value, config *Config) bool {
 	// result: (MOVWconst [c|d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVWconst)
 		v.AuxInt = c | d
 		return true
@@ -11432,10 +11738,11 @@ func rewriteValueAMD64_OpAMD64SARB(v *Value, config *Config) bool {
 	// result: (SARBconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARBconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -11446,10 +11753,11 @@ func rewriteValueAMD64_OpAMD64SARB(v *Value, config *Config) bool {
 	// result: (SARBconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARBconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -11460,10 +11768,11 @@ func rewriteValueAMD64_OpAMD64SARB(v *Value, config *Config) bool {
 	// result: (SARBconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARBconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -11474,10 +11783,11 @@ func rewriteValueAMD64_OpAMD64SARB(v *Value, config *Config) bool {
 	// result: (SARBconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARBconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -11493,10 +11803,11 @@ func rewriteValueAMD64_OpAMD64SARBconst(v *Value, config *Config) bool {
 	// result: (MOVQconst [d>>uint64(c)])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVQconst)
 		v.AuxInt = d >> uint64(c)
 		return true
@@ -11511,10 +11822,11 @@ func rewriteValueAMD64_OpAMD64SARL(v *Value, config *Config) bool {
 	// result: (SARLconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARLconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -11525,10 +11837,11 @@ func rewriteValueAMD64_OpAMD64SARL(v *Value, config *Config) bool {
 	// result: (SARLconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARLconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -11539,10 +11852,11 @@ func rewriteValueAMD64_OpAMD64SARL(v *Value, config *Config) bool {
 	// result: (SARLconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARLconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -11553,10 +11867,11 @@ func rewriteValueAMD64_OpAMD64SARL(v *Value, config *Config) bool {
 	// result: (SARLconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARLconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -11572,10 +11887,11 @@ func rewriteValueAMD64_OpAMD64SARLconst(v *Value, config *Config) bool {
 	// result: (MOVQconst [d>>uint64(c)])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVQconst)
 		v.AuxInt = d >> uint64(c)
 		return true
@@ -11590,10 +11906,11 @@ func rewriteValueAMD64_OpAMD64SARQ(v *Value, config *Config) bool {
 	// result: (SARQconst [c&63] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARQconst)
 		v.AuxInt = c & 63
 		v.AddArg(x)
@@ -11604,10 +11921,11 @@ func rewriteValueAMD64_OpAMD64SARQ(v *Value, config *Config) bool {
 	// result: (SARQconst [c&63] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARQconst)
 		v.AuxInt = c & 63
 		v.AddArg(x)
@@ -11618,10 +11936,11 @@ func rewriteValueAMD64_OpAMD64SARQ(v *Value, config *Config) bool {
 	// result: (SARQconst [c&63] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARQconst)
 		v.AuxInt = c & 63
 		v.AddArg(x)
@@ -11632,10 +11951,11 @@ func rewriteValueAMD64_OpAMD64SARQ(v *Value, config *Config) bool {
 	// result: (SARQconst [c&63] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARQconst)
 		v.AuxInt = c & 63
 		v.AddArg(x)
@@ -11651,10 +11971,11 @@ func rewriteValueAMD64_OpAMD64SARQconst(v *Value, config *Config) bool {
 	// result: (MOVQconst [d>>uint64(c)])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVQconst)
 		v.AuxInt = d >> uint64(c)
 		return true
@@ -11669,10 +11990,11 @@ func rewriteValueAMD64_OpAMD64SARW(v *Value, config *Config) bool {
 	// result: (SARWconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARWconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -11683,10 +12005,11 @@ func rewriteValueAMD64_OpAMD64SARW(v *Value, config *Config) bool {
 	// result: (SARWconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARWconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -11697,10 +12020,11 @@ func rewriteValueAMD64_OpAMD64SARW(v *Value, config *Config) bool {
 	// result: (SARWconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARWconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -11711,10 +12035,11 @@ func rewriteValueAMD64_OpAMD64SARW(v *Value, config *Config) bool {
 	// result: (SARWconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SARWconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -11730,10 +12055,11 @@ func rewriteValueAMD64_OpAMD64SARWconst(v *Value, config *Config) bool {
 	// result: (MOVQconst [d>>uint64(c)])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVQconst)
 		v.AuxInt = d >> uint64(c)
 		return true
@@ -11747,7 +12073,8 @@ func rewriteValueAMD64_OpAMD64SBBLcarrymask(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVLconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagEQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagEQ {
 			break
 		}
 		v.reset(OpAMD64MOVLconst)
@@ -11758,7 +12085,8 @@ func rewriteValueAMD64_OpAMD64SBBLcarrymask(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVLconst [-1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVLconst)
@@ -11769,7 +12097,8 @@ func rewriteValueAMD64_OpAMD64SBBLcarrymask(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVLconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVLconst)
@@ -11780,7 +12109,8 @@ func rewriteValueAMD64_OpAMD64SBBLcarrymask(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVLconst [-1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVLconst)
@@ -11791,7 +12121,8 @@ func rewriteValueAMD64_OpAMD64SBBLcarrymask(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVLconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVLconst)
@@ -11807,7 +12138,8 @@ func rewriteValueAMD64_OpAMD64SBBQcarrymask(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVQconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagEQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagEQ {
 			break
 		}
 		v.reset(OpAMD64MOVQconst)
@@ -11818,7 +12150,8 @@ func rewriteValueAMD64_OpAMD64SBBQcarrymask(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVQconst [-1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVQconst)
@@ -11829,7 +12162,8 @@ func rewriteValueAMD64_OpAMD64SBBQcarrymask(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVQconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVQconst)
@@ -11840,7 +12174,8 @@ func rewriteValueAMD64_OpAMD64SBBQcarrymask(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVQconst [-1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVQconst)
@@ -11851,7 +12186,8 @@ func rewriteValueAMD64_OpAMD64SBBQcarrymask(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVQconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVQconst)
@@ -11867,10 +12203,11 @@ func rewriteValueAMD64_OpAMD64SETA(v *Value, config *Config) bool {
 	// cond:
 	// result: (SETB x)
 	for {
-		if v.Args[0].Op != OpAMD64InvertFlags {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64InvertFlags {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpAMD64SETB)
 		v.AddArg(x)
 		return true
@@ -11879,7 +12216,8 @@ func rewriteValueAMD64_OpAMD64SETA(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagEQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagEQ {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -11890,7 +12228,8 @@ func rewriteValueAMD64_OpAMD64SETA(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -11901,7 +12240,8 @@ func rewriteValueAMD64_OpAMD64SETA(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -11912,7 +12252,8 @@ func rewriteValueAMD64_OpAMD64SETA(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -11923,7 +12264,8 @@ func rewriteValueAMD64_OpAMD64SETA(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -11939,10 +12281,11 @@ func rewriteValueAMD64_OpAMD64SETAE(v *Value, config *Config) bool {
 	// cond:
 	// result: (SETBE x)
 	for {
-		if v.Args[0].Op != OpAMD64InvertFlags {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64InvertFlags {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpAMD64SETBE)
 		v.AddArg(x)
 		return true
@@ -11951,7 +12294,8 @@ func rewriteValueAMD64_OpAMD64SETAE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagEQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagEQ {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -11962,7 +12306,8 @@ func rewriteValueAMD64_OpAMD64SETAE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -11973,7 +12318,8 @@ func rewriteValueAMD64_OpAMD64SETAE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -11984,7 +12330,8 @@ func rewriteValueAMD64_OpAMD64SETAE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -11995,7 +12342,8 @@ func rewriteValueAMD64_OpAMD64SETAE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12011,10 +12359,11 @@ func rewriteValueAMD64_OpAMD64SETB(v *Value, config *Config) bool {
 	// cond:
 	// result: (SETA x)
 	for {
-		if v.Args[0].Op != OpAMD64InvertFlags {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64InvertFlags {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpAMD64SETA)
 		v.AddArg(x)
 		return true
@@ -12023,7 +12372,8 @@ func rewriteValueAMD64_OpAMD64SETB(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagEQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagEQ {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12034,7 +12384,8 @@ func rewriteValueAMD64_OpAMD64SETB(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12045,7 +12396,8 @@ func rewriteValueAMD64_OpAMD64SETB(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12056,7 +12408,8 @@ func rewriteValueAMD64_OpAMD64SETB(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12067,7 +12420,8 @@ func rewriteValueAMD64_OpAMD64SETB(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12083,10 +12437,11 @@ func rewriteValueAMD64_OpAMD64SETBE(v *Value, config *Config) bool {
 	// cond:
 	// result: (SETAE x)
 	for {
-		if v.Args[0].Op != OpAMD64InvertFlags {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64InvertFlags {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpAMD64SETAE)
 		v.AddArg(x)
 		return true
@@ -12095,7 +12450,8 @@ func rewriteValueAMD64_OpAMD64SETBE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagEQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagEQ {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12106,7 +12462,8 @@ func rewriteValueAMD64_OpAMD64SETBE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12117,7 +12474,8 @@ func rewriteValueAMD64_OpAMD64SETBE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12128,7 +12486,8 @@ func rewriteValueAMD64_OpAMD64SETBE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12139,7 +12498,8 @@ func rewriteValueAMD64_OpAMD64SETBE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12155,10 +12515,11 @@ func rewriteValueAMD64_OpAMD64SETEQ(v *Value, config *Config) bool {
 	// cond:
 	// result: (SETEQ x)
 	for {
-		if v.Args[0].Op != OpAMD64InvertFlags {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64InvertFlags {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpAMD64SETEQ)
 		v.AddArg(x)
 		return true
@@ -12167,7 +12528,8 @@ func rewriteValueAMD64_OpAMD64SETEQ(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagEQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagEQ {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12178,7 +12540,8 @@ func rewriteValueAMD64_OpAMD64SETEQ(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12189,7 +12552,8 @@ func rewriteValueAMD64_OpAMD64SETEQ(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12200,7 +12564,8 @@ func rewriteValueAMD64_OpAMD64SETEQ(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12211,7 +12576,8 @@ func rewriteValueAMD64_OpAMD64SETEQ(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12227,10 +12593,11 @@ func rewriteValueAMD64_OpAMD64SETG(v *Value, config *Config) bool {
 	// cond:
 	// result: (SETL x)
 	for {
-		if v.Args[0].Op != OpAMD64InvertFlags {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64InvertFlags {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpAMD64SETL)
 		v.AddArg(x)
 		return true
@@ -12239,7 +12606,8 @@ func rewriteValueAMD64_OpAMD64SETG(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagEQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagEQ {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12250,7 +12618,8 @@ func rewriteValueAMD64_OpAMD64SETG(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12261,7 +12630,8 @@ func rewriteValueAMD64_OpAMD64SETG(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12272,7 +12642,8 @@ func rewriteValueAMD64_OpAMD64SETG(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12283,7 +12654,8 @@ func rewriteValueAMD64_OpAMD64SETG(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12299,10 +12671,11 @@ func rewriteValueAMD64_OpAMD64SETGE(v *Value, config *Config) bool {
 	// cond:
 	// result: (SETLE x)
 	for {
-		if v.Args[0].Op != OpAMD64InvertFlags {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64InvertFlags {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpAMD64SETLE)
 		v.AddArg(x)
 		return true
@@ -12311,7 +12684,8 @@ func rewriteValueAMD64_OpAMD64SETGE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagEQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagEQ {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12322,7 +12696,8 @@ func rewriteValueAMD64_OpAMD64SETGE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12333,7 +12708,8 @@ func rewriteValueAMD64_OpAMD64SETGE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12344,7 +12720,8 @@ func rewriteValueAMD64_OpAMD64SETGE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12355,7 +12732,8 @@ func rewriteValueAMD64_OpAMD64SETGE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12371,10 +12749,11 @@ func rewriteValueAMD64_OpAMD64SETL(v *Value, config *Config) bool {
 	// cond:
 	// result: (SETG x)
 	for {
-		if v.Args[0].Op != OpAMD64InvertFlags {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64InvertFlags {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpAMD64SETG)
 		v.AddArg(x)
 		return true
@@ -12383,7 +12762,8 @@ func rewriteValueAMD64_OpAMD64SETL(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagEQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagEQ {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12394,7 +12774,8 @@ func rewriteValueAMD64_OpAMD64SETL(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12405,7 +12786,8 @@ func rewriteValueAMD64_OpAMD64SETL(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12416,7 +12798,8 @@ func rewriteValueAMD64_OpAMD64SETL(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12427,7 +12810,8 @@ func rewriteValueAMD64_OpAMD64SETL(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12443,10 +12827,11 @@ func rewriteValueAMD64_OpAMD64SETLE(v *Value, config *Config) bool {
 	// cond:
 	// result: (SETGE x)
 	for {
-		if v.Args[0].Op != OpAMD64InvertFlags {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64InvertFlags {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpAMD64SETGE)
 		v.AddArg(x)
 		return true
@@ -12455,7 +12840,8 @@ func rewriteValueAMD64_OpAMD64SETLE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagEQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagEQ {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12466,7 +12852,8 @@ func rewriteValueAMD64_OpAMD64SETLE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12477,7 +12864,8 @@ func rewriteValueAMD64_OpAMD64SETLE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12488,7 +12876,8 @@ func rewriteValueAMD64_OpAMD64SETLE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12499,7 +12888,8 @@ func rewriteValueAMD64_OpAMD64SETLE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12515,10 +12905,11 @@ func rewriteValueAMD64_OpAMD64SETNE(v *Value, config *Config) bool {
 	// cond:
 	// result: (SETNE x)
 	for {
-		if v.Args[0].Op != OpAMD64InvertFlags {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64InvertFlags {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpAMD64SETNE)
 		v.AddArg(x)
 		return true
@@ -12527,7 +12918,8 @@ func rewriteValueAMD64_OpAMD64SETNE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [0])
 	for {
-		if v.Args[0].Op != OpAMD64FlagEQ {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagEQ {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12538,7 +12930,8 @@ func rewriteValueAMD64_OpAMD64SETNE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12549,7 +12942,8 @@ func rewriteValueAMD64_OpAMD64SETNE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagLT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagLT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12560,7 +12954,8 @@ func rewriteValueAMD64_OpAMD64SETNE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_ULT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_ULT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12571,7 +12966,8 @@ func rewriteValueAMD64_OpAMD64SETNE(v *Value, config *Config) bool {
 	// cond:
 	// result: (MOVBconst [1])
 	for {
-		if v.Args[0].Op != OpAMD64FlagGT_UGT {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64FlagGT_UGT {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -12588,10 +12984,11 @@ func rewriteValueAMD64_OpAMD64SHLB(v *Value, config *Config) bool {
 	// result: (SHLBconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLBconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12602,10 +12999,11 @@ func rewriteValueAMD64_OpAMD64SHLB(v *Value, config *Config) bool {
 	// result: (SHLBconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLBconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12616,10 +13014,11 @@ func rewriteValueAMD64_OpAMD64SHLB(v *Value, config *Config) bool {
 	// result: (SHLBconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLBconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12630,10 +13029,11 @@ func rewriteValueAMD64_OpAMD64SHLB(v *Value, config *Config) bool {
 	// result: (SHLBconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLBconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12649,10 +13049,11 @@ func rewriteValueAMD64_OpAMD64SHLL(v *Value, config *Config) bool {
 	// result: (SHLLconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLLconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12663,10 +13064,11 @@ func rewriteValueAMD64_OpAMD64SHLL(v *Value, config *Config) bool {
 	// result: (SHLLconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLLconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12677,10 +13079,11 @@ func rewriteValueAMD64_OpAMD64SHLL(v *Value, config *Config) bool {
 	// result: (SHLLconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLLconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12691,10 +13094,11 @@ func rewriteValueAMD64_OpAMD64SHLL(v *Value, config *Config) bool {
 	// result: (SHLLconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLLconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12710,10 +13114,11 @@ func rewriteValueAMD64_OpAMD64SHLQ(v *Value, config *Config) bool {
 	// result: (SHLQconst [c&63] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLQconst)
 		v.AuxInt = c & 63
 		v.AddArg(x)
@@ -12724,10 +13129,11 @@ func rewriteValueAMD64_OpAMD64SHLQ(v *Value, config *Config) bool {
 	// result: (SHLQconst [c&63] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLQconst)
 		v.AuxInt = c & 63
 		v.AddArg(x)
@@ -12738,10 +13144,11 @@ func rewriteValueAMD64_OpAMD64SHLQ(v *Value, config *Config) bool {
 	// result: (SHLQconst [c&63] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLQconst)
 		v.AuxInt = c & 63
 		v.AddArg(x)
@@ -12752,10 +13159,11 @@ func rewriteValueAMD64_OpAMD64SHLQ(v *Value, config *Config) bool {
 	// result: (SHLQconst [c&63] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLQconst)
 		v.AuxInt = c & 63
 		v.AddArg(x)
@@ -12771,10 +13179,11 @@ func rewriteValueAMD64_OpAMD64SHLW(v *Value, config *Config) bool {
 	// result: (SHLWconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLWconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12785,10 +13194,11 @@ func rewriteValueAMD64_OpAMD64SHLW(v *Value, config *Config) bool {
 	// result: (SHLWconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLWconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12799,10 +13209,11 @@ func rewriteValueAMD64_OpAMD64SHLW(v *Value, config *Config) bool {
 	// result: (SHLWconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLWconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12813,10 +13224,11 @@ func rewriteValueAMD64_OpAMD64SHLW(v *Value, config *Config) bool {
 	// result: (SHLWconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHLWconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12832,10 +13244,11 @@ func rewriteValueAMD64_OpAMD64SHRB(v *Value, config *Config) bool {
 	// result: (SHRBconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRBconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12846,10 +13259,11 @@ func rewriteValueAMD64_OpAMD64SHRB(v *Value, config *Config) bool {
 	// result: (SHRBconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRBconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12860,10 +13274,11 @@ func rewriteValueAMD64_OpAMD64SHRB(v *Value, config *Config) bool {
 	// result: (SHRBconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRBconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12874,10 +13289,11 @@ func rewriteValueAMD64_OpAMD64SHRB(v *Value, config *Config) bool {
 	// result: (SHRBconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRBconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12893,10 +13309,11 @@ func rewriteValueAMD64_OpAMD64SHRL(v *Value, config *Config) bool {
 	// result: (SHRLconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRLconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12907,10 +13324,11 @@ func rewriteValueAMD64_OpAMD64SHRL(v *Value, config *Config) bool {
 	// result: (SHRLconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRLconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12921,10 +13339,11 @@ func rewriteValueAMD64_OpAMD64SHRL(v *Value, config *Config) bool {
 	// result: (SHRLconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRLconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12935,10 +13354,11 @@ func rewriteValueAMD64_OpAMD64SHRL(v *Value, config *Config) bool {
 	// result: (SHRLconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRLconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -12954,10 +13374,11 @@ func rewriteValueAMD64_OpAMD64SHRQ(v *Value, config *Config) bool {
 	// result: (SHRQconst [c&63] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRQconst)
 		v.AuxInt = c & 63
 		v.AddArg(x)
@@ -12968,10 +13389,11 @@ func rewriteValueAMD64_OpAMD64SHRQ(v *Value, config *Config) bool {
 	// result: (SHRQconst [c&63] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRQconst)
 		v.AuxInt = c & 63
 		v.AddArg(x)
@@ -12982,10 +13404,11 @@ func rewriteValueAMD64_OpAMD64SHRQ(v *Value, config *Config) bool {
 	// result: (SHRQconst [c&63] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRQconst)
 		v.AuxInt = c & 63
 		v.AddArg(x)
@@ -12996,10 +13419,11 @@ func rewriteValueAMD64_OpAMD64SHRQ(v *Value, config *Config) bool {
 	// result: (SHRQconst [c&63] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRQconst)
 		v.AuxInt = c & 63
 		v.AddArg(x)
@@ -13015,10 +13439,11 @@ func rewriteValueAMD64_OpAMD64SHRW(v *Value, config *Config) bool {
 	// result: (SHRWconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRWconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -13029,10 +13454,11 @@ func rewriteValueAMD64_OpAMD64SHRW(v *Value, config *Config) bool {
 	// result: (SHRWconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRWconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -13043,10 +13469,11 @@ func rewriteValueAMD64_OpAMD64SHRW(v *Value, config *Config) bool {
 	// result: (SHRWconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRWconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -13057,10 +13484,11 @@ func rewriteValueAMD64_OpAMD64SHRW(v *Value, config *Config) bool {
 	// result: (SHRWconst [c&31] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SHRWconst)
 		v.AuxInt = c & 31
 		v.AddArg(x)
@@ -13076,10 +13504,11 @@ func rewriteValueAMD64_OpAMD64SUBB(v *Value, config *Config) bool {
 	// result: (SUBBconst x [c])
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SUBBconst)
 		v.AddArg(x)
 		v.AuxInt = c
@@ -13089,10 +13518,11 @@ func rewriteValueAMD64_OpAMD64SUBB(v *Value, config *Config) bool {
 	// cond:
 	// result: (NEGB (SUBBconst <v.Type> x [c]))
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64NEGB)
 		v0 := b.NewValue0(v.Line, OpAMD64SUBBconst, v.Type)
@@ -13106,7 +13536,7 @@ func rewriteValueAMD64_OpAMD64SUBB(v *Value, config *Config) bool {
 	// result: (MOVBconst [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -13137,10 +13567,11 @@ func rewriteValueAMD64_OpAMD64SUBBconst(v *Value, config *Config) bool {
 	// result: (MOVBconst [d-c])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVBconst)
 		v.AuxInt = d - c
 		return true
@@ -13150,11 +13581,12 @@ func rewriteValueAMD64_OpAMD64SUBBconst(v *Value, config *Config) bool {
 	// result: (ADDBconst [-c-d] x)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64SUBBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64SUBBconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		v.reset(OpAMD64ADDBconst)
 		v.AuxInt = -c - d
 		v.AddArg(x)
@@ -13170,10 +13602,11 @@ func rewriteValueAMD64_OpAMD64SUBL(v *Value, config *Config) bool {
 	// result: (SUBLconst x [c])
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SUBLconst)
 		v.AddArg(x)
 		v.AuxInt = c
@@ -13183,10 +13616,11 @@ func rewriteValueAMD64_OpAMD64SUBL(v *Value, config *Config) bool {
 	// cond:
 	// result: (NEGL (SUBLconst <v.Type> x [c]))
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64NEGL)
 		v0 := b.NewValue0(v.Line, OpAMD64SUBLconst, v.Type)
@@ -13200,7 +13634,7 @@ func rewriteValueAMD64_OpAMD64SUBL(v *Value, config *Config) bool {
 	// result: (MOVLconst [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpAMD64MOVLconst)
@@ -13231,10 +13665,11 @@ func rewriteValueAMD64_OpAMD64SUBLconst(v *Value, config *Config) bool {
 	// result: (MOVLconst [d-c])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVLconst)
 		v.AuxInt = d - c
 		return true
@@ -13244,11 +13679,12 @@ func rewriteValueAMD64_OpAMD64SUBLconst(v *Value, config *Config) bool {
 	// result: (ADDLconst [-c-d] x)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64SUBLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64SUBLconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		v.reset(OpAMD64ADDLconst)
 		v.AuxInt = -c - d
 		v.AddArg(x)
@@ -13264,10 +13700,11 @@ func rewriteValueAMD64_OpAMD64SUBQ(v *Value, config *Config) bool {
 	// result: (SUBQconst x [c])
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(is32Bit(c)) {
 			break
 		}
@@ -13280,10 +13717,11 @@ func rewriteValueAMD64_OpAMD64SUBQ(v *Value, config *Config) bool {
 	// cond: is32Bit(c)
 	// result: (NEGQ (SUBQconst <v.Type> x [c]))
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		if !(is32Bit(c)) {
 			break
@@ -13300,7 +13738,7 @@ func rewriteValueAMD64_OpAMD64SUBQ(v *Value, config *Config) bool {
 	// result: (MOVQconst [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpAMD64MOVQconst)
@@ -13330,10 +13768,11 @@ func rewriteValueAMD64_OpAMD64SUBQconst(v *Value, config *Config) bool {
 	// result: (MOVQconst [d-c])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVQconst)
 		v.AuxInt = d - c
 		return true
@@ -13343,11 +13782,12 @@ func rewriteValueAMD64_OpAMD64SUBQconst(v *Value, config *Config) bool {
 	// result: (ADDQconst [-c-d] x)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64SUBQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64SUBQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		if !(is32Bit(-c - d)) {
 			break
 		}
@@ -13366,10 +13806,11 @@ func rewriteValueAMD64_OpAMD64SUBW(v *Value, config *Config) bool {
 	// result: (SUBWconst x [c])
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64SUBWconst)
 		v.AddArg(x)
 		v.AuxInt = c
@@ -13379,10 +13820,11 @@ func rewriteValueAMD64_OpAMD64SUBW(v *Value, config *Config) bool {
 	// cond:
 	// result: (NEGW (SUBWconst <v.Type> x [c]))
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64NEGW)
 		v0 := b.NewValue0(v.Line, OpAMD64SUBWconst, v.Type)
@@ -13396,7 +13838,7 @@ func rewriteValueAMD64_OpAMD64SUBW(v *Value, config *Config) bool {
 	// result: (MOVWconst [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpAMD64MOVWconst)
@@ -13427,10 +13869,11 @@ func rewriteValueAMD64_OpAMD64SUBWconst(v *Value, config *Config) bool {
 	// result: (MOVWconst [d-c])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVWconst)
 		v.AuxInt = d - c
 		return true
@@ -13440,11 +13883,12 @@ func rewriteValueAMD64_OpAMD64SUBWconst(v *Value, config *Config) bool {
 	// result: (ADDWconst [-c-d] x)
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64SUBWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64SUBWconst {
 			break
 		}
-		d := v.Args[0].AuxInt
-		x := v.Args[0].Args[0]
+		d := v_0.AuxInt
+		x := v_0.Args[0]
 		v.reset(OpAMD64ADDWconst)
 		v.AuxInt = -c - d
 		v.AddArg(x)
@@ -13885,10 +14329,11 @@ func rewriteValueAMD64_OpAMD64XORB(v *Value, config *Config) bool {
 	// result: (XORBconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVBconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64XORBconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -13898,10 +14343,11 @@ func rewriteValueAMD64_OpAMD64XORB(v *Value, config *Config) bool {
 	// cond:
 	// result: (XORBconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64XORBconst)
 		v.AuxInt = c
@@ -13913,7 +14359,7 @@ func rewriteValueAMD64_OpAMD64XORB(v *Value, config *Config) bool {
 	// result: (MOVBconst [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpAMD64MOVBconst)
@@ -13944,10 +14390,11 @@ func rewriteValueAMD64_OpAMD64XORBconst(v *Value, config *Config) bool {
 	// result: (MOVBconst [c^d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVBconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVBconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVBconst)
 		v.AuxInt = c ^ d
 		return true
@@ -13962,10 +14409,11 @@ func rewriteValueAMD64_OpAMD64XORL(v *Value, config *Config) bool {
 	// result: (XORLconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVLconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64XORLconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -13975,10 +14423,11 @@ func rewriteValueAMD64_OpAMD64XORL(v *Value, config *Config) bool {
 	// cond:
 	// result: (XORLconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64XORLconst)
 		v.AuxInt = c
@@ -13990,7 +14439,7 @@ func rewriteValueAMD64_OpAMD64XORL(v *Value, config *Config) bool {
 	// result: (MOVLconst [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpAMD64MOVLconst)
@@ -14021,10 +14470,11 @@ func rewriteValueAMD64_OpAMD64XORLconst(v *Value, config *Config) bool {
 	// result: (MOVLconst [c^d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVLconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVLconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVLconst)
 		v.AuxInt = c ^ d
 		return true
@@ -14039,10 +14489,11 @@ func rewriteValueAMD64_OpAMD64XORQ(v *Value, config *Config) bool {
 	// result: (XORQconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVQconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(is32Bit(c)) {
 			break
 		}
@@ -14055,10 +14506,11 @@ func rewriteValueAMD64_OpAMD64XORQ(v *Value, config *Config) bool {
 	// cond: is32Bit(c)
 	// result: (XORQconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		if !(is32Bit(c)) {
 			break
@@ -14073,7 +14525,7 @@ func rewriteValueAMD64_OpAMD64XORQ(v *Value, config *Config) bool {
 	// result: (MOVQconst [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpAMD64MOVQconst)
@@ -14103,10 +14555,11 @@ func rewriteValueAMD64_OpAMD64XORQconst(v *Value, config *Config) bool {
 	// result: (MOVQconst [c^d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVQconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVQconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVQconst)
 		v.AuxInt = c ^ d
 		return true
@@ -14121,10 +14574,11 @@ func rewriteValueAMD64_OpAMD64XORW(v *Value, config *Config) bool {
 	// result: (XORWconst [c] x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpAMD64MOVWconst {
+		v_1 := v.Args[1]
+		if v_1.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpAMD64XORWconst)
 		v.AuxInt = c
 		v.AddArg(x)
@@ -14134,10 +14588,11 @@ func rewriteValueAMD64_OpAMD64XORW(v *Value, config *Config) bool {
 	// cond:
 	// result: (XORWconst [c] x)
 	for {
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		x := v.Args[1]
 		v.reset(OpAMD64XORWconst)
 		v.AuxInt = c
@@ -14149,7 +14604,7 @@ func rewriteValueAMD64_OpAMD64XORW(v *Value, config *Config) bool {
 	// result: (MOVWconst [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpAMD64MOVWconst)
@@ -14180,10 +14635,11 @@ func rewriteValueAMD64_OpAMD64XORWconst(v *Value, config *Config) bool {
 	// result: (MOVWconst [c^d])
 	for {
 		c := v.AuxInt
-		if v.Args[0].Op != OpAMD64MOVWconst {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAMD64MOVWconst {
 			break
 		}
-		d := v.Args[0].AuxInt
+		d := v_0.AuxInt
 		v.reset(OpAMD64MOVWconst)
 		v.AuxInt = c ^ d
 		return true
@@ -15201,7 +15657,7 @@ func rewriteBlockAMD64(b *Block) bool {
 		// result: (NE (TESTB cond cond) yes no)
 		for {
 			v := b.Control
-			cond := v
+			cond := b.Control
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64NE
@@ -15423,10 +15879,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETL {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETL {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64LT
@@ -15443,10 +15900,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETLE {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETLE {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64LE
@@ -15463,10 +15921,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETG {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETG {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64GT
@@ -15483,10 +15942,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETGE {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETGE {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64GE
@@ -15503,10 +15963,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETEQ {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETEQ {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64EQ
@@ -15523,10 +15984,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETNE {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETNE {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64NE
@@ -15543,10 +16005,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETB {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETB {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64ULT
@@ -15563,10 +16026,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETBE {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETBE {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64ULE
@@ -15583,10 +16047,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETA {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETA {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64UGT
@@ -15603,10 +16068,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETAE {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETAE {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64UGE
@@ -15623,10 +16089,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETGF {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETGF {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64UGT
@@ -15643,10 +16110,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETGEF {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETGEF {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64UGE
@@ -15663,10 +16131,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETEQF {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETEQF {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64EQF
@@ -15683,10 +16152,11 @@ func rewriteBlockAMD64(b *Block) bool {
 			if v.Op != OpAMD64TESTB {
 				break
 			}
-			if v.Args[0].Op != OpAMD64SETNEF {
+			v_0 := v.Args[0]
+			if v_0.Op != OpAMD64SETNEF {
 				break
 			}
-			cmp := v.Args[0].Args[0]
+			cmp := v_0.Args[0]
 			yes := b.Succs[0]
 			no := b.Succs[1]
 			b.Kind = BlockAMD64NEF
diff --git a/src/cmd/compile/internal/ssa/rewritedec.go b/src/cmd/compile/internal/ssa/rewritedec.go
index e62c789368..c51cd88833 100644
--- a/src/cmd/compile/internal/ssa/rewritedec.go
+++ b/src/cmd/compile/internal/ssa/rewritedec.go
@@ -28,10 +28,11 @@ func rewriteValuedec_OpSliceCap(v *Value, config *Config) bool {
 	// cond:
 	// result: cap
 	for {
-		if v.Args[0].Op != OpSliceMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSliceMake {
 			break
 		}
-		cap := v.Args[0].Args[2]
+		cap := v_0.Args[2]
 		v.reset(OpCopy)
 		v.Type = cap.Type
 		v.AddArg(cap)
@@ -46,10 +47,11 @@ func rewriteValuedec_OpSliceLen(v *Value, config *Config) bool {
 	// cond:
 	// result: len
 	for {
-		if v.Args[0].Op != OpSliceMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSliceMake {
 			break
 		}
-		len := v.Args[0].Args[1]
+		len := v_0.Args[1]
 		v.reset(OpCopy)
 		v.Type = len.Type
 		v.AddArg(len)
@@ -64,10 +66,11 @@ func rewriteValuedec_OpSlicePtr(v *Value, config *Config) bool {
 	// cond:
 	// result: ptr
 	for {
-		if v.Args[0].Op != OpSliceMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSliceMake {
 			break
 		}
-		ptr := v.Args[0].Args[0]
+		ptr := v_0.Args[0]
 		v.reset(OpCopy)
 		v.Type = ptr.Type
 		v.AddArg(ptr)
@@ -82,10 +85,11 @@ func rewriteValuedec_OpStringLen(v *Value, config *Config) bool {
 	// cond:
 	// result: len
 	for {
-		if v.Args[0].Op != OpStringMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStringMake {
 			break
 		}
-		len := v.Args[0].Args[1]
+		len := v_0.Args[1]
 		v.reset(OpCopy)
 		v.Type = len.Type
 		v.AddArg(len)
@@ -100,10 +104,11 @@ func rewriteValuedec_OpStringPtr(v *Value, config *Config) bool {
 	// cond:
 	// result: ptr
 	for {
-		if v.Args[0].Op != OpStringMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStringMake {
 			break
 		}
-		ptr := v.Args[0].Args[0]
+		ptr := v_0.Args[0]
 		v.reset(OpCopy)
 		v.Type = ptr.Type
 		v.AddArg(ptr)
diff --git a/src/cmd/compile/internal/ssa/rewritegeneric.go b/src/cmd/compile/internal/ssa/rewritegeneric.go
index ebc241ef63..4094d862da 100644
--- a/src/cmd/compile/internal/ssa/rewritegeneric.go
+++ b/src/cmd/compile/internal/ssa/rewritegeneric.go
@@ -356,14 +356,16 @@ func rewriteValuegeneric_OpAdd16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [c+d])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst16)
 		v.AuxInt = c + d
 		return true
@@ -373,11 +375,12 @@ func rewriteValuegeneric_OpAdd16(v *Value, config *Config) bool {
 	// result: (Add16 (Const16 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst16) {
 			break
 		}
@@ -392,10 +395,11 @@ func rewriteValuegeneric_OpAdd16(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -413,14 +417,16 @@ func rewriteValuegeneric_OpAdd32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [c+d])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst32)
 		v.AuxInt = c + d
 		return true
@@ -430,11 +436,12 @@ func rewriteValuegeneric_OpAdd32(v *Value, config *Config) bool {
 	// result: (Add32 (Const32 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst32) {
 			break
 		}
@@ -449,10 +456,11 @@ func rewriteValuegeneric_OpAdd32(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -470,14 +478,16 @@ func rewriteValuegeneric_OpAdd32F(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32F [f2i(float64(i2f32(c) + i2f32(d)))])
 	for {
-		if v.Args[0].Op != OpConst32F {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32F {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32F {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32F {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst32F)
 		v.AuxInt = f2i(float64(i2f32(c) + i2f32(d)))
 		return true
@@ -491,14 +501,16 @@ func rewriteValuegeneric_OpAdd64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [c+d])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst64)
 		v.AuxInt = c + d
 		return true
@@ -508,11 +520,12 @@ func rewriteValuegeneric_OpAdd64(v *Value, config *Config) bool {
 	// result: (Add64 (Const64 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst64) {
 			break
 		}
@@ -527,10 +540,11 @@ func rewriteValuegeneric_OpAdd64(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -548,14 +562,16 @@ func rewriteValuegeneric_OpAdd64F(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64F [f2i(i2f(c) + i2f(d))])
 	for {
-		if v.Args[0].Op != OpConst64F {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64F {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64F {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64F {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst64F)
 		v.AuxInt = f2i(i2f(c) + i2f(d))
 		return true
@@ -569,14 +585,16 @@ func rewriteValuegeneric_OpAdd8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8 [c+d])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst8)
 		v.AuxInt = c + d
 		return true
@@ -586,11 +604,12 @@ func rewriteValuegeneric_OpAdd8(v *Value, config *Config) bool {
 	// result: (Add8 (Const8 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst8) {
 			break
 		}
@@ -605,10 +624,11 @@ func rewriteValuegeneric_OpAdd8(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -628,10 +648,11 @@ func rewriteValuegeneric_OpAddPtr(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpOffPtr)
 		v.Type = t
 		v.AddArg(x)
@@ -648,11 +669,12 @@ func rewriteValuegeneric_OpAnd16(v *Value, config *Config) bool {
 	// result: (And16 (Const16 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst16) {
 			break
 		}
@@ -668,7 +690,7 @@ func rewriteValuegeneric_OpAnd16(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -680,10 +702,11 @@ func rewriteValuegeneric_OpAnd16(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		if v.Args[0].AuxInt != -1 {
+		if v_0.AuxInt != -1 {
 			break
 		}
 		x := v.Args[1]
@@ -696,10 +719,11 @@ func rewriteValuegeneric_OpAnd16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [0])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst16)
@@ -716,11 +740,12 @@ func rewriteValuegeneric_OpAnd32(v *Value, config *Config) bool {
 	// result: (And32 (Const32 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst32) {
 			break
 		}
@@ -736,7 +761,7 @@ func rewriteValuegeneric_OpAnd32(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -748,10 +773,11 @@ func rewriteValuegeneric_OpAnd32(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[0].AuxInt != -1 {
+		if v_0.AuxInt != -1 {
 			break
 		}
 		x := v.Args[1]
@@ -764,10 +790,11 @@ func rewriteValuegeneric_OpAnd32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [0])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst32)
@@ -784,11 +811,12 @@ func rewriteValuegeneric_OpAnd64(v *Value, config *Config) bool {
 	// result: (And64 (Const64 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst64) {
 			break
 		}
@@ -804,7 +832,7 @@ func rewriteValuegeneric_OpAnd64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -816,10 +844,11 @@ func rewriteValuegeneric_OpAnd64(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != -1 {
+		if v_0.AuxInt != -1 {
 			break
 		}
 		x := v.Args[1]
@@ -832,10 +861,11 @@ func rewriteValuegeneric_OpAnd64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -847,10 +877,11 @@ func rewriteValuegeneric_OpAnd64(v *Value, config *Config) bool {
 	// result: (Rsh64Ux64 (Lsh64x64 <t> x (Const64 <t> [nlz(y)])) (Const64 <t> [nlz(y)]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		y := v.Args[0].AuxInt
+		y := v_0.AuxInt
 		x := v.Args[1]
 		if !(nlz(y)+nto(y) == 64 && nto(y) >= 32) {
 			break
@@ -872,10 +903,11 @@ func rewriteValuegeneric_OpAnd64(v *Value, config *Config) bool {
 	// result: (Lsh64x64 (Rsh64Ux64 <t> x (Const64 <t> [ntz(y)])) (Const64 <t> [ntz(y)]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		y := v.Args[0].AuxInt
+		y := v_0.AuxInt
 		x := v.Args[1]
 		if !(nlo(y)+ntz(y) == 64 && ntz(y) >= 32) {
 			break
@@ -902,11 +934,12 @@ func rewriteValuegeneric_OpAnd8(v *Value, config *Config) bool {
 	// result: (And8 (Const8 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst8) {
 			break
 		}
@@ -922,7 +955,7 @@ func rewriteValuegeneric_OpAnd8(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -934,10 +967,11 @@ func rewriteValuegeneric_OpAnd8(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		if v.Args[0].AuxInt != -1 {
+		if v_0.AuxInt != -1 {
 			break
 		}
 		x := v.Args[1]
@@ -950,10 +984,11 @@ func rewriteValuegeneric_OpAnd8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8 [0])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst8)
@@ -1177,20 +1212,21 @@ func rewriteValuegeneric_OpArg(v *Value, config *Config) bool {
 func rewriteValuegeneric_OpArrayIndex(v *Value, config *Config) bool {
 	b := v.Block
 	_ = b
-	// match: (ArrayIndex <t> [0] (Load ptr mem))
+	// match: (ArrayIndex <t> [0] x:(Load ptr mem))
 	// cond:
-	// result: @v.Args[0].Block (Load <t> ptr mem)
+	// result: @x.Block (Load <t> ptr mem)
 	for {
 		t := v.Type
 		if v.AuxInt != 0 {
 			break
 		}
-		if v.Args[0].Op != OpLoad {
+		x := v.Args[0]
+		if x.Op != OpLoad {
 			break
 		}
-		ptr := v.Args[0].Args[0]
-		mem := v.Args[0].Args[1]
-		b = v.Args[0].Block
+		ptr := x.Args[0]
+		mem := x.Args[1]
+		b = x.Block
 		v0 := b.NewValue0(v.Line, OpLoad, t)
 		v.reset(OpCopy)
 		v.AddArg(v0)
@@ -1207,10 +1243,11 @@ func rewriteValuegeneric_OpCom16(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpCom16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpCom16 {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -1225,10 +1262,11 @@ func rewriteValuegeneric_OpCom32(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpCom32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpCom32 {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -1243,10 +1281,11 @@ func rewriteValuegeneric_OpCom64(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpCom64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpCom64 {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -1261,10 +1300,11 @@ func rewriteValuegeneric_OpCom8(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpCom8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpCom8 {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -1279,10 +1319,11 @@ func rewriteValuegeneric_OpComplexImag(v *Value, config *Config) bool {
 	// cond:
 	// result: imag
 	for {
-		if v.Args[0].Op != OpComplexMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpComplexMake {
 			break
 		}
-		imag := v.Args[0].Args[1]
+		imag := v_0.Args[1]
 		v.reset(OpCopy)
 		v.Type = imag.Type
 		v.AddArg(imag)
@@ -1297,10 +1338,11 @@ func rewriteValuegeneric_OpComplexReal(v *Value, config *Config) bool {
 	// cond:
 	// result: real
 	for {
-		if v.Args[0].Op != OpComplexMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpComplexMake {
 			break
 		}
-		real := v.Args[0].Args[0]
+		real := v_0.Args[0]
 		v.reset(OpCopy)
 		v.Type = real.Type
 		v.AddArg(real)
@@ -1447,16 +1489,18 @@ func rewriteValuegeneric_OpConvert(v *Value, config *Config) bool {
 	// cond:
 	// result: (Add64 ptr off)
 	for {
-		if v.Args[0].Op != OpAdd64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAdd64 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConvert {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConvert {
 			break
 		}
-		ptr := v.Args[0].Args[0].Args[0]
-		mem := v.Args[0].Args[0].Args[1]
-		off := v.Args[0].Args[1]
-		if v.Args[1] != mem {
+		ptr := v_0_0.Args[0]
+		mem := v_0_0.Args[1]
+		off := v_0.Args[1]
+		if mem != v.Args[1] {
 			break
 		}
 		v.reset(OpAdd64)
@@ -1468,16 +1512,18 @@ func rewriteValuegeneric_OpConvert(v *Value, config *Config) bool {
 	// cond:
 	// result: (Add64 ptr off)
 	for {
-		if v.Args[0].Op != OpAdd64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAdd64 {
 			break
 		}
-		off := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConvert {
+		off := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConvert {
 			break
 		}
-		ptr := v.Args[0].Args[1].Args[0]
-		mem := v.Args[0].Args[1].Args[1]
-		if v.Args[1] != mem {
+		ptr := v_0_1.Args[0]
+		mem := v_0_1.Args[1]
+		if mem != v.Args[1] {
 			break
 		}
 		v.reset(OpAdd64)
@@ -1489,12 +1535,13 @@ func rewriteValuegeneric_OpConvert(v *Value, config *Config) bool {
 	// cond:
 	// result: ptr
 	for {
-		if v.Args[0].Op != OpConvert {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConvert {
 			break
 		}
-		ptr := v.Args[0].Args[0]
-		mem := v.Args[0].Args[1]
-		if v.Args[1] != mem {
+		ptr := v_0.Args[0]
+		mem := v_0.Args[1]
+		if mem != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -1511,10 +1558,11 @@ func rewriteValuegeneric_OpCvt32Fto64F(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64F [c])
 	for {
-		if v.Args[0].Op != OpConst32F {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32F {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpConst64F)
 		v.AuxInt = c
 		return true
@@ -1528,10 +1576,11 @@ func rewriteValuegeneric_OpCvt64Fto32F(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32F [f2i(float64(i2f32(c)))])
 	for {
-		if v.Args[0].Op != OpConst64F {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64F {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpConst32F)
 		v.AuxInt = f2i(float64(i2f32(c)))
 		return true
@@ -1547,10 +1596,11 @@ func rewriteValuegeneric_OpDiv64(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(c > 0 && smagic64ok(c) && smagic64m(c) > 0) {
 			break
 		}
@@ -1581,10 +1631,11 @@ func rewriteValuegeneric_OpDiv64(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(c > 0 && smagic64ok(c) && smagic64m(c) < 0) {
 			break
 		}
@@ -1618,10 +1669,11 @@ func rewriteValuegeneric_OpDiv64(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(c < 0 && smagic64ok(c) && smagic64m(c) > 0) {
 			break
 		}
@@ -1654,10 +1706,11 @@ func rewriteValuegeneric_OpDiv64(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(c < 0 && smagic64ok(c) && smagic64m(c) < 0) {
 			break
 		}
@@ -1698,10 +1751,11 @@ func rewriteValuegeneric_OpDiv64u(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		n := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(isPowerOfTwo(c)) {
 			break
 		}
@@ -1718,10 +1772,11 @@ func rewriteValuegeneric_OpDiv64u(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(umagic64ok(c) && !umagic64a(c)) {
 			break
 		}
@@ -1743,10 +1798,11 @@ func rewriteValuegeneric_OpDiv64u(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(umagic64ok(c) && umagic64a(c)) {
 			break
 		}
@@ -1775,7 +1831,7 @@ func rewriteValuegeneric_OpEq16(v *Value, config *Config) bool {
 	// result: (ConstBool [1])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConstBool)
@@ -1786,22 +1842,25 @@ func rewriteValuegeneric_OpEq16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Eq16 (Const16 <t> [c-d]) x)
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		t := v.Args[0].Type
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpAdd16 {
+		t := v_0.Type
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpAdd16 {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpConst16 {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpConst16 {
 			break
 		}
-		if v.Args[1].Args[0].Type != t {
+		if v_1_0.Type != t {
 			break
 		}
-		d := v.Args[1].Args[0].AuxInt
-		x := v.Args[1].Args[1]
+		d := v_1_0.AuxInt
+		x := v_1.Args[1]
 		v.reset(OpEq16)
 		v0 := b.NewValue0(v.Line, OpConst16, t)
 		v0.AuxInt = c - d
@@ -1814,11 +1873,12 @@ func rewriteValuegeneric_OpEq16(v *Value, config *Config) bool {
 	// result: (Eq16 (Const16 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst16) {
 			break
 		}
@@ -1833,14 +1893,16 @@ func rewriteValuegeneric_OpEq16(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int16(c) == int16(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int16(c) == int16(d))
 		return true
@@ -1855,7 +1917,7 @@ func rewriteValuegeneric_OpEq32(v *Value, config *Config) bool {
 	// result: (ConstBool [1])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConstBool)
@@ -1866,22 +1928,25 @@ func rewriteValuegeneric_OpEq32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Eq32 (Const32 <t> [c-d]) x)
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		t := v.Args[0].Type
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpAdd32 {
+		t := v_0.Type
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpAdd32 {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpConst32 {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[1].Args[0].Type != t {
+		if v_1_0.Type != t {
 			break
 		}
-		d := v.Args[1].Args[0].AuxInt
-		x := v.Args[1].Args[1]
+		d := v_1_0.AuxInt
+		x := v_1.Args[1]
 		v.reset(OpEq32)
 		v0 := b.NewValue0(v.Line, OpConst32, t)
 		v0.AuxInt = c - d
@@ -1894,11 +1959,12 @@ func rewriteValuegeneric_OpEq32(v *Value, config *Config) bool {
 	// result: (Eq32 (Const32 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst32) {
 			break
 		}
@@ -1913,14 +1979,16 @@ func rewriteValuegeneric_OpEq32(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int32(c) == int32(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int32(c) == int32(d))
 		return true
@@ -1935,7 +2003,7 @@ func rewriteValuegeneric_OpEq64(v *Value, config *Config) bool {
 	// result: (ConstBool [1])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConstBool)
@@ -1946,22 +2014,25 @@ func rewriteValuegeneric_OpEq64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Eq64 (Const64 <t> [c-d]) x)
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		t := v.Args[0].Type
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpAdd64 {
+		t := v_0.Type
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpAdd64 {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpConst64 {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].Args[0].Type != t {
+		if v_1_0.Type != t {
 			break
 		}
-		d := v.Args[1].Args[0].AuxInt
-		x := v.Args[1].Args[1]
+		d := v_1_0.AuxInt
+		x := v_1.Args[1]
 		v.reset(OpEq64)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
 		v0.AuxInt = c - d
@@ -1974,11 +2045,12 @@ func rewriteValuegeneric_OpEq64(v *Value, config *Config) bool {
 	// result: (Eq64 (Const64 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst64) {
 			break
 		}
@@ -1993,14 +2065,16 @@ func rewriteValuegeneric_OpEq64(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int64(c) == int64(d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int64(c) == int64(d))
 		return true
@@ -2015,7 +2089,7 @@ func rewriteValuegeneric_OpEq8(v *Value, config *Config) bool {
 	// result: (ConstBool [1])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConstBool)
@@ -2026,14 +2100,16 @@ func rewriteValuegeneric_OpEq8(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i((int8(c) != 0) == (int8(d) != 0))])
 	for {
-		if v.Args[0].Op != OpConstBool {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConstBool {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConstBool {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConstBool {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i((int8(c) != 0) == (int8(d) != 0))
 		return true
@@ -2042,10 +2118,11 @@ func rewriteValuegeneric_OpEq8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Not x)
 	for {
-		if v.Args[0].Op != OpConstBool {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConstBool {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -2057,10 +2134,11 @@ func rewriteValuegeneric_OpEq8(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConstBool {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConstBool {
 			break
 		}
-		if v.Args[0].AuxInt != 1 {
+		if v_0.AuxInt != 1 {
 			break
 		}
 		x := v.Args[1]
@@ -2073,22 +2151,25 @@ func rewriteValuegeneric_OpEq8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Eq8 (Const8 <t> [c-d]) x)
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		t := v.Args[0].Type
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpAdd8 {
+		t := v_0.Type
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpAdd8 {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpConst8 {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpConst8 {
 			break
 		}
-		if v.Args[1].Args[0].Type != t {
+		if v_1_0.Type != t {
 			break
 		}
-		d := v.Args[1].Args[0].AuxInt
-		x := v.Args[1].Args[1]
+		d := v_1_0.AuxInt
+		x := v_1.Args[1]
 		v.reset(OpEq8)
 		v0 := b.NewValue0(v.Line, OpConst8, t)
 		v0.AuxInt = c - d
@@ -2101,11 +2182,12 @@ func rewriteValuegeneric_OpEq8(v *Value, config *Config) bool {
 	// result: (Eq8 (Const8 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst8) {
 			break
 		}
@@ -2121,11 +2203,12 @@ func rewriteValuegeneric_OpEq8(v *Value, config *Config) bool {
 	// result: (Eq8 (ConstBool <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConstBool {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConstBool {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConstBool) {
 			break
 		}
@@ -2140,14 +2223,16 @@ func rewriteValuegeneric_OpEq8(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int8(c)  == int8(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int8(c) == int8(d))
 		return true
@@ -2182,7 +2267,8 @@ func rewriteValuegeneric_OpEqPtr(v *Value, config *Config) bool {
 	// result: (Not (IsNonNil p))
 	for {
 		p := v.Args[0]
-		if v.Args[1].Op != OpConstNil {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConstNil {
 			break
 		}
 		v.reset(OpNot)
@@ -2195,7 +2281,8 @@ func rewriteValuegeneric_OpEqPtr(v *Value, config *Config) bool {
 	// cond:
 	// result: (Not (IsNonNil p))
 	for {
-		if v.Args[0].Op != OpConstNil {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConstNil {
 			break
 		}
 		p := v.Args[1]
@@ -2234,14 +2321,16 @@ func rewriteValuegeneric_OpGeq16(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int16(c) >= int16(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int16(c) >= int16(d))
 		return true
@@ -2255,14 +2344,16 @@ func rewriteValuegeneric_OpGeq16U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint16(c) >= uint16(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint16(c) >= uint16(d))
 		return true
@@ -2276,14 +2367,16 @@ func rewriteValuegeneric_OpGeq32(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int32(c) >= int32(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int32(c) >= int32(d))
 		return true
@@ -2297,14 +2390,16 @@ func rewriteValuegeneric_OpGeq32U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint32(c) >= uint32(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint32(c) >= uint32(d))
 		return true
@@ -2318,14 +2413,16 @@ func rewriteValuegeneric_OpGeq64(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int64(c) >= int64(d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int64(c) >= int64(d))
 		return true
@@ -2339,14 +2436,16 @@ func rewriteValuegeneric_OpGeq64U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint64(c) >= uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint64(c) >= uint64(d))
 		return true
@@ -2360,14 +2459,16 @@ func rewriteValuegeneric_OpGeq8(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int8(c)  >= int8(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int8(c) >= int8(d))
 		return true
@@ -2381,14 +2482,16 @@ func rewriteValuegeneric_OpGeq8U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint8(c)  >= uint8(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint8(c) >= uint8(d))
 		return true
@@ -2402,14 +2505,16 @@ func rewriteValuegeneric_OpGreater16(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int16(c) > int16(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int16(c) > int16(d))
 		return true
@@ -2423,14 +2528,16 @@ func rewriteValuegeneric_OpGreater16U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint16(c) > uint16(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint16(c) > uint16(d))
 		return true
@@ -2444,14 +2551,16 @@ func rewriteValuegeneric_OpGreater32(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int32(c) > int32(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int32(c) > int32(d))
 		return true
@@ -2465,14 +2574,16 @@ func rewriteValuegeneric_OpGreater32U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint32(c) > uint32(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint32(c) > uint32(d))
 		return true
@@ -2486,14 +2597,16 @@ func rewriteValuegeneric_OpGreater64(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int64(c) > int64(d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int64(c) > int64(d))
 		return true
@@ -2507,14 +2620,16 @@ func rewriteValuegeneric_OpGreater64U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint64(c) > uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint64(c) > uint64(d))
 		return true
@@ -2528,14 +2643,16 @@ func rewriteValuegeneric_OpGreater8(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int8(c)  > int8(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int8(c) > int8(d))
 		return true
@@ -2549,14 +2666,16 @@ func rewriteValuegeneric_OpGreater8U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint8(c)  > uint8(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint8(c) > uint8(d))
 		return true
@@ -2570,10 +2689,11 @@ func rewriteValuegeneric_OpIData(v *Value, config *Config) bool {
 	// cond:
 	// result: data
 	for {
-		if v.Args[0].Op != OpIMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpIMake {
 			break
 		}
-		data := v.Args[0].Args[1]
+		data := v_0.Args[1]
 		v.reset(OpCopy)
 		v.Type = data.Type
 		v.AddArg(data)
@@ -2588,10 +2708,11 @@ func rewriteValuegeneric_OpITab(v *Value, config *Config) bool {
 	// cond:
 	// result: itab
 	for {
-		if v.Args[0].Op != OpIMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpIMake {
 			break
 		}
-		itab := v.Args[0].Args[0]
+		itab := v_0.Args[0]
 		v.reset(OpCopy)
 		v.Type = itab.Type
 		v.AddArg(itab)
@@ -2679,7 +2800,7 @@ func rewriteValuegeneric_OpIsInBounds(v *Value, config *Config) bool {
 	// result: (ConstBool [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConstBool)
@@ -2690,17 +2811,20 @@ func rewriteValuegeneric_OpIsInBounds(v *Value, config *Config) bool {
 	// cond: inBounds32(c, d)
 	// result: (ConstBool [1])
 	for {
-		if v.Args[0].Op != OpAnd32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAnd32 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConst32 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(inBounds32(c, d)) {
 			break
 		}
@@ -2712,17 +2836,20 @@ func rewriteValuegeneric_OpIsInBounds(v *Value, config *Config) bool {
 	// cond: inBounds64(c, d)
 	// result: (ConstBool [1])
 	for {
-		if v.Args[0].Op != OpAnd64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAnd64 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConst64 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(inBounds64(c, d)) {
 			break
 		}
@@ -2734,14 +2861,16 @@ func rewriteValuegeneric_OpIsInBounds(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(inBounds32(c,d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(inBounds32(c, d))
 		return true
@@ -2750,14 +2879,16 @@ func rewriteValuegeneric_OpIsInBounds(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(inBounds64(c,d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(inBounds64(c, d))
 		return true
@@ -2772,7 +2903,7 @@ func rewriteValuegeneric_OpIsSliceInBounds(v *Value, config *Config) bool {
 	// result: (ConstBool [1])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConstBool)
@@ -2783,17 +2914,20 @@ func rewriteValuegeneric_OpIsSliceInBounds(v *Value, config *Config) bool {
 	// cond: sliceInBounds32(c, d)
 	// result: (ConstBool [1])
 	for {
-		if v.Args[0].Op != OpAnd32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAnd32 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConst32 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(sliceInBounds32(c, d)) {
 			break
 		}
@@ -2805,17 +2939,20 @@ func rewriteValuegeneric_OpIsSliceInBounds(v *Value, config *Config) bool {
 	// cond: sliceInBounds64(c, d)
 	// result: (ConstBool [1])
 	for {
-		if v.Args[0].Op != OpAnd64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAnd64 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConst64 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(sliceInBounds64(c, d)) {
 			break
 		}
@@ -2827,10 +2964,11 @@ func rewriteValuegeneric_OpIsSliceInBounds(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [1])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConstBool)
@@ -2841,10 +2979,11 @@ func rewriteValuegeneric_OpIsSliceInBounds(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [1])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConstBool)
@@ -2855,14 +2994,16 @@ func rewriteValuegeneric_OpIsSliceInBounds(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(sliceInBounds32(c,d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(sliceInBounds32(c, d))
 		return true
@@ -2871,14 +3012,16 @@ func rewriteValuegeneric_OpIsSliceInBounds(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(sliceInBounds64(c,d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(sliceInBounds64(c, d))
 		return true
@@ -2887,14 +3030,16 @@ func rewriteValuegeneric_OpIsSliceInBounds(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [1])
 	for {
-		if v.Args[0].Op != OpSliceLen {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSliceLen {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[1].Op != OpSliceCap {
+		x := v_0.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpSliceCap {
 			break
 		}
-		if v.Args[1].Args[0] != x {
+		if x != v_1.Args[0] {
 			break
 		}
 		v.reset(OpConstBool)
@@ -2910,14 +3055,16 @@ func rewriteValuegeneric_OpLeq16(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int16(c) <= int16(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int16(c) <= int16(d))
 		return true
@@ -2931,14 +3078,16 @@ func rewriteValuegeneric_OpLeq16U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint16(c) <= uint16(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint16(c) <= uint16(d))
 		return true
@@ -2952,14 +3101,16 @@ func rewriteValuegeneric_OpLeq32(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int32(c) <= int32(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int32(c) <= int32(d))
 		return true
@@ -2973,14 +3124,16 @@ func rewriteValuegeneric_OpLeq32U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint32(c) <= uint32(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint32(c) <= uint32(d))
 		return true
@@ -2994,14 +3147,16 @@ func rewriteValuegeneric_OpLeq64(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int64(c) <= int64(d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int64(c) <= int64(d))
 		return true
@@ -3015,14 +3170,16 @@ func rewriteValuegeneric_OpLeq64U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint64(c) <= uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint64(c) <= uint64(d))
 		return true
@@ -3036,14 +3193,16 @@ func rewriteValuegeneric_OpLeq8(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int8(c)  <= int8(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int8(c) <= int8(d))
 		return true
@@ -3057,14 +3216,16 @@ func rewriteValuegeneric_OpLeq8U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint8(c)  <= uint8(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint8(c) <= uint8(d))
 		return true
@@ -3078,14 +3239,16 @@ func rewriteValuegeneric_OpLess16(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int16(c) < int16(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int16(c) < int16(d))
 		return true
@@ -3099,14 +3262,16 @@ func rewriteValuegeneric_OpLess16U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint16(c) < uint16(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint16(c) < uint16(d))
 		return true
@@ -3120,14 +3285,16 @@ func rewriteValuegeneric_OpLess32(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int32(c) < int32(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int32(c) < int32(d))
 		return true
@@ -3141,14 +3308,16 @@ func rewriteValuegeneric_OpLess32U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint32(c) < uint32(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint32(c) < uint32(d))
 		return true
@@ -3162,14 +3331,16 @@ func rewriteValuegeneric_OpLess64(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int64(c) < int64(d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int64(c) < int64(d))
 		return true
@@ -3183,14 +3354,16 @@ func rewriteValuegeneric_OpLess64U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint64(c) < uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint64(c) < uint64(d))
 		return true
@@ -3204,14 +3377,16 @@ func rewriteValuegeneric_OpLess8(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int8(c)  < int8(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int8(c) < int8(d))
 		return true
@@ -3225,14 +3400,16 @@ func rewriteValuegeneric_OpLess8U(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(uint8(c)  < uint8(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(uint8(c) < uint8(d))
 		return true
@@ -3248,12 +3425,13 @@ func rewriteValuegeneric_OpLoad(v *Value, config *Config) bool {
 	for {
 		t1 := v.Type
 		p1 := v.Args[0]
-		if v.Args[1].Op != OpStore {
+		v_1 := v.Args[1]
+		if v_1.Op != OpStore {
 			break
 		}
-		w := v.Args[1].AuxInt
-		p2 := v.Args[1].Args[0]
-		x := v.Args[1].Args[1]
+		w := v_1.AuxInt
+		p2 := v_1.Args[0]
+		x := v_1.Args[1]
 		if !(isSamePtr(p1, p2) && t1.Compare(x.Type) == CMPeq && w == t1.Size()) {
 			break
 		}
@@ -3519,25 +3697,30 @@ func rewriteValuegeneric_OpLsh16x16(v *Value, config *Config) bool {
 	// cond: c1 >= c2 && c3 >= c2
 	// result: (Lsh16x16 x (Const16 <config.fe.TypeUInt16()> [c1-c2+c3]))
 	for {
-		if v.Args[0].Op != OpRsh16Ux16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpRsh16Ux16 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpLsh16x16 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpLsh16x16 {
 			break
 		}
-		x := v.Args[0].Args[0].Args[0]
-		if v.Args[0].Args[0].Args[1].Op != OpConst16 {
+		x := v_0_0.Args[0]
+		v_0_0_1 := v_0_0.Args[1]
+		if v_0_0_1.Op != OpConst16 {
 			break
 		}
-		c1 := v.Args[0].Args[0].Args[1].AuxInt
-		if v.Args[0].Args[1].Op != OpConst16 {
+		c1 := v_0_0_1.AuxInt
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst16 {
 			break
 		}
-		c2 := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c2 := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c3 := v.Args[1].AuxInt
+		c3 := v_1.AuxInt
 		if !(c1 >= c2 && c3 >= c2) {
 			break
 		}
@@ -3554,10 +3737,11 @@ func rewriteValuegeneric_OpLsh16x16(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpLsh16x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -3576,10 +3760,11 @@ func rewriteValuegeneric_OpLsh16x32(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpLsh16x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -3596,14 +3781,16 @@ func rewriteValuegeneric_OpLsh16x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [int64(int16(c) << uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst16)
 		v.AuxInt = int64(int16(c) << uint64(d))
 		return true
@@ -3612,10 +3799,11 @@ func rewriteValuegeneric_OpLsh16x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [0])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst16)
@@ -3627,10 +3815,11 @@ func rewriteValuegeneric_OpLsh16x64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != 0 {
+		if v_1.AuxInt != 0 {
 			break
 		}
 		v.reset(OpCopy)
@@ -3642,10 +3831,11 @@ func rewriteValuegeneric_OpLsh16x64(v *Value, config *Config) bool {
 	// cond: uint64(c) >= 16
 	// result: (Const16 [0])
 	for {
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(uint64(c) >= 16) {
 			break
 		}
@@ -3658,18 +3848,21 @@ func rewriteValuegeneric_OpLsh16x64(v *Value, config *Config) bool {
 	// result: (Lsh16x64 x (Const64 <t> [c+d]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpLsh16x64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpLsh16x64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConst64 {
+		x := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(!uaddOvf(c, d)) {
 			break
 		}
@@ -3691,10 +3884,11 @@ func rewriteValuegeneric_OpLsh16x8(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpLsh16x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -3713,10 +3907,11 @@ func rewriteValuegeneric_OpLsh32x16(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpLsh32x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -3733,25 +3928,30 @@ func rewriteValuegeneric_OpLsh32x32(v *Value, config *Config) bool {
 	// cond: c1 >= c2 && c3 >= c2
 	// result: (Lsh32x32 x (Const32 <config.fe.TypeUInt32()> [c1-c2+c3]))
 	for {
-		if v.Args[0].Op != OpRsh32Ux32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpRsh32Ux32 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpLsh32x32 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpLsh32x32 {
 			break
 		}
-		x := v.Args[0].Args[0].Args[0]
-		if v.Args[0].Args[0].Args[1].Op != OpConst32 {
+		x := v_0_0.Args[0]
+		v_0_0_1 := v_0_0.Args[1]
+		if v_0_0_1.Op != OpConst32 {
 			break
 		}
-		c1 := v.Args[0].Args[0].Args[1].AuxInt
-		if v.Args[0].Args[1].Op != OpConst32 {
+		c1 := v_0_0_1.AuxInt
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst32 {
 			break
 		}
-		c2 := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c2 := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c3 := v.Args[1].AuxInt
+		c3 := v_1.AuxInt
 		if !(c1 >= c2 && c3 >= c2) {
 			break
 		}
@@ -3768,10 +3968,11 @@ func rewriteValuegeneric_OpLsh32x32(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpLsh32x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -3788,14 +3989,16 @@ func rewriteValuegeneric_OpLsh32x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [int64(int32(c) << uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst32)
 		v.AuxInt = int64(int32(c) << uint64(d))
 		return true
@@ -3804,10 +4007,11 @@ func rewriteValuegeneric_OpLsh32x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [0])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst32)
@@ -3819,10 +4023,11 @@ func rewriteValuegeneric_OpLsh32x64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != 0 {
+		if v_1.AuxInt != 0 {
 			break
 		}
 		v.reset(OpCopy)
@@ -3834,10 +4039,11 @@ func rewriteValuegeneric_OpLsh32x64(v *Value, config *Config) bool {
 	// cond: uint64(c) >= 32
 	// result: (Const32 [0])
 	for {
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(uint64(c) >= 32) {
 			break
 		}
@@ -3850,18 +4056,21 @@ func rewriteValuegeneric_OpLsh32x64(v *Value, config *Config) bool {
 	// result: (Lsh32x64 x (Const64 <t> [c+d]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpLsh32x64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpLsh32x64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConst64 {
+		x := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(!uaddOvf(c, d)) {
 			break
 		}
@@ -3883,10 +4092,11 @@ func rewriteValuegeneric_OpLsh32x8(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpLsh32x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -3905,10 +4115,11 @@ func rewriteValuegeneric_OpLsh64x16(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpLsh64x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -3920,10 +4131,11 @@ func rewriteValuegeneric_OpLsh64x16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -3941,10 +4153,11 @@ func rewriteValuegeneric_OpLsh64x32(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpLsh64x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -3956,10 +4169,11 @@ func rewriteValuegeneric_OpLsh64x32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -3975,14 +4189,16 @@ func rewriteValuegeneric_OpLsh64x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [c << uint64(d)])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst64)
 		v.AuxInt = c << uint64(d)
 		return true
@@ -3991,10 +4207,11 @@ func rewriteValuegeneric_OpLsh64x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -4005,25 +4222,30 @@ func rewriteValuegeneric_OpLsh64x64(v *Value, config *Config) bool {
 	// cond: c1 >= c2 && c3 >= c2
 	// result: (Lsh64x64 x (Const64 <config.fe.TypeUInt64()> [c1-c2+c3]))
 	for {
-		if v.Args[0].Op != OpRsh64Ux64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpRsh64Ux64 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpLsh64x64 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpLsh64x64 {
 			break
 		}
-		x := v.Args[0].Args[0].Args[0]
-		if v.Args[0].Args[0].Args[1].Op != OpConst64 {
+		x := v_0_0.Args[0]
+		v_0_0_1 := v_0_0.Args[1]
+		if v_0_0_1.Op != OpConst64 {
 			break
 		}
-		c1 := v.Args[0].Args[0].Args[1].AuxInt
-		if v.Args[0].Args[1].Op != OpConst64 {
+		c1 := v_0_0_1.AuxInt
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c2 := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c2 := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c3 := v.Args[1].AuxInt
+		c3 := v_1.AuxInt
 		if !(c1 >= c2 && c3 >= c2) {
 			break
 		}
@@ -4039,10 +4261,11 @@ func rewriteValuegeneric_OpLsh64x64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != 0 {
+		if v_1.AuxInt != 0 {
 			break
 		}
 		v.reset(OpCopy)
@@ -4054,10 +4277,11 @@ func rewriteValuegeneric_OpLsh64x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -4068,10 +4292,11 @@ func rewriteValuegeneric_OpLsh64x64(v *Value, config *Config) bool {
 	// cond: uint64(c) >= 64
 	// result: (Const64 [0])
 	for {
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(uint64(c) >= 64) {
 			break
 		}
@@ -4084,18 +4309,21 @@ func rewriteValuegeneric_OpLsh64x64(v *Value, config *Config) bool {
 	// result: (Lsh64x64 x (Const64 <t> [c+d]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpLsh64x64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpLsh64x64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConst64 {
+		x := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(!uaddOvf(c, d)) {
 			break
 		}
@@ -4117,10 +4345,11 @@ func rewriteValuegeneric_OpLsh64x8(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpLsh64x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -4132,10 +4361,11 @@ func rewriteValuegeneric_OpLsh64x8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -4153,10 +4383,11 @@ func rewriteValuegeneric_OpLsh8x16(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpLsh8x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -4175,10 +4406,11 @@ func rewriteValuegeneric_OpLsh8x32(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpLsh8x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -4195,14 +4427,16 @@ func rewriteValuegeneric_OpLsh8x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8  [int64(int8(c) << uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst8)
 		v.AuxInt = int64(int8(c) << uint64(d))
 		return true
@@ -4211,10 +4445,11 @@ func rewriteValuegeneric_OpLsh8x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8  [0])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst8)
@@ -4226,10 +4461,11 @@ func rewriteValuegeneric_OpLsh8x64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != 0 {
+		if v_1.AuxInt != 0 {
 			break
 		}
 		v.reset(OpCopy)
@@ -4241,10 +4477,11 @@ func rewriteValuegeneric_OpLsh8x64(v *Value, config *Config) bool {
 	// cond: uint64(c) >= 8
 	// result: (Const8 [0])
 	for {
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(uint64(c) >= 8) {
 			break
 		}
@@ -4257,18 +4494,21 @@ func rewriteValuegeneric_OpLsh8x64(v *Value, config *Config) bool {
 	// result: (Lsh8x64  x (Const64 <t> [c+d]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpLsh8x64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpLsh8x64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConst64 {
+		x := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(!uaddOvf(c, d)) {
 			break
 		}
@@ -4288,25 +4528,30 @@ func rewriteValuegeneric_OpLsh8x8(v *Value, config *Config) bool {
 	// cond: c1 >= c2 && c3 >= c2
 	// result: (Lsh8x8 x (Const8 <config.fe.TypeUInt8()> [c1-c2+c3]))
 	for {
-		if v.Args[0].Op != OpRsh8Ux8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpRsh8Ux8 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpLsh8x8 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpLsh8x8 {
 			break
 		}
-		x := v.Args[0].Args[0].Args[0]
-		if v.Args[0].Args[0].Args[1].Op != OpConst8 {
+		x := v_0_0.Args[0]
+		v_0_0_1 := v_0_0.Args[1]
+		if v_0_0_1.Op != OpConst8 {
 			break
 		}
-		c1 := v.Args[0].Args[0].Args[1].AuxInt
-		if v.Args[0].Args[1].Op != OpConst8 {
+		c1 := v_0_0_1.AuxInt
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst8 {
 			break
 		}
-		c2 := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c2 := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c3 := v.Args[1].AuxInt
+		c3 := v_1.AuxInt
 		if !(c1 >= c2 && c3 >= c2) {
 			break
 		}
@@ -4323,10 +4568,11 @@ func rewriteValuegeneric_OpLsh8x8(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpLsh8x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -4345,10 +4591,11 @@ func rewriteValuegeneric_OpMod64(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(smagic64ok(c)) {
 			break
 		}
@@ -4378,10 +4625,11 @@ func rewriteValuegeneric_OpMod64u(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		n := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(isPowerOfTwo(c)) {
 			break
 		}
@@ -4398,10 +4646,11 @@ func rewriteValuegeneric_OpMod64u(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(umagic64ok(c)) {
 			break
 		}
@@ -4429,14 +4678,16 @@ func rewriteValuegeneric_OpMul16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [c*d])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst16)
 		v.AuxInt = c * d
 		return true
@@ -4446,11 +4697,12 @@ func rewriteValuegeneric_OpMul16(v *Value, config *Config) bool {
 	// result: (Mul16 (Const16 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst16) {
 			break
 		}
@@ -4465,10 +4717,11 @@ func rewriteValuegeneric_OpMul16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [0])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst16)
@@ -4484,14 +4737,16 @@ func rewriteValuegeneric_OpMul32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [c*d])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst32)
 		v.AuxInt = c * d
 		return true
@@ -4501,11 +4756,12 @@ func rewriteValuegeneric_OpMul32(v *Value, config *Config) bool {
 	// result: (Mul32 (Const32 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst32) {
 			break
 		}
@@ -4520,25 +4776,28 @@ func rewriteValuegeneric_OpMul32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Add32 (Const32 <t> [c*d]) (Mul32 <t> (Const32 <t> [c]) x))
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		t := v.Args[0].Type
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpAdd32 {
+		t := v_0.Type
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpAdd32 {
 			break
 		}
-		if v.Args[1].Type != t {
+		if v_1.Type != t {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpConst32 {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[1].Args[0].Type != t {
+		if v_1_0.Type != t {
 			break
 		}
-		d := v.Args[1].Args[0].AuxInt
-		x := v.Args[1].Args[1]
+		d := v_1_0.AuxInt
+		x := v_1.Args[1]
 		v.reset(OpAdd32)
 		v0 := b.NewValue0(v.Line, OpConst32, t)
 		v0.AuxInt = c * d
@@ -4555,10 +4814,11 @@ func rewriteValuegeneric_OpMul32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [0])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst32)
@@ -4574,14 +4834,16 @@ func rewriteValuegeneric_OpMul32F(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32F [f2i(float64(i2f32(c) * i2f32(d)))])
 	for {
-		if v.Args[0].Op != OpConst32F {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32F {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32F {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32F {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst32F)
 		v.AuxInt = f2i(float64(i2f32(c) * i2f32(d)))
 		return true
@@ -4595,14 +4857,16 @@ func rewriteValuegeneric_OpMul64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [c*d])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst64)
 		v.AuxInt = c * d
 		return true
@@ -4612,11 +4876,12 @@ func rewriteValuegeneric_OpMul64(v *Value, config *Config) bool {
 	// result: (Mul64 (Const64 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst64) {
 			break
 		}
@@ -4631,25 +4896,28 @@ func rewriteValuegeneric_OpMul64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Add64 (Const64 <t> [c*d]) (Mul64 <t> (Const64 <t> [c]) x))
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		t := v.Args[0].Type
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpAdd64 {
+		t := v_0.Type
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpAdd64 {
 			break
 		}
-		if v.Args[1].Type != t {
+		if v_1.Type != t {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpConst64 {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].Args[0].Type != t {
+		if v_1_0.Type != t {
 			break
 		}
-		d := v.Args[1].Args[0].AuxInt
-		x := v.Args[1].Args[1]
+		d := v_1_0.AuxInt
+		x := v_1.Args[1]
 		v.reset(OpAdd64)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
 		v0.AuxInt = c * d
@@ -4666,10 +4934,11 @@ func rewriteValuegeneric_OpMul64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -4685,14 +4954,16 @@ func rewriteValuegeneric_OpMul64F(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64F [f2i(i2f(c) * i2f(d))])
 	for {
-		if v.Args[0].Op != OpConst64F {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64F {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64F {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64F {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst64F)
 		v.AuxInt = f2i(i2f(c) * i2f(d))
 		return true
@@ -4706,14 +4977,16 @@ func rewriteValuegeneric_OpMul8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8 [c*d])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst8)
 		v.AuxInt = c * d
 		return true
@@ -4723,11 +4996,12 @@ func rewriteValuegeneric_OpMul8(v *Value, config *Config) bool {
 	// result: (Mul8 (Const8 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst8) {
 			break
 		}
@@ -4742,10 +5016,11 @@ func rewriteValuegeneric_OpMul8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8 [0])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst8)
@@ -4761,11 +5036,12 @@ func rewriteValuegeneric_OpNeg16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Sub16 y x)
 	for {
-		if v.Args[0].Op != OpSub16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSub16 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		v.reset(OpSub16)
 		v.AddArg(y)
 		v.AddArg(x)
@@ -4780,11 +5056,12 @@ func rewriteValuegeneric_OpNeg32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Sub32 y x)
 	for {
-		if v.Args[0].Op != OpSub32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSub32 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		v.reset(OpSub32)
 		v.AddArg(y)
 		v.AddArg(x)
@@ -4799,11 +5076,12 @@ func rewriteValuegeneric_OpNeg64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Sub64 y x)
 	for {
-		if v.Args[0].Op != OpSub64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSub64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		v.reset(OpSub64)
 		v.AddArg(y)
 		v.AddArg(x)
@@ -4818,11 +5096,12 @@ func rewriteValuegeneric_OpNeg8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Sub8 y x)
 	for {
-		if v.Args[0].Op != OpSub8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSub8 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
+		x := v_0.Args[0]
+		y := v_0.Args[1]
 		v.reset(OpSub8)
 		v.AddArg(y)
 		v.AddArg(x)
@@ -4838,7 +5117,7 @@ func rewriteValuegeneric_OpNeq16(v *Value, config *Config) bool {
 	// result: (ConstBool [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConstBool)
@@ -4849,22 +5128,25 @@ func rewriteValuegeneric_OpNeq16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Neq16 (Const16 <t> [c-d]) x)
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		t := v.Args[0].Type
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpAdd16 {
+		t := v_0.Type
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpAdd16 {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpConst16 {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpConst16 {
 			break
 		}
-		if v.Args[1].Args[0].Type != t {
+		if v_1_0.Type != t {
 			break
 		}
-		d := v.Args[1].Args[0].AuxInt
-		x := v.Args[1].Args[1]
+		d := v_1_0.AuxInt
+		x := v_1.Args[1]
 		v.reset(OpNeq16)
 		v0 := b.NewValue0(v.Line, OpConst16, t)
 		v0.AuxInt = c - d
@@ -4877,11 +5159,12 @@ func rewriteValuegeneric_OpNeq16(v *Value, config *Config) bool {
 	// result: (Neq16 (Const16 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst16) {
 			break
 		}
@@ -4896,14 +5179,16 @@ func rewriteValuegeneric_OpNeq16(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int16(c) != int16(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int16(c) != int16(d))
 		return true
@@ -4918,7 +5203,7 @@ func rewriteValuegeneric_OpNeq32(v *Value, config *Config) bool {
 	// result: (ConstBool [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConstBool)
@@ -4929,22 +5214,25 @@ func rewriteValuegeneric_OpNeq32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Neq32 (Const32 <t> [c-d]) x)
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		t := v.Args[0].Type
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpAdd32 {
+		t := v_0.Type
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpAdd32 {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpConst32 {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[1].Args[0].Type != t {
+		if v_1_0.Type != t {
 			break
 		}
-		d := v.Args[1].Args[0].AuxInt
-		x := v.Args[1].Args[1]
+		d := v_1_0.AuxInt
+		x := v_1.Args[1]
 		v.reset(OpNeq32)
 		v0 := b.NewValue0(v.Line, OpConst32, t)
 		v0.AuxInt = c - d
@@ -4957,11 +5245,12 @@ func rewriteValuegeneric_OpNeq32(v *Value, config *Config) bool {
 	// result: (Neq32 (Const32 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst32) {
 			break
 		}
@@ -4976,14 +5265,16 @@ func rewriteValuegeneric_OpNeq32(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int32(c) != int32(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int32(c) != int32(d))
 		return true
@@ -4998,7 +5289,7 @@ func rewriteValuegeneric_OpNeq64(v *Value, config *Config) bool {
 	// result: (ConstBool [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConstBool)
@@ -5009,22 +5300,25 @@ func rewriteValuegeneric_OpNeq64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Neq64 (Const64 <t> [c-d]) x)
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		t := v.Args[0].Type
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpAdd64 {
+		t := v_0.Type
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpAdd64 {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpConst64 {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].Args[0].Type != t {
+		if v_1_0.Type != t {
 			break
 		}
-		d := v.Args[1].Args[0].AuxInt
-		x := v.Args[1].Args[1]
+		d := v_1_0.AuxInt
+		x := v_1.Args[1]
 		v.reset(OpNeq64)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
 		v0.AuxInt = c - d
@@ -5037,11 +5331,12 @@ func rewriteValuegeneric_OpNeq64(v *Value, config *Config) bool {
 	// result: (Neq64 (Const64 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst64) {
 			break
 		}
@@ -5056,14 +5351,16 @@ func rewriteValuegeneric_OpNeq64(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int64(c) != int64(d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int64(c) != int64(d))
 		return true
@@ -5078,7 +5375,7 @@ func rewriteValuegeneric_OpNeq8(v *Value, config *Config) bool {
 	// result: (ConstBool [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConstBool)
@@ -5089,14 +5386,16 @@ func rewriteValuegeneric_OpNeq8(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i((int8(c) != 0) != (int8(d) != 0))])
 	for {
-		if v.Args[0].Op != OpConstBool {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConstBool {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConstBool {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConstBool {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i((int8(c) != 0) != (int8(d) != 0))
 		return true
@@ -5105,10 +5404,11 @@ func rewriteValuegeneric_OpNeq8(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConstBool {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConstBool {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -5121,10 +5421,11 @@ func rewriteValuegeneric_OpNeq8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Not x)
 	for {
-		if v.Args[0].Op != OpConstBool {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConstBool {
 			break
 		}
-		if v.Args[0].AuxInt != 1 {
+		if v_0.AuxInt != 1 {
 			break
 		}
 		x := v.Args[1]
@@ -5136,22 +5437,25 @@ func rewriteValuegeneric_OpNeq8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Neq8 (Const8 <t> [c-d]) x)
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		t := v.Args[0].Type
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpAdd8 {
+		t := v_0.Type
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpAdd8 {
 			break
 		}
-		if v.Args[1].Args[0].Op != OpConst8 {
+		v_1_0 := v_1.Args[0]
+		if v_1_0.Op != OpConst8 {
 			break
 		}
-		if v.Args[1].Args[0].Type != t {
+		if v_1_0.Type != t {
 			break
 		}
-		d := v.Args[1].Args[0].AuxInt
-		x := v.Args[1].Args[1]
+		d := v_1_0.AuxInt
+		x := v_1.Args[1]
 		v.reset(OpNeq8)
 		v0 := b.NewValue0(v.Line, OpConst8, t)
 		v0.AuxInt = c - d
@@ -5164,11 +5468,12 @@ func rewriteValuegeneric_OpNeq8(v *Value, config *Config) bool {
 	// result: (Neq8 (Const8 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst8) {
 			break
 		}
@@ -5184,11 +5489,12 @@ func rewriteValuegeneric_OpNeq8(v *Value, config *Config) bool {
 	// result: (Neq8 (ConstBool <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConstBool {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConstBool {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConstBool) {
 			break
 		}
@@ -5203,14 +5509,16 @@ func rewriteValuegeneric_OpNeq8(v *Value, config *Config) bool {
 	// cond:
 	// result: (ConstBool [b2i(int8(c)  != int8(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConstBool)
 		v.AuxInt = b2i(int8(c) != int8(d))
 		return true
@@ -5245,7 +5553,8 @@ func rewriteValuegeneric_OpNeqPtr(v *Value, config *Config) bool {
 	// result: (IsNonNil p)
 	for {
 		p := v.Args[0]
-		if v.Args[1].Op != OpConstNil {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConstNil {
 			break
 		}
 		v.reset(OpIsNonNil)
@@ -5256,7 +5565,8 @@ func rewriteValuegeneric_OpNeqPtr(v *Value, config *Config) bool {
 	// cond:
 	// result: (IsNonNil p)
 	for {
-		if v.Args[0].Op != OpConstNil {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConstNil {
 			break
 		}
 		p := v.Args[1]
@@ -5289,27 +5599,30 @@ func rewriteValuegeneric_OpNeqSlice(v *Value, config *Config) bool {
 func rewriteValuegeneric_OpNilCheck(v *Value, config *Config) bool {
 	b := v.Block
 	_ = b
-	// match: (NilCheck (Phi x (Add64 (Const64 [c]) y)) mem)
-	// cond: c > 0 && v.Args[0] == y
+	// match: (NilCheck z:(Phi x (Add64 (Const64 [c]) y)) mem)
+	// cond: c > 0 && z == y
 	// result: (NilCheck x mem)
 	for {
-		if v.Args[0].Op != OpPhi {
+		z := v.Args[0]
+		if z.Op != OpPhi {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpAdd64 {
+		x := z.Args[0]
+		z_1 := z.Args[1]
+		if z_1.Op != OpAdd64 {
 			break
 		}
-		if v.Args[0].Args[1].Args[0].Op != OpConst64 {
+		z_1_0 := z_1.Args[0]
+		if z_1_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].Args[0].AuxInt
-		y := v.Args[0].Args[1].Args[1]
-		if len(v.Args[0].Args) != 2 {
+		c := z_1_0.AuxInt
+		y := z_1.Args[1]
+		if len(z.Args) != 2 {
 			break
 		}
 		mem := v.Args[1]
-		if !(c > 0 && v.Args[0] == y) {
+		if !(c > 0 && z == y) {
 			break
 		}
 		v.reset(OpNilCheck)
@@ -5326,11 +5639,12 @@ func rewriteValuegeneric_OpOffPtr(v *Value, config *Config) bool {
 	// cond:
 	// result: (OffPtr p [a+b])
 	for {
-		if v.Args[0].Op != OpOffPtr {
+		v_0 := v.Args[0]
+		if v_0.Op != OpOffPtr {
 			break
 		}
-		p := v.Args[0].Args[0]
-		b := v.Args[0].AuxInt
+		p := v_0.Args[0]
+		b := v_0.AuxInt
 		a := v.AuxInt
 		v.reset(OpOffPtr)
 		v.AddArg(p)
@@ -5363,11 +5677,12 @@ func rewriteValuegeneric_OpOr16(v *Value, config *Config) bool {
 	// result: (Or16 (Const16 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst16) {
 			break
 		}
@@ -5383,7 +5698,7 @@ func rewriteValuegeneric_OpOr16(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -5395,10 +5710,11 @@ func rewriteValuegeneric_OpOr16(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -5411,10 +5727,11 @@ func rewriteValuegeneric_OpOr16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [-1])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		if v.Args[0].AuxInt != -1 {
+		if v_0.AuxInt != -1 {
 			break
 		}
 		v.reset(OpConst16)
@@ -5431,11 +5748,12 @@ func rewriteValuegeneric_OpOr32(v *Value, config *Config) bool {
 	// result: (Or32 (Const32 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst32) {
 			break
 		}
@@ -5451,7 +5769,7 @@ func rewriteValuegeneric_OpOr32(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -5463,10 +5781,11 @@ func rewriteValuegeneric_OpOr32(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -5479,10 +5798,11 @@ func rewriteValuegeneric_OpOr32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [-1])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[0].AuxInt != -1 {
+		if v_0.AuxInt != -1 {
 			break
 		}
 		v.reset(OpConst32)
@@ -5499,11 +5819,12 @@ func rewriteValuegeneric_OpOr64(v *Value, config *Config) bool {
 	// result: (Or64 (Const64 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst64) {
 			break
 		}
@@ -5519,7 +5840,7 @@ func rewriteValuegeneric_OpOr64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -5531,10 +5852,11 @@ func rewriteValuegeneric_OpOr64(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -5547,10 +5869,11 @@ func rewriteValuegeneric_OpOr64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [-1])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != -1 {
+		if v_0.AuxInt != -1 {
 			break
 		}
 		v.reset(OpConst64)
@@ -5567,11 +5890,12 @@ func rewriteValuegeneric_OpOr8(v *Value, config *Config) bool {
 	// result: (Or8 (Const8 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst8) {
 			break
 		}
@@ -5587,7 +5911,7 @@ func rewriteValuegeneric_OpOr8(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -5599,10 +5923,11 @@ func rewriteValuegeneric_OpOr8(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -5615,10 +5940,11 @@ func rewriteValuegeneric_OpOr8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8 [-1])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		if v.Args[0].AuxInt != -1 {
+		if v_0.AuxInt != -1 {
 			break
 		}
 		v.reset(OpConst8)
@@ -5634,14 +5960,16 @@ func rewriteValuegeneric_OpPhi(v *Value, config *Config) bool {
 	// cond: int8(c) == int8(d)
 	// result: (Const8 [c])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if len(v.Args) != 2 {
 			break
 		}
@@ -5656,14 +5984,16 @@ func rewriteValuegeneric_OpPhi(v *Value, config *Config) bool {
 	// cond: int16(c) == int16(d)
 	// result: (Const16 [c])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if len(v.Args) != 2 {
 			break
 		}
@@ -5678,14 +6008,16 @@ func rewriteValuegeneric_OpPhi(v *Value, config *Config) bool {
 	// cond: int32(c) == int32(d)
 	// result: (Const32 [c])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if len(v.Args) != 2 {
 			break
 		}
@@ -5700,14 +6032,16 @@ func rewriteValuegeneric_OpPhi(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [c])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != c {
+		if v_1.AuxInt != c {
 			break
 		}
 		if len(v.Args) != 2 {
@@ -5771,25 +6105,30 @@ func rewriteValuegeneric_OpRsh16Ux16(v *Value, config *Config) bool {
 	// cond: c1 >= c2 && c3 >= c2
 	// result: (Rsh16Ux16 x (Const16 <config.fe.TypeUInt16()> [c1-c2+c3]))
 	for {
-		if v.Args[0].Op != OpLsh16x16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpLsh16x16 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpRsh16Ux16 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpRsh16Ux16 {
 			break
 		}
-		x := v.Args[0].Args[0].Args[0]
-		if v.Args[0].Args[0].Args[1].Op != OpConst16 {
+		x := v_0_0.Args[0]
+		v_0_0_1 := v_0_0.Args[1]
+		if v_0_0_1.Op != OpConst16 {
 			break
 		}
-		c1 := v.Args[0].Args[0].Args[1].AuxInt
-		if v.Args[0].Args[1].Op != OpConst16 {
+		c1 := v_0_0_1.AuxInt
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst16 {
 			break
 		}
-		c2 := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c2 := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c3 := v.Args[1].AuxInt
+		c3 := v_1.AuxInt
 		if !(c1 >= c2 && c3 >= c2) {
 			break
 		}
@@ -5806,10 +6145,11 @@ func rewriteValuegeneric_OpRsh16Ux16(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh16Ux64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -5828,10 +6168,11 @@ func rewriteValuegeneric_OpRsh16Ux32(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh16Ux64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -5848,14 +6189,16 @@ func rewriteValuegeneric_OpRsh16Ux64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [int64(uint16(c) >> uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst16)
 		v.AuxInt = int64(uint16(c) >> uint64(d))
 		return true
@@ -5864,10 +6207,11 @@ func rewriteValuegeneric_OpRsh16Ux64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [0])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst16)
@@ -5879,10 +6223,11 @@ func rewriteValuegeneric_OpRsh16Ux64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != 0 {
+		if v_1.AuxInt != 0 {
 			break
 		}
 		v.reset(OpCopy)
@@ -5894,10 +6239,11 @@ func rewriteValuegeneric_OpRsh16Ux64(v *Value, config *Config) bool {
 	// cond: uint64(c) >= 16
 	// result: (Const16 [0])
 	for {
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(uint64(c) >= 16) {
 			break
 		}
@@ -5910,18 +6256,21 @@ func rewriteValuegeneric_OpRsh16Ux64(v *Value, config *Config) bool {
 	// result: (Rsh16Ux64 x (Const64 <t> [c+d]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpRsh16Ux64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpRsh16Ux64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConst64 {
+		x := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(!uaddOvf(c, d)) {
 			break
 		}
@@ -5943,10 +6292,11 @@ func rewriteValuegeneric_OpRsh16Ux8(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh16Ux64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -5965,10 +6315,11 @@ func rewriteValuegeneric_OpRsh16x16(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh16x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -5987,10 +6338,11 @@ func rewriteValuegeneric_OpRsh16x32(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh16x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6007,14 +6359,16 @@ func rewriteValuegeneric_OpRsh16x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [int64(int16(c) >> uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst16)
 		v.AuxInt = int64(int16(c) >> uint64(d))
 		return true
@@ -6023,10 +6377,11 @@ func rewriteValuegeneric_OpRsh16x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [0])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst16)
@@ -6038,10 +6393,11 @@ func rewriteValuegeneric_OpRsh16x64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != 0 {
+		if v_1.AuxInt != 0 {
 			break
 		}
 		v.reset(OpCopy)
@@ -6054,18 +6410,21 @@ func rewriteValuegeneric_OpRsh16x64(v *Value, config *Config) bool {
 	// result: (Rsh16x64 x (Const64 <t> [c+d]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpRsh16x64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpRsh16x64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConst64 {
+		x := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(!uaddOvf(c, d)) {
 			break
 		}
@@ -6087,10 +6446,11 @@ func rewriteValuegeneric_OpRsh16x8(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh16x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6109,10 +6469,11 @@ func rewriteValuegeneric_OpRsh32Ux16(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh32Ux64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6129,25 +6490,30 @@ func rewriteValuegeneric_OpRsh32Ux32(v *Value, config *Config) bool {
 	// cond: c1 >= c2 && c3 >= c2
 	// result: (Rsh32Ux32 x (Const32 <config.fe.TypeUInt32()> [c1-c2+c3]))
 	for {
-		if v.Args[0].Op != OpLsh32x32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpLsh32x32 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpRsh32Ux32 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpRsh32Ux32 {
 			break
 		}
-		x := v.Args[0].Args[0].Args[0]
-		if v.Args[0].Args[0].Args[1].Op != OpConst32 {
+		x := v_0_0.Args[0]
+		v_0_0_1 := v_0_0.Args[1]
+		if v_0_0_1.Op != OpConst32 {
 			break
 		}
-		c1 := v.Args[0].Args[0].Args[1].AuxInt
-		if v.Args[0].Args[1].Op != OpConst32 {
+		c1 := v_0_0_1.AuxInt
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst32 {
 			break
 		}
-		c2 := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c2 := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c3 := v.Args[1].AuxInt
+		c3 := v_1.AuxInt
 		if !(c1 >= c2 && c3 >= c2) {
 			break
 		}
@@ -6164,10 +6530,11 @@ func rewriteValuegeneric_OpRsh32Ux32(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh32Ux64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6184,14 +6551,16 @@ func rewriteValuegeneric_OpRsh32Ux64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [int64(uint32(c) >> uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst32)
 		v.AuxInt = int64(uint32(c) >> uint64(d))
 		return true
@@ -6200,10 +6569,11 @@ func rewriteValuegeneric_OpRsh32Ux64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [0])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst32)
@@ -6215,10 +6585,11 @@ func rewriteValuegeneric_OpRsh32Ux64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != 0 {
+		if v_1.AuxInt != 0 {
 			break
 		}
 		v.reset(OpCopy)
@@ -6230,10 +6601,11 @@ func rewriteValuegeneric_OpRsh32Ux64(v *Value, config *Config) bool {
 	// cond: uint64(c) >= 32
 	// result: (Const32 [0])
 	for {
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(uint64(c) >= 32) {
 			break
 		}
@@ -6246,18 +6618,21 @@ func rewriteValuegeneric_OpRsh32Ux64(v *Value, config *Config) bool {
 	// result: (Rsh32Ux64 x (Const64 <t> [c+d]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpRsh32Ux64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpRsh32Ux64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConst64 {
+		x := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(!uaddOvf(c, d)) {
 			break
 		}
@@ -6279,10 +6654,11 @@ func rewriteValuegeneric_OpRsh32Ux8(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh32Ux64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6301,10 +6677,11 @@ func rewriteValuegeneric_OpRsh32x16(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh32x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6323,10 +6700,11 @@ func rewriteValuegeneric_OpRsh32x32(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh32x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6343,14 +6721,16 @@ func rewriteValuegeneric_OpRsh32x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [int64(int32(c) >> uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst32)
 		v.AuxInt = int64(int32(c) >> uint64(d))
 		return true
@@ -6359,10 +6739,11 @@ func rewriteValuegeneric_OpRsh32x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [0])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst32)
@@ -6374,10 +6755,11 @@ func rewriteValuegeneric_OpRsh32x64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != 0 {
+		if v_1.AuxInt != 0 {
 			break
 		}
 		v.reset(OpCopy)
@@ -6390,18 +6772,21 @@ func rewriteValuegeneric_OpRsh32x64(v *Value, config *Config) bool {
 	// result: (Rsh32x64 x (Const64 <t> [c+d]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpRsh32x64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpRsh32x64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConst64 {
+		x := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(!uaddOvf(c, d)) {
 			break
 		}
@@ -6423,10 +6808,11 @@ func rewriteValuegeneric_OpRsh32x8(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh32x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6445,10 +6831,11 @@ func rewriteValuegeneric_OpRsh64Ux16(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh64Ux64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6460,10 +6847,11 @@ func rewriteValuegeneric_OpRsh64Ux16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -6481,10 +6869,11 @@ func rewriteValuegeneric_OpRsh64Ux32(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh64Ux64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6496,10 +6885,11 @@ func rewriteValuegeneric_OpRsh64Ux32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -6515,14 +6905,16 @@ func rewriteValuegeneric_OpRsh64Ux64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [int64(uint64(c) >> uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst64)
 		v.AuxInt = int64(uint64(c) >> uint64(d))
 		return true
@@ -6531,10 +6923,11 @@ func rewriteValuegeneric_OpRsh64Ux64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -6545,25 +6938,30 @@ func rewriteValuegeneric_OpRsh64Ux64(v *Value, config *Config) bool {
 	// cond: c1 >= c2 && c3 >= c2
 	// result: (Rsh64Ux64 x (Const64 <config.fe.TypeUInt64()> [c1-c2+c3]))
 	for {
-		if v.Args[0].Op != OpLsh64x64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpLsh64x64 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpRsh64Ux64 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpRsh64Ux64 {
 			break
 		}
-		x := v.Args[0].Args[0].Args[0]
-		if v.Args[0].Args[0].Args[1].Op != OpConst64 {
+		x := v_0_0.Args[0]
+		v_0_0_1 := v_0_0.Args[1]
+		if v_0_0_1.Op != OpConst64 {
 			break
 		}
-		c1 := v.Args[0].Args[0].Args[1].AuxInt
-		if v.Args[0].Args[1].Op != OpConst64 {
+		c1 := v_0_0_1.AuxInt
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c2 := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c2 := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c3 := v.Args[1].AuxInt
+		c3 := v_1.AuxInt
 		if !(c1 >= c2 && c3 >= c2) {
 			break
 		}
@@ -6579,10 +6977,11 @@ func rewriteValuegeneric_OpRsh64Ux64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != 0 {
+		if v_1.AuxInt != 0 {
 			break
 		}
 		v.reset(OpCopy)
@@ -6594,10 +6993,11 @@ func rewriteValuegeneric_OpRsh64Ux64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -6608,10 +7008,11 @@ func rewriteValuegeneric_OpRsh64Ux64(v *Value, config *Config) bool {
 	// cond: uint64(c) >= 64
 	// result: (Const64 [0])
 	for {
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(uint64(c) >= 64) {
 			break
 		}
@@ -6624,18 +7025,21 @@ func rewriteValuegeneric_OpRsh64Ux64(v *Value, config *Config) bool {
 	// result: (Rsh64Ux64 x (Const64 <t> [c+d]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpRsh64Ux64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpRsh64Ux64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConst64 {
+		x := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(!uaddOvf(c, d)) {
 			break
 		}
@@ -6657,10 +7061,11 @@ func rewriteValuegeneric_OpRsh64Ux8(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh64Ux64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6672,10 +7077,11 @@ func rewriteValuegeneric_OpRsh64Ux8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -6693,10 +7099,11 @@ func rewriteValuegeneric_OpRsh64x16(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh64x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6708,10 +7115,11 @@ func rewriteValuegeneric_OpRsh64x16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -6729,10 +7137,11 @@ func rewriteValuegeneric_OpRsh64x32(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh64x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6744,10 +7153,11 @@ func rewriteValuegeneric_OpRsh64x32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -6763,14 +7173,16 @@ func rewriteValuegeneric_OpRsh64x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [c >> uint64(d)])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst64)
 		v.AuxInt = c >> uint64(d)
 		return true
@@ -6779,10 +7191,11 @@ func rewriteValuegeneric_OpRsh64x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -6794,10 +7207,11 @@ func rewriteValuegeneric_OpRsh64x64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != 0 {
+		if v_1.AuxInt != 0 {
 			break
 		}
 		v.reset(OpCopy)
@@ -6809,10 +7223,11 @@ func rewriteValuegeneric_OpRsh64x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -6824,18 +7239,21 @@ func rewriteValuegeneric_OpRsh64x64(v *Value, config *Config) bool {
 	// result: (Rsh64x64 x (Const64 <t> [c+d]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpRsh64x64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpRsh64x64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConst64 {
+		x := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(!uaddOvf(c, d)) {
 			break
 		}
@@ -6857,10 +7275,11 @@ func rewriteValuegeneric_OpRsh64x8(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh64x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6872,10 +7291,11 @@ func rewriteValuegeneric_OpRsh64x8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [0])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst64)
@@ -6893,10 +7313,11 @@ func rewriteValuegeneric_OpRsh8Ux16(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh8Ux64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6915,10 +7336,11 @@ func rewriteValuegeneric_OpRsh8Ux32(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh8Ux64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -6935,14 +7357,16 @@ func rewriteValuegeneric_OpRsh8Ux64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8  [int64(uint8(c) >> uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst8)
 		v.AuxInt = int64(uint8(c) >> uint64(d))
 		return true
@@ -6951,10 +7375,11 @@ func rewriteValuegeneric_OpRsh8Ux64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8  [0])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst8)
@@ -6966,10 +7391,11 @@ func rewriteValuegeneric_OpRsh8Ux64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != 0 {
+		if v_1.AuxInt != 0 {
 			break
 		}
 		v.reset(OpCopy)
@@ -6981,10 +7407,11 @@ func rewriteValuegeneric_OpRsh8Ux64(v *Value, config *Config) bool {
 	// cond: uint64(c) >= 8
 	// result: (Const8 [0])
 	for {
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		if !(uint64(c) >= 8) {
 			break
 		}
@@ -6997,18 +7424,21 @@ func rewriteValuegeneric_OpRsh8Ux64(v *Value, config *Config) bool {
 	// result: (Rsh8Ux64  x (Const64 <t> [c+d]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpRsh8Ux64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpRsh8Ux64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConst64 {
+		x := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(!uaddOvf(c, d)) {
 			break
 		}
@@ -7028,25 +7458,30 @@ func rewriteValuegeneric_OpRsh8Ux8(v *Value, config *Config) bool {
 	// cond: c1 >= c2 && c3 >= c2
 	// result: (Rsh8Ux8 x (Const8 <config.fe.TypeUInt8()> [c1-c2+c3]))
 	for {
-		if v.Args[0].Op != OpLsh8x8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpLsh8x8 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpRsh8Ux8 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpRsh8Ux8 {
 			break
 		}
-		x := v.Args[0].Args[0].Args[0]
-		if v.Args[0].Args[0].Args[1].Op != OpConst8 {
+		x := v_0_0.Args[0]
+		v_0_0_1 := v_0_0.Args[1]
+		if v_0_0_1.Op != OpConst8 {
 			break
 		}
-		c1 := v.Args[0].Args[0].Args[1].AuxInt
-		if v.Args[0].Args[1].Op != OpConst8 {
+		c1 := v_0_0_1.AuxInt
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst8 {
 			break
 		}
-		c2 := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c2 := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c3 := v.Args[1].AuxInt
+		c3 := v_1.AuxInt
 		if !(c1 >= c2 && c3 >= c2) {
 			break
 		}
@@ -7063,10 +7498,11 @@ func rewriteValuegeneric_OpRsh8Ux8(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh8Ux64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -7085,10 +7521,11 @@ func rewriteValuegeneric_OpRsh8x16(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh8x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -7107,10 +7544,11 @@ func rewriteValuegeneric_OpRsh8x32(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh8x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -7127,14 +7565,16 @@ func rewriteValuegeneric_OpRsh8x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8  [int64(int8(c) >> uint64(d))])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst8)
 		v.AuxInt = int64(int8(c) >> uint64(d))
 		return true
@@ -7143,10 +7583,11 @@ func rewriteValuegeneric_OpRsh8x64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8  [0])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		v.reset(OpConst8)
@@ -7158,10 +7599,11 @@ func rewriteValuegeneric_OpRsh8x64(v *Value, config *Config) bool {
 	// result: x
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		if v.Args[1].AuxInt != 0 {
+		if v_1.AuxInt != 0 {
 			break
 		}
 		v.reset(OpCopy)
@@ -7174,18 +7616,21 @@ func rewriteValuegeneric_OpRsh8x64(v *Value, config *Config) bool {
 	// result: (Rsh8x64  x (Const64 <t> [c+d]))
 	for {
 		t := v.Type
-		if v.Args[0].Op != OpRsh8x64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpRsh8x64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		if v.Args[0].Args[1].Op != OpConst64 {
+		x := v_0.Args[0]
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].Args[1].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0_1.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		if !(!uaddOvf(c, d)) {
 			break
 		}
@@ -7207,10 +7652,11 @@ func rewriteValuegeneric_OpRsh8x8(v *Value, config *Config) bool {
 	for {
 		t := v.Type
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		c := v.Args[1].AuxInt
+		c := v_1.AuxInt
 		v.reset(OpRsh8x64)
 		v.AddArg(x)
 		v0 := b.NewValue0(v.Line, OpConst64, t)
@@ -7227,14 +7673,16 @@ func rewriteValuegeneric_OpSliceCap(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 <t> [c])
 	for {
-		if v.Args[0].Op != OpSliceMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSliceMake {
 			break
 		}
-		if v.Args[0].Args[2].Op != OpConst64 {
+		v_0_2 := v_0.Args[2]
+		if v_0_2.Op != OpConst64 {
 			break
 		}
-		t := v.Args[0].Args[2].Type
-		c := v.Args[0].Args[2].AuxInt
+		t := v_0_2.Type
+		c := v_0_2.AuxInt
 		v.reset(OpConst64)
 		v.Type = t
 		v.AuxInt = c
@@ -7244,13 +7692,15 @@ func rewriteValuegeneric_OpSliceCap(v *Value, config *Config) bool {
 	// cond:
 	// result: (SliceCap x)
 	for {
-		if v.Args[0].Op != OpSliceMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSliceMake {
 			break
 		}
-		if v.Args[0].Args[2].Op != OpSliceCap {
+		v_0_2 := v_0.Args[2]
+		if v_0_2.Op != OpSliceCap {
 			break
 		}
-		x := v.Args[0].Args[2].Args[0]
+		x := v_0_2.Args[0]
 		v.reset(OpSliceCap)
 		v.AddArg(x)
 		return true
@@ -7264,14 +7714,16 @@ func rewriteValuegeneric_OpSliceLen(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 <t> [c])
 	for {
-		if v.Args[0].Op != OpSliceMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSliceMake {
 			break
 		}
-		if v.Args[0].Args[1].Op != OpConst64 {
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		t := v.Args[0].Args[1].Type
-		c := v.Args[0].Args[1].AuxInt
+		t := v_0_1.Type
+		c := v_0_1.AuxInt
 		v.reset(OpConst64)
 		v.Type = t
 		v.AuxInt = c
@@ -7281,13 +7733,15 @@ func rewriteValuegeneric_OpSliceLen(v *Value, config *Config) bool {
 	// cond:
 	// result: (SliceLen x)
 	for {
-		if v.Args[0].Op != OpSliceMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSliceMake {
 			break
 		}
-		if v.Args[0].Args[1].Op != OpSliceLen {
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpSliceLen {
 			break
 		}
-		x := v.Args[0].Args[1].Args[0]
+		x := v_0_1.Args[0]
 		v.reset(OpSliceLen)
 		v.AddArg(x)
 		return true
@@ -7301,14 +7755,16 @@ func rewriteValuegeneric_OpSlicePtr(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 <t> [c])
 	for {
-		if v.Args[0].Op != OpSliceMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSliceMake {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConst64 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConst64 {
 			break
 		}
-		t := v.Args[0].Args[0].Type
-		c := v.Args[0].Args[0].AuxInt
+		t := v_0_0.Type
+		c := v_0_0.AuxInt
 		v.reset(OpConst64)
 		v.Type = t
 		v.AuxInt = c
@@ -7318,13 +7774,15 @@ func rewriteValuegeneric_OpSlicePtr(v *Value, config *Config) bool {
 	// cond:
 	// result: (SlicePtr x)
 	for {
-		if v.Args[0].Op != OpSliceMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpSliceMake {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpSlicePtr {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpSlicePtr {
 			break
 		}
-		x := v.Args[0].Args[0].Args[0]
+		x := v_0_0.Args[0]
 		v.reset(OpSlicePtr)
 		v.AddArg(x)
 		return true
@@ -7338,7 +7796,8 @@ func rewriteValuegeneric_OpStore(v *Value, config *Config) bool {
 	// cond:
 	// result: mem
 	for {
-		if v.Args[1].Op != OpStructMake0 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpStructMake0 {
 			break
 		}
 		mem := v.Args[2]
@@ -7352,11 +7811,12 @@ func rewriteValuegeneric_OpStore(v *Value, config *Config) bool {
 	// result: (Store [t.FieldType(0).Size()] dst f0 mem)
 	for {
 		dst := v.Args[0]
-		if v.Args[1].Op != OpStructMake1 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpStructMake1 {
 			break
 		}
-		t := v.Args[1].Type
-		f0 := v.Args[1].Args[0]
+		t := v_1.Type
+		f0 := v_1.Args[0]
 		mem := v.Args[2]
 		v.reset(OpStore)
 		v.AuxInt = t.FieldType(0).Size()
@@ -7370,12 +7830,13 @@ func rewriteValuegeneric_OpStore(v *Value, config *Config) bool {
 	// result: (Store [t.FieldType(1).Size()]     (OffPtr <t.FieldType(1).PtrTo()> [t.FieldOff(1)] dst)     f1     (Store [t.FieldType(0).Size()] dst f0 mem))
 	for {
 		dst := v.Args[0]
-		if v.Args[1].Op != OpStructMake2 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpStructMake2 {
 			break
 		}
-		t := v.Args[1].Type
-		f0 := v.Args[1].Args[0]
-		f1 := v.Args[1].Args[1]
+		t := v_1.Type
+		f0 := v_1.Args[0]
+		f1 := v_1.Args[1]
 		mem := v.Args[2]
 		v.reset(OpStore)
 		v.AuxInt = t.FieldType(1).Size()
@@ -7397,13 +7858,14 @@ func rewriteValuegeneric_OpStore(v *Value, config *Config) bool {
 	// result: (Store [t.FieldType(2).Size()]     (OffPtr <t.FieldType(2).PtrTo()> [t.FieldOff(2)] dst)     f2     (Store [t.FieldType(1).Size()]       (OffPtr <t.FieldType(1).PtrTo()> [t.FieldOff(1)] dst)       f1       (Store [t.FieldType(0).Size()] dst f0 mem)))
 	for {
 		dst := v.Args[0]
-		if v.Args[1].Op != OpStructMake3 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpStructMake3 {
 			break
 		}
-		t := v.Args[1].Type
-		f0 := v.Args[1].Args[0]
-		f1 := v.Args[1].Args[1]
-		f2 := v.Args[1].Args[2]
+		t := v_1.Type
+		f0 := v_1.Args[0]
+		f1 := v_1.Args[1]
+		f2 := v_1.Args[2]
 		mem := v.Args[2]
 		v.reset(OpStore)
 		v.AuxInt = t.FieldType(2).Size()
@@ -7433,14 +7895,15 @@ func rewriteValuegeneric_OpStore(v *Value, config *Config) bool {
 	// result: (Store [t.FieldType(3).Size()]     (OffPtr <t.FieldType(3).PtrTo()> [t.FieldOff(3)] dst)     f3     (Store [t.FieldType(2).Size()]       (OffPtr <t.FieldType(2).PtrTo()> [t.FieldOff(2)] dst)       f2       (Store [t.FieldType(1).Size()]         (OffPtr <t.FieldType(1).PtrTo()> [t.FieldOff(1)] dst)         f1         (Store [t.FieldType(0).Size()] dst f0 mem))))
 	for {
 		dst := v.Args[0]
-		if v.Args[1].Op != OpStructMake4 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpStructMake4 {
 			break
 		}
-		t := v.Args[1].Type
-		f0 := v.Args[1].Args[0]
-		f1 := v.Args[1].Args[1]
-		f2 := v.Args[1].Args[2]
-		f3 := v.Args[1].Args[3]
+		t := v_1.Type
+		f0 := v_1.Args[0]
+		f1 := v_1.Args[1]
+		f2 := v_1.Args[2]
+		f3 := v_1.Args[3]
 		mem := v.Args[2]
 		v.reset(OpStore)
 		v.AuxInt = t.FieldType(3).Size()
@@ -7481,11 +7944,12 @@ func rewriteValuegeneric_OpStore(v *Value, config *Config) bool {
 			break
 		}
 		dst := v.Args[0]
-		if v.Args[1].Op != OpComplexMake {
+		v_1 := v.Args[1]
+		if v_1.Op != OpComplexMake {
 			break
 		}
-		real := v.Args[1].Args[0]
-		imag := v.Args[1].Args[1]
+		real := v_1.Args[0]
+		imag := v_1.Args[1]
 		mem := v.Args[2]
 		v.reset(OpStore)
 		v.AuxInt = 4
@@ -7510,11 +7974,12 @@ func rewriteValuegeneric_OpStore(v *Value, config *Config) bool {
 			break
 		}
 		dst := v.Args[0]
-		if v.Args[1].Op != OpComplexMake {
+		v_1 := v.Args[1]
+		if v_1.Op != OpComplexMake {
 			break
 		}
-		real := v.Args[1].Args[0]
-		imag := v.Args[1].Args[1]
+		real := v_1.Args[0]
+		imag := v_1.Args[1]
 		mem := v.Args[2]
 		v.reset(OpStore)
 		v.AuxInt = 8
@@ -7539,11 +8004,12 @@ func rewriteValuegeneric_OpStore(v *Value, config *Config) bool {
 			break
 		}
 		dst := v.Args[0]
-		if v.Args[1].Op != OpStringMake {
+		v_1 := v.Args[1]
+		if v_1.Op != OpStringMake {
 			break
 		}
-		ptr := v.Args[1].Args[0]
-		len := v.Args[1].Args[1]
+		ptr := v_1.Args[0]
+		len := v_1.Args[1]
 		mem := v.Args[2]
 		v.reset(OpStore)
 		v.AuxInt = config.PtrSize
@@ -7568,12 +8034,13 @@ func rewriteValuegeneric_OpStore(v *Value, config *Config) bool {
 			break
 		}
 		dst := v.Args[0]
-		if v.Args[1].Op != OpSliceMake {
+		v_1 := v.Args[1]
+		if v_1.Op != OpSliceMake {
 			break
 		}
-		ptr := v.Args[1].Args[0]
-		len := v.Args[1].Args[1]
-		cap := v.Args[1].Args[2]
+		ptr := v_1.Args[0]
+		len := v_1.Args[1]
+		cap := v_1.Args[2]
 		mem := v.Args[2]
 		v.reset(OpStore)
 		v.AuxInt = config.PtrSize
@@ -7606,11 +8073,12 @@ func rewriteValuegeneric_OpStore(v *Value, config *Config) bool {
 			break
 		}
 		dst := v.Args[0]
-		if v.Args[1].Op != OpIMake {
+		v_1 := v.Args[1]
+		if v_1.Op != OpIMake {
 			break
 		}
-		itab := v.Args[1].Args[0]
-		data := v.Args[1].Args[1]
+		itab := v_1.Args[0]
+		data := v_1.Args[1]
 		mem := v.Args[2]
 		v.reset(OpStore)
 		v.AuxInt = config.PtrSize
@@ -7633,13 +8101,14 @@ func rewriteValuegeneric_OpStore(v *Value, config *Config) bool {
 	for {
 		size := v.AuxInt
 		dst := v.Args[0]
-		if v.Args[1].Op != OpLoad {
+		v_1 := v.Args[1]
+		if v_1.Op != OpLoad {
 			break
 		}
-		t := v.Args[1].Type
-		src := v.Args[1].Args[0]
-		mem := v.Args[1].Args[1]
-		if v.Args[2] != mem {
+		t := v_1.Type
+		src := v_1.Args[0]
+		mem := v_1.Args[1]
+		if mem != v.Args[2] {
 			break
 		}
 		if !(!config.fe.CanSSA(t)) {
@@ -7658,17 +8127,19 @@ func rewriteValuegeneric_OpStore(v *Value, config *Config) bool {
 	for {
 		size := v.AuxInt
 		dst := v.Args[0]
-		if v.Args[1].Op != OpLoad {
+		v_1 := v.Args[1]
+		if v_1.Op != OpLoad {
 			break
 		}
-		t := v.Args[1].Type
-		src := v.Args[1].Args[0]
-		mem := v.Args[1].Args[1]
-		if v.Args[2].Op != OpVarDef {
+		t := v_1.Type
+		src := v_1.Args[0]
+		mem := v_1.Args[1]
+		v_2 := v.Args[2]
+		if v_2.Op != OpVarDef {
 			break
 		}
-		x := v.Args[2].Aux
-		if v.Args[2].Args[0] != mem {
+		x := v_2.Aux
+		if mem != v_2.Args[0] {
 			break
 		}
 		if !(!config.fe.CanSSA(t)) {
@@ -7693,14 +8164,16 @@ func rewriteValuegeneric_OpStringLen(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 <t> [c])
 	for {
-		if v.Args[0].Op != OpStringMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStringMake {
 			break
 		}
-		if v.Args[0].Args[1].Op != OpConst64 {
+		v_0_1 := v_0.Args[1]
+		if v_0_1.Op != OpConst64 {
 			break
 		}
-		t := v.Args[0].Args[1].Type
-		c := v.Args[0].Args[1].AuxInt
+		t := v_0_1.Type
+		c := v_0_1.AuxInt
 		v.reset(OpConst64)
 		v.Type = t
 		v.AuxInt = c
@@ -7715,14 +8188,16 @@ func rewriteValuegeneric_OpStringPtr(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 <t> [c])
 	for {
-		if v.Args[0].Op != OpStringMake {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStringMake {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConst64 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConst64 {
 			break
 		}
-		t := v.Args[0].Args[0].Type
-		c := v.Args[0].Args[0].AuxInt
+		t := v_0_0.Type
+		c := v_0_0.AuxInt
 		v.reset(OpConst64)
 		v.Type = t
 		v.AuxInt = c
@@ -7737,10 +8212,11 @@ func rewriteValuegeneric_OpStructSelect(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpStructMake1 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStructMake1 {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -7753,10 +8229,11 @@ func rewriteValuegeneric_OpStructSelect(v *Value, config *Config) bool {
 		if v.AuxInt != 0 {
 			break
 		}
-		if v.Args[0].Op != OpStructMake2 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStructMake2 {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -7769,10 +8246,11 @@ func rewriteValuegeneric_OpStructSelect(v *Value, config *Config) bool {
 		if v.AuxInt != 1 {
 			break
 		}
-		if v.Args[0].Op != OpStructMake2 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStructMake2 {
 			break
 		}
-		x := v.Args[0].Args[1]
+		x := v_0.Args[1]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -7785,10 +8263,11 @@ func rewriteValuegeneric_OpStructSelect(v *Value, config *Config) bool {
 		if v.AuxInt != 0 {
 			break
 		}
-		if v.Args[0].Op != OpStructMake3 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStructMake3 {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -7801,10 +8280,11 @@ func rewriteValuegeneric_OpStructSelect(v *Value, config *Config) bool {
 		if v.AuxInt != 1 {
 			break
 		}
-		if v.Args[0].Op != OpStructMake3 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStructMake3 {
 			break
 		}
-		x := v.Args[0].Args[1]
+		x := v_0.Args[1]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -7817,10 +8297,11 @@ func rewriteValuegeneric_OpStructSelect(v *Value, config *Config) bool {
 		if v.AuxInt != 2 {
 			break
 		}
-		if v.Args[0].Op != OpStructMake3 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStructMake3 {
 			break
 		}
-		x := v.Args[0].Args[2]
+		x := v_0.Args[2]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -7833,10 +8314,11 @@ func rewriteValuegeneric_OpStructSelect(v *Value, config *Config) bool {
 		if v.AuxInt != 0 {
 			break
 		}
-		if v.Args[0].Op != OpStructMake4 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStructMake4 {
 			break
 		}
-		x := v.Args[0].Args[0]
+		x := v_0.Args[0]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -7849,10 +8331,11 @@ func rewriteValuegeneric_OpStructSelect(v *Value, config *Config) bool {
 		if v.AuxInt != 1 {
 			break
 		}
-		if v.Args[0].Op != OpStructMake4 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStructMake4 {
 			break
 		}
-		x := v.Args[0].Args[1]
+		x := v_0.Args[1]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -7865,10 +8348,11 @@ func rewriteValuegeneric_OpStructSelect(v *Value, config *Config) bool {
 		if v.AuxInt != 2 {
 			break
 		}
-		if v.Args[0].Op != OpStructMake4 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStructMake4 {
 			break
 		}
-		x := v.Args[0].Args[2]
+		x := v_0.Args[2]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
@@ -7881,30 +8365,32 @@ func rewriteValuegeneric_OpStructSelect(v *Value, config *Config) bool {
 		if v.AuxInt != 3 {
 			break
 		}
-		if v.Args[0].Op != OpStructMake4 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpStructMake4 {
 			break
 		}
-		x := v.Args[0].Args[3]
+		x := v_0.Args[3]
 		v.reset(OpCopy)
 		v.Type = x.Type
 		v.AddArg(x)
 		return true
 	}
-	// match: (StructSelect [i] (Load <t> ptr mem))
+	// match: (StructSelect [i] x:(Load <t> ptr mem))
 	// cond: !config.fe.CanSSA(t)
-	// result: @v.Args[0].Block (Load <v.Type> (OffPtr <v.Type.PtrTo()> [t.FieldOff(int(i))] ptr) mem)
+	// result: @x.Block (Load <v.Type> (OffPtr <v.Type.PtrTo()> [t.FieldOff(int(i))] ptr) mem)
 	for {
 		i := v.AuxInt
-		if v.Args[0].Op != OpLoad {
+		x := v.Args[0]
+		if x.Op != OpLoad {
 			break
 		}
-		t := v.Args[0].Type
-		ptr := v.Args[0].Args[0]
-		mem := v.Args[0].Args[1]
+		t := x.Type
+		ptr := x.Args[0]
+		mem := x.Args[1]
 		if !(!config.fe.CanSSA(t)) {
 			break
 		}
-		b = v.Args[0].Block
+		b = x.Block
 		v0 := b.NewValue0(v.Line, OpLoad, v.Type)
 		v.reset(OpCopy)
 		v.AddArg(v0)
@@ -7924,14 +8410,16 @@ func rewriteValuegeneric_OpSub16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [c-d])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst16 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst16)
 		v.AuxInt = c - d
 		return true
@@ -7941,11 +8429,12 @@ func rewriteValuegeneric_OpSub16(v *Value, config *Config) bool {
 	// result: (Add16 (Const16 <t> [-c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst16) {
 			break
 		}
@@ -7961,7 +8450,7 @@ func rewriteValuegeneric_OpSub16(v *Value, config *Config) bool {
 	// result: (Const16 [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConst16)
@@ -7972,12 +8461,13 @@ func rewriteValuegeneric_OpSub16(v *Value, config *Config) bool {
 	// cond:
 	// result: y
 	for {
-		if v.Args[0].Op != OpAdd16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAdd16 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
-		if v.Args[1] != x {
+		x := v_0.Args[0]
+		y := v_0.Args[1]
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -7989,12 +8479,13 @@ func rewriteValuegeneric_OpSub16(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpAdd16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAdd16 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
-		if v.Args[1] != y {
+		x := v_0.Args[0]
+		y := v_0.Args[1]
+		if y != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -8011,14 +8502,16 @@ func rewriteValuegeneric_OpSub32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [c-d])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst32)
 		v.AuxInt = c - d
 		return true
@@ -8028,11 +8521,12 @@ func rewriteValuegeneric_OpSub32(v *Value, config *Config) bool {
 	// result: (Add32 (Const32 <t> [-c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst32) {
 			break
 		}
@@ -8048,7 +8542,7 @@ func rewriteValuegeneric_OpSub32(v *Value, config *Config) bool {
 	// result: (Const32 [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConst32)
@@ -8059,12 +8553,13 @@ func rewriteValuegeneric_OpSub32(v *Value, config *Config) bool {
 	// cond:
 	// result: y
 	for {
-		if v.Args[0].Op != OpAdd32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAdd32 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
-		if v.Args[1] != x {
+		x := v_0.Args[0]
+		y := v_0.Args[1]
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -8076,12 +8571,13 @@ func rewriteValuegeneric_OpSub32(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpAdd32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAdd32 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
-		if v.Args[1] != y {
+		x := v_0.Args[0]
+		y := v_0.Args[1]
+		if y != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -8098,14 +8594,16 @@ func rewriteValuegeneric_OpSub32F(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32F [f2i(float64(i2f32(c) - i2f32(d)))])
 	for {
-		if v.Args[0].Op != OpConst32F {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32F {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst32F {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32F {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst32F)
 		v.AuxInt = f2i(float64(i2f32(c) - i2f32(d)))
 		return true
@@ -8119,14 +8617,16 @@ func rewriteValuegeneric_OpSub64(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64 [c-d])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst64)
 		v.AuxInt = c - d
 		return true
@@ -8136,11 +8636,12 @@ func rewriteValuegeneric_OpSub64(v *Value, config *Config) bool {
 	// result: (Add64 (Const64 <t> [-c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst64) {
 			break
 		}
@@ -8156,7 +8657,7 @@ func rewriteValuegeneric_OpSub64(v *Value, config *Config) bool {
 	// result: (Const64 [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConst64)
@@ -8167,12 +8668,13 @@ func rewriteValuegeneric_OpSub64(v *Value, config *Config) bool {
 	// cond:
 	// result: y
 	for {
-		if v.Args[0].Op != OpAdd64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAdd64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
-		if v.Args[1] != x {
+		x := v_0.Args[0]
+		y := v_0.Args[1]
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -8184,12 +8686,13 @@ func rewriteValuegeneric_OpSub64(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpAdd64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAdd64 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
-		if v.Args[1] != y {
+		x := v_0.Args[0]
+		y := v_0.Args[1]
+		if y != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -8206,14 +8709,16 @@ func rewriteValuegeneric_OpSub64F(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const64F [f2i(i2f(c) - i2f(d))])
 	for {
-		if v.Args[0].Op != OpConst64F {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64F {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst64F {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64F {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst64F)
 		v.AuxInt = f2i(i2f(c) - i2f(d))
 		return true
@@ -8227,14 +8732,16 @@ func rewriteValuegeneric_OpSub8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8 [c-d])
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		c := v.Args[0].AuxInt
-		if v.Args[1].Op != OpConst8 {
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		d := v.Args[1].AuxInt
+		d := v_1.AuxInt
 		v.reset(OpConst8)
 		v.AuxInt = c - d
 		return true
@@ -8244,11 +8751,12 @@ func rewriteValuegeneric_OpSub8(v *Value, config *Config) bool {
 	// result: (Add8 (Const8 <t> [-c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst8) {
 			break
 		}
@@ -8264,7 +8772,7 @@ func rewriteValuegeneric_OpSub8(v *Value, config *Config) bool {
 	// result: (Const8 [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConst8)
@@ -8275,12 +8783,13 @@ func rewriteValuegeneric_OpSub8(v *Value, config *Config) bool {
 	// cond:
 	// result: y
 	for {
-		if v.Args[0].Op != OpAdd8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAdd8 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
-		if v.Args[1] != x {
+		x := v_0.Args[0]
+		y := v_0.Args[1]
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -8292,12 +8801,13 @@ func rewriteValuegeneric_OpSub8(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpAdd8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAdd8 {
 			break
 		}
-		x := v.Args[0].Args[0]
-		y := v.Args[0].Args[1]
-		if v.Args[1] != y {
+		x := v_0.Args[0]
+		y := v_0.Args[1]
+		if y != v.Args[1] {
 			break
 		}
 		v.reset(OpCopy)
@@ -8314,10 +8824,11 @@ func rewriteValuegeneric_OpTrunc16to8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8 [int64(int8(c))])
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpConst8)
 		v.AuxInt = int64(int8(c))
 		return true
@@ -8326,14 +8837,16 @@ func rewriteValuegeneric_OpTrunc16to8(v *Value, config *Config) bool {
 	// cond: y&0xFF == 0xFF
 	// result: (Trunc16to8 x)
 	for {
-		if v.Args[0].Op != OpAnd16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAnd16 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConst16 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConst16 {
 			break
 		}
-		y := v.Args[0].Args[0].AuxInt
-		x := v.Args[0].Args[1]
+		y := v_0_0.AuxInt
+		x := v_0.Args[1]
 		if !(y&0xFF == 0xFF) {
 			break
 		}
@@ -8350,10 +8863,11 @@ func rewriteValuegeneric_OpTrunc32to16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [int64(int16(c))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpConst16)
 		v.AuxInt = int64(int16(c))
 		return true
@@ -8362,14 +8876,16 @@ func rewriteValuegeneric_OpTrunc32to16(v *Value, config *Config) bool {
 	// cond: y&0xFFFF == 0xFFFF
 	// result: (Trunc32to16 x)
 	for {
-		if v.Args[0].Op != OpAnd32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAnd32 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConst32 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConst32 {
 			break
 		}
-		y := v.Args[0].Args[0].AuxInt
-		x := v.Args[0].Args[1]
+		y := v_0_0.AuxInt
+		x := v_0.Args[1]
 		if !(y&0xFFFF == 0xFFFF) {
 			break
 		}
@@ -8386,10 +8902,11 @@ func rewriteValuegeneric_OpTrunc32to8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8 [int64(int8(c))])
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpConst8)
 		v.AuxInt = int64(int8(c))
 		return true
@@ -8398,14 +8915,16 @@ func rewriteValuegeneric_OpTrunc32to8(v *Value, config *Config) bool {
 	// cond: y&0xFF == 0xFF
 	// result: (Trunc32to8 x)
 	for {
-		if v.Args[0].Op != OpAnd32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAnd32 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConst32 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConst32 {
 			break
 		}
-		y := v.Args[0].Args[0].AuxInt
-		x := v.Args[0].Args[1]
+		y := v_0_0.AuxInt
+		x := v_0.Args[1]
 		if !(y&0xFF == 0xFF) {
 			break
 		}
@@ -8422,10 +8941,11 @@ func rewriteValuegeneric_OpTrunc64to16(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const16 [int64(int16(c))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpConst16)
 		v.AuxInt = int64(int16(c))
 		return true
@@ -8434,14 +8954,16 @@ func rewriteValuegeneric_OpTrunc64to16(v *Value, config *Config) bool {
 	// cond: y&0xFFFF == 0xFFFF
 	// result: (Trunc64to16 x)
 	for {
-		if v.Args[0].Op != OpAnd64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAnd64 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConst64 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConst64 {
 			break
 		}
-		y := v.Args[0].Args[0].AuxInt
-		x := v.Args[0].Args[1]
+		y := v_0_0.AuxInt
+		x := v_0.Args[1]
 		if !(y&0xFFFF == 0xFFFF) {
 			break
 		}
@@ -8458,10 +8980,11 @@ func rewriteValuegeneric_OpTrunc64to32(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const32 [int64(int32(c))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpConst32)
 		v.AuxInt = int64(int32(c))
 		return true
@@ -8470,14 +8993,16 @@ func rewriteValuegeneric_OpTrunc64to32(v *Value, config *Config) bool {
 	// cond: y&0xFFFFFFFF == 0xFFFFFFFF
 	// result: (Trunc64to32 x)
 	for {
-		if v.Args[0].Op != OpAnd64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAnd64 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConst64 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConst64 {
 			break
 		}
-		y := v.Args[0].Args[0].AuxInt
-		x := v.Args[0].Args[1]
+		y := v_0_0.AuxInt
+		x := v_0.Args[1]
 		if !(y&0xFFFFFFFF == 0xFFFFFFFF) {
 			break
 		}
@@ -8494,10 +9019,11 @@ func rewriteValuegeneric_OpTrunc64to8(v *Value, config *Config) bool {
 	// cond:
 	// result: (Const8 [int64(int8(c))])
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		c := v.Args[0].AuxInt
+		c := v_0.AuxInt
 		v.reset(OpConst8)
 		v.AuxInt = int64(int8(c))
 		return true
@@ -8506,14 +9032,16 @@ func rewriteValuegeneric_OpTrunc64to8(v *Value, config *Config) bool {
 	// cond: y&0xFF == 0xFF
 	// result: (Trunc64to8 x)
 	for {
-		if v.Args[0].Op != OpAnd64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpAnd64 {
 			break
 		}
-		if v.Args[0].Args[0].Op != OpConst64 {
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpConst64 {
 			break
 		}
-		y := v.Args[0].Args[0].AuxInt
-		x := v.Args[0].Args[1]
+		y := v_0_0.AuxInt
+		x := v_0.Args[1]
 		if !(y&0xFF == 0xFF) {
 			break
 		}
@@ -8531,11 +9059,12 @@ func rewriteValuegeneric_OpXor16(v *Value, config *Config) bool {
 	// result: (Xor16 (Const16 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst16 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst16 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst16) {
 			break
 		}
@@ -8551,7 +9080,7 @@ func rewriteValuegeneric_OpXor16(v *Value, config *Config) bool {
 	// result: (Const16 [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConst16)
@@ -8562,10 +9091,11 @@ func rewriteValuegeneric_OpXor16(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst16 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst16 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -8584,11 +9114,12 @@ func rewriteValuegeneric_OpXor32(v *Value, config *Config) bool {
 	// result: (Xor32 (Const32 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst32 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst32 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst32) {
 			break
 		}
@@ -8604,7 +9135,7 @@ func rewriteValuegeneric_OpXor32(v *Value, config *Config) bool {
 	// result: (Const32 [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConst32)
@@ -8615,10 +9146,11 @@ func rewriteValuegeneric_OpXor32(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst32 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst32 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -8637,11 +9169,12 @@ func rewriteValuegeneric_OpXor64(v *Value, config *Config) bool {
 	// result: (Xor64 (Const64 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst64 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst64) {
 			break
 		}
@@ -8657,7 +9190,7 @@ func rewriteValuegeneric_OpXor64(v *Value, config *Config) bool {
 	// result: (Const64 [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConst64)
@@ -8668,10 +9201,11 @@ func rewriteValuegeneric_OpXor64(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst64 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst64 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -8690,11 +9224,12 @@ func rewriteValuegeneric_OpXor8(v *Value, config *Config) bool {
 	// result: (Xor8 (Const8 <t> [c]) x)
 	for {
 		x := v.Args[0]
-		if v.Args[1].Op != OpConst8 {
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst8 {
 			break
 		}
-		t := v.Args[1].Type
-		c := v.Args[1].AuxInt
+		t := v_1.Type
+		c := v_1.AuxInt
 		if !(x.Op != OpConst8) {
 			break
 		}
@@ -8710,7 +9245,7 @@ func rewriteValuegeneric_OpXor8(v *Value, config *Config) bool {
 	// result: (Const8 [0])
 	for {
 		x := v.Args[0]
-		if v.Args[1] != x {
+		if x != v.Args[1] {
 			break
 		}
 		v.reset(OpConst8)
@@ -8721,10 +9256,11 @@ func rewriteValuegeneric_OpXor8(v *Value, config *Config) bool {
 	// cond:
 	// result: x
 	for {
-		if v.Args[0].Op != OpConst8 {
+		v_0 := v.Args[0]
+		if v_0.Op != OpConst8 {
 			break
 		}
-		if v.Args[0].AuxInt != 0 {
+		if v_0.AuxInt != 0 {
 			break
 		}
 		x := v.Args[1]
@@ -8746,7 +9282,8 @@ func rewriteBlockgeneric(b *Block) bool {
 			if v.Op != OpNilCheck {
 				break
 			}
-			if v.Args[0].Op != OpGetG {
+			v_0 := v.Args[0]
+			if v_0.Op != OpGetG {
 				break
 			}
 			next := b.Succs[0]
