commit a7cc865edbb9568c1a695bde8e8459d97e129f08
Author: Xiaodong Liu <liuxiaodong@loongson.cn>
Date:   Thu May 19 20:01:10 2022 +0800

    runtime/internal/atomic: add atomic support for loong64
    
    Contributors to the loong64 port are:
      Weining Lu <luweining@loongson.cn>
      Lei Wang <wanglei@loongson.cn>
      Lingqin Gong <gonglingqin@loongson.cn>
      Xiaolin Zhao <zhaoxiaolin@loongson.cn>
      Meidan Li <limeidan@loongson.cn>
      Xiaojuan Zhai <zhaixiaojuan@loongson.cn>
      Qiyuan Pu <puqiyuan@loongson.cn>
      Guoqi Chen <chenguoqi@loongson.cn>
    
    This port has been updated to Go 1.15.6:
      https://github.com/loongson/go
    
    Updates #46229
    
    Change-Id: I0333503db044c6f39df2d7f8d9dff213b1361d6c
    Reviewed-on: https://go-review.googlesource.com/c/go/+/342320
    Reviewed-by: David Chase <drchase@google.com>
    Auto-Submit: Ian Lance Taylor <iant@google.com>
    TryBot-Result: Gopher Robot <gobot@golang.org>
    Run-TryBot: Ian Lance Taylor <iant@google.com>
    Reviewed-by: Ian Lance Taylor <iant@google.com>
---
 src/runtime/internal/atomic/atomic_loong64.go |  83 +++++++
 src/runtime/internal/atomic/atomic_loong64.s  | 299 ++++++++++++++++++++++++++
 2 files changed, 382 insertions(+)

diff --git a/src/runtime/internal/atomic/atomic_loong64.go b/src/runtime/internal/atomic/atomic_loong64.go
new file mode 100644
index 0000000000..908a7d69aa
--- /dev/null
+++ b/src/runtime/internal/atomic/atomic_loong64.go
@@ -0,0 +1,83 @@
+// Copyright 2022 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+//go:build loong64
+
+package atomic
+
+import "unsafe"
+
+//go:noescape
+func Xadd(ptr *uint32, delta int32) uint32
+
+//go:noescape
+func Xadd64(ptr *uint64, delta int64) uint64
+
+//go:noescape
+func Xadduintptr(ptr *uintptr, delta uintptr) uintptr
+
+//go:noescape
+func Xchg(ptr *uint32, new uint32) uint32
+
+//go:noescape
+func Xchg64(ptr *uint64, new uint64) uint64
+
+//go:noescape
+func Xchguintptr(ptr *uintptr, new uintptr) uintptr
+
+//go:noescape
+func Load(ptr *uint32) uint32
+
+//go:noescape
+func Load8(ptr *uint8) uint8
+
+//go:noescape
+func Load64(ptr *uint64) uint64
+
+// NO go:noescape annotation; *ptr escapes if result escapes (#31525)
+func Loadp(ptr unsafe.Pointer) unsafe.Pointer
+
+//go:noescape
+func LoadAcq(ptr *uint32) uint32
+
+//go:noescape
+func LoadAcquintptr(ptr *uintptr) uintptr
+
+//go:noescape
+func And8(ptr *uint8, val uint8)
+
+//go:noescape
+func And(ptr *uint32, val uint32)
+
+//go:noescape
+func Or8(ptr *uint8, val uint8)
+
+//go:noescape
+func Or(ptr *uint32, val uint32)
+
+// NOTE: Do not add atomicxor8 (XOR is not idempotent).
+
+//go:noescape
+func Cas64(ptr *uint64, old, new uint64) bool
+
+//go:noescape
+func CasRel(ptr *uint32, old, new uint32) bool
+
+//go:noescape
+func Store(ptr *uint32, val uint32)
+
+//go:noescape
+func Store8(ptr *uint8, val uint8)
+
+//go:noescape
+func Store64(ptr *uint64, val uint64)
+
+// NO go:noescape annotation; see atomic_pointer.go.
+func StorepNoWB(ptr unsafe.Pointer, val unsafe.Pointer)
+
+//go:noescape
+func StoreRel(ptr *uint32, val uint32)
+
+//go:noescape
+func StoreReluintptr(ptr *uintptr, val uintptr)
diff --git a/src/runtime/internal/atomic/atomic_loong64.s b/src/runtime/internal/atomic/atomic_loong64.s
new file mode 100644
index 0000000000..bfb6c7e130
--- /dev/null
+++ b/src/runtime/internal/atomic/atomic_loong64.s
@@ -0,0 +1,299 @@
+// Copyright 2022 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+// bool cas(uint32 *ptr, uint32 old, uint32 new)
+// Atomically:
+//	if(*ptr == old){
+//		*ptr = new;
+//		return 1;
+//	} else
+//		return 0;
+TEXT ·Cas(SB), NOSPLIT, $0-17
+	MOVV	ptr+0(FP), R4
+	MOVW	old+8(FP), R5
+	MOVW	new+12(FP), R6
+	DBAR
+cas_again:
+	MOVV	R6, R7
+	LL	(R4), R8
+	BNE	R5, R8, cas_fail
+	SC	R7, (R4)
+	BEQ	R7, cas_again
+	MOVV	$1, R4
+	MOVB	R4, ret+16(FP)
+	DBAR
+	RET
+cas_fail:
+	MOVV	$0, R4
+	JMP	-4(PC)
+
+// bool	cas64(uint64 *ptr, uint64 old, uint64 new)
+// Atomically:
+//	if(*ptr == old){
+//		*ptr = new;
+//		return 1;
+//	} else {
+//		return 0;
+//	}
+TEXT ·Cas64(SB), NOSPLIT, $0-25
+	MOVV	ptr+0(FP), R4
+	MOVV	old+8(FP), R5
+	MOVV	new+16(FP), R6
+	DBAR
+cas64_again:
+	MOVV	R6, R7
+	LLV	(R4), R8
+	BNE	R5, R8, cas64_fail
+	SCV	R7, (R4)
+	BEQ	R7, cas64_again
+	MOVV	$1, R4
+	MOVB	R4, ret+24(FP)
+	DBAR
+	RET
+cas64_fail:
+	MOVV	$0, R4
+	JMP	-4(PC)
+
+TEXT ·Casuintptr(SB), NOSPLIT, $0-25
+	JMP	·Cas64(SB)
+
+TEXT ·CasRel(SB), NOSPLIT, $0-17
+	JMP	·Cas(SB)
+
+TEXT ·Loaduintptr(SB),  NOSPLIT|NOFRAME, $0-16
+	JMP	·Load64(SB)
+
+TEXT ·Loaduint(SB), NOSPLIT|NOFRAME, $0-16
+	JMP	·Load64(SB)
+
+TEXT ·Storeuintptr(SB), NOSPLIT, $0-16
+	JMP	·Store64(SB)
+
+TEXT ·Xadduintptr(SB), NOSPLIT, $0-24
+	JMP	·Xadd64(SB)
+
+TEXT ·Loadint64(SB), NOSPLIT, $0-16
+	JMP	·Load64(SB)
+
+TEXT ·Xaddint64(SB), NOSPLIT, $0-24
+	JMP	·Xadd64(SB)
+
+// bool casp(void **val, void *old, void *new)
+// Atomically:
+//	if(*val == old){
+//		*val = new;
+//		return 1;
+//	} else
+//		return 0;
+TEXT ·Casp1(SB), NOSPLIT, $0-25
+	JMP runtime∕internal∕atomic·Cas64(SB)
+
+// uint32 xadd(uint32 volatile *ptr, int32 delta)
+// Atomically:
+//	*val += delta;
+//	return *val;
+TEXT ·Xadd(SB), NOSPLIT, $0-20
+	MOVV	ptr+0(FP), R4
+	MOVW	delta+8(FP), R5
+	DBAR
+	LL	(R4), R6
+	ADDU	R6, R5, R7
+	MOVV	R7, R6
+	SC	R7, (R4)
+	BEQ	R7, -4(PC)
+	MOVW	R6, ret+16(FP)
+	DBAR
+	RET
+
+TEXT ·Xadd64(SB), NOSPLIT, $0-24
+	MOVV	ptr+0(FP), R4
+	MOVV	delta+8(FP), R5
+	DBAR
+	LLV	(R4), R6
+	ADDVU	R6, R5, R7
+	MOVV	R7, R6
+	SCV	R7, (R4)
+	BEQ	R7, -4(PC)
+	MOVV	R6, ret+16(FP)
+	DBAR
+	RET
+
+TEXT ·Xchg(SB), NOSPLIT, $0-20
+	MOVV	ptr+0(FP), R4
+	MOVW	new+8(FP), R5
+
+	DBAR
+	MOVV	R5, R6
+	LL	(R4), R7
+	SC	R6, (R4)
+	BEQ	R6, -3(PC)
+	MOVW	R7, ret+16(FP)
+	DBAR
+	RET
+
+TEXT ·Xchg64(SB), NOSPLIT, $0-24
+	MOVV	ptr+0(FP), R4
+	MOVV	new+8(FP), R5
+
+	DBAR
+	MOVV	R5, R6
+	LLV	(R4), R7
+	SCV	R6, (R4)
+	BEQ	R6, -3(PC)
+	MOVV	R7, ret+16(FP)
+	DBAR
+	RET
+
+TEXT ·Xchguintptr(SB), NOSPLIT, $0-24
+	JMP	·Xchg64(SB)
+
+TEXT ·StorepNoWB(SB), NOSPLIT, $0-16
+	JMP	·Store64(SB)
+
+TEXT ·StoreRel(SB), NOSPLIT, $0-12
+	JMP	·Store(SB)
+
+TEXT ·StoreReluintptr(SB), NOSPLIT, $0-16
+	JMP     ·Store64(SB)
+
+TEXT ·Store(SB), NOSPLIT, $0-12
+	MOVV	ptr+0(FP), R4
+	MOVW	val+8(FP), R5
+	DBAR
+	MOVW	R5, 0(R4)
+	DBAR
+	RET
+
+TEXT ·Store8(SB), NOSPLIT, $0-9
+	MOVV	ptr+0(FP), R4
+	MOVB	val+8(FP), R5
+	DBAR
+	MOVB	R5, 0(R4)
+	DBAR
+	RET
+
+TEXT ·Store64(SB), NOSPLIT, $0-16
+	MOVV	ptr+0(FP), R4
+	MOVV	val+8(FP), R5
+	DBAR
+	MOVV	R5, 0(R4)
+	DBAR
+	RET
+
+// void	Or8(byte volatile*, byte);
+TEXT ·Or8(SB), NOSPLIT, $0-9
+	MOVV	ptr+0(FP), R4
+	MOVBU	val+8(FP), R5
+	// Align ptr down to 4 bytes so we can use 32-bit load/store.
+	MOVV	$~3, R6
+	AND	R4, R6
+	// R7 = ((ptr & 3) * 8)
+	AND	$3, R4, R7
+	SLLV	$3, R7
+	// Shift val for aligned ptr. R5 = val << R4
+	SLLV	R7, R5
+
+	DBAR
+	LL	(R6), R7
+	OR	R5, R7
+	SC	R7, (R6)
+	BEQ	R7, -4(PC)
+	DBAR
+	RET
+
+// void	And8(byte volatile*, byte);
+TEXT ·And8(SB), NOSPLIT, $0-9
+	MOVV	ptr+0(FP), R4
+	MOVBU	val+8(FP), R5
+	// Align ptr down to 4 bytes so we can use 32-bit load/store.
+	MOVV	$~3, R6
+	AND	R4, R6
+	// R7 = ((ptr & 3) * 8)
+	AND	$3, R4, R7
+	SLLV	$3, R7
+	// Shift val for aligned ptr. R5 = val << R7 | ^(0xFF << R7)
+	MOVV	$0xFF, R8
+	SLLV	R7, R5
+	SLLV	R7, R8
+	NOR	R0, R8
+	OR	R8, R5
+
+	DBAR
+	LL	(R6), R7
+	AND	R5, R7
+	SC	R7, (R6)
+	BEQ	R7, -4(PC)
+	DBAR
+	RET
+
+// func Or(addr *uint32, v uint32)
+TEXT ·Or(SB), NOSPLIT, $0-12
+	MOVV	ptr+0(FP), R4
+	MOVW	val+8(FP), R5
+	DBAR
+	LL	(R4), R6
+	OR	R5, R6
+	SC	R6, (R4)
+	BEQ	R6, -4(PC)
+	DBAR
+	RET
+
+// func And(addr *uint32, v uint32)
+TEXT ·And(SB), NOSPLIT, $0-12
+	MOVV	ptr+0(FP), R4
+	MOVW	val+8(FP), R5
+	DBAR
+	LL	(R4), R6
+	AND	R5, R6
+	SC	R6, (R4)
+	BEQ	R6, -4(PC)
+	DBAR
+	RET
+
+// uint32 runtime∕internal∕atomic·Load(uint32 volatile* ptr)
+TEXT ·Load(SB),NOSPLIT|NOFRAME,$0-12
+	MOVV	ptr+0(FP), R19
+	DBAR
+	MOVWU	0(R19), R19
+	DBAR
+	MOVW	R19, ret+8(FP)
+	RET
+
+// uint8 runtime∕internal∕atomic·Load8(uint8 volatile* ptr)
+TEXT ·Load8(SB),NOSPLIT|NOFRAME,$0-9
+	MOVV	ptr+0(FP), R19
+	DBAR
+	MOVBU	0(R19), R19
+	DBAR
+	MOVB	R19, ret+8(FP)
+	RET
+
+// uint64 runtime∕internal∕atomic·Load64(uint64 volatile* ptr)
+TEXT ·Load64(SB),NOSPLIT|NOFRAME,$0-16
+	MOVV	ptr+0(FP), R19
+	DBAR
+	MOVV	0(R19), R19
+	DBAR
+	MOVV	R19, ret+8(FP)
+	RET
+
+// void *runtime∕internal∕atomic·Loadp(void *volatile *ptr)
+TEXT ·Loadp(SB),NOSPLIT|NOFRAME,$0-16
+	MOVV	ptr+0(FP), R19
+	DBAR
+	MOVV	0(R19), R19
+	DBAR
+	MOVV	R19, ret+8(FP)
+	RET
+
+// uint32 runtime∕internal∕atomic·LoadAcq(uint32 volatile* ptr)
+TEXT ·LoadAcq(SB),NOSPLIT|NOFRAME,$0-12
+	JMP	atomic·Load(SB)
+
+// uintptr ·LoadAcquintptr(uintptr volatile* ptr)
+TEXT ·LoadAcquintptr(SB),NOSPLIT|NOFRAME,$0-16
+	JMP     atomic·Load64(SB)
+
